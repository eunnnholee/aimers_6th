{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# WideDeep_FTTransformer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 라이브러리 설치"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip -q install torch==2.5.1 torchvision==0.20.1 torchaudio==2.5.1 --index-url https://download.pytorch.org/whl/cu124"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install pytorch-widedeep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install opencv-python-headless -> 저는 이거 없으면 엘리스에서 에러 나서 깔았습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/miniconda/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "2025-04-05 22:43:41.730137: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-04-05 22:43:41.743994: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1743893021.758963 1850157 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1743893021.763558 1850157 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2025-04-05 22:43:41.780925: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sqlite3\n",
    "import shutil\n",
    "import datetime\n",
    "import os\n",
    "import warnings\n",
    "import random\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "\n",
    "import optuna\n",
    "\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.preprocessing import LabelEncoder, FunctionTransformer, QuantileTransformer, MultiLabelBinarizer\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# pytorch-widedeep 라이브러리 import\n",
    "from pytorch_widedeep import Trainer\n",
    "from pytorch_widedeep.preprocessing import WidePreprocessor, TabPreprocessor\n",
    "from pytorch_widedeep.models import Wide, FTTransformer, WideDeep, TabFastFormer\n",
    "from pytorch_widedeep.metrics import Accuracy,F1Score\n",
    "from pytorch_widedeep.callbacks import EarlyStopping, ModelCheckpoint\n",
    "\n",
    "\n",
    "from 평가_Metric import competition_metric, f1_score, weighted_brier_score\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(126244, 34) (54412, 33)\n"
     ]
    }
   ],
   "source": [
    "train_path = f'../data/train.csv'\n",
    "test_path = f'../data/test.csv'\n",
    "sample_path = f'../data/sample_submission.csv'\n",
    "\n",
    "\n",
    "train = pd.read_csv(train_path).drop(columns=[\"ID\"])\n",
    "test = pd.read_csv(test_path).drop(columns=[\"ID\"])\n",
    "print(train.shape, test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def drop_cols_with_na(train_df, val_df):\n",
    "    # 나중에 결측치 대체하면서 반영할 예정\n",
    "\n",
    "    cat_cols_with_na = [\n",
    "        '이전 총 임신 횟수',\n",
    "        '이전 총 임신 성공 횟수',\n",
    "\n",
    "        '총 생성 배아 수', ## 여기부터 100% DI\n",
    "        '저장된 배아 수',\n",
    "        '채취된 신선 난자 수',\n",
    "        '수정 시도된 난자 수'\n",
    "    ]\n",
    "\n",
    "    numeric_cols_with_na = [\n",
    "        '이식된 배아 수', ## only DI\n",
    "        '미세주입(ICSI) 배아 이식 수',\n",
    "        '배아 이식 후 경과일',\n",
    "    ]\n",
    "    train_df = train_df.drop(columns=cat_cols_with_na)\n",
    "    train_df = train_df.drop(columns=numeric_cols_with_na)\n",
    "    val_df = val_df.drop(columns=cat_cols_with_na)\n",
    "    val_df = val_df.drop(columns=numeric_cols_with_na)\n",
    "    return train_df, val_df\n",
    "\n",
    "\n",
    "def 시술유형(train, test):\n",
    "    train['세부 시술 유형'] = train['세부 시술 유형'].fillna(\"Unknown\")\n",
    "    test['세부 시술 유형'] = test['세부 시술 유형'].fillna(\"Unknown\")\n",
    "\n",
    "    def categorize_procedure(proc):\n",
    "        tokens = [token.strip() for token in proc.split(\",\") if token.strip() and not token.strip().isdigit()]\n",
    "        # 우선순위에 따른 범주화\n",
    "        if tokens.count(\"Unknown\") >= 1:\n",
    "            return \"Unknown\"\n",
    "        if tokens.count(\"AH\") >= 1:\n",
    "            return \"AH\"\n",
    "        if tokens.count(\"BLASTOCYST\") >= 1:\n",
    "            return \"BLASTOCYST\"\n",
    "        if tokens.count(\"ICSI\") >= 2 or tokens.count(\"IVF\") >= 2:\n",
    "            return \"2ICSI_2IVF\"\n",
    "        if tokens.count(\"IVF\") >= 1 and tokens.count(\"ICSI\") >= 1:\n",
    "            return \"IVF_ICSI\"\n",
    "        if tokens == \"ICSI\":\n",
    "            return \"ICSI\"\n",
    "        if tokens == \"IVF\":\n",
    "            return \"IVF\"\n",
    "        return \",\".join(tokens) if tokens else None\n",
    "\n",
    "    for df in [train, test]:\n",
    "        df['세부 시술 유형'] = df['세부 시술 유형'].str.replace(\" / \", \",\")\n",
    "        df['세부 시술 유형'] = df['세부 시술 유형'].str.replace(\":\", \",\")\n",
    "        df['세부 시술 유형'] = df['세부 시술 유형'].str.replace(\" \", \"\")\n",
    "\n",
    "    counts = train['세부 시술 유형'].value_counts()\n",
    "    allowed_categories = counts[counts >= 100].index.tolist()\n",
    "\n",
    "    # allowed_categories에 속하지 않는 값은 \"Unknown\"으로 대체\n",
    "    train.loc[~train['세부 시술 유형'].isin(allowed_categories), '세부 시술 유형'] = \"Unknown\"\n",
    "    test.loc[~test['세부 시술 유형'].isin(allowed_categories), '세부 시술 유형'] = \"Unknown\"\n",
    "\n",
    "    train['세부 시술 유형'] = train['세부 시술 유형'].apply(categorize_procedure)\n",
    "    test['세부 시술 유형'] = test['세부 시술 유형'].apply(categorize_procedure)\n",
    "\n",
    "    train['시술유형_통합'] = train['시술 유형'].astype(str) + '_' + train['세부 시술 유형'].astype(str)\n",
    "    test['시술유형_통합'] = test['시술 유형'].astype(str) + '_' + test['세부 시술 유형'].astype(str)\n",
    "\n",
    "    drop_cols = ['시술 유형', '세부 시술 유형']\n",
    "    train = train.drop(drop_cols, axis=1)\n",
    "    test = test.drop(drop_cols, axis=1)\n",
    "\n",
    "    return train, test\n",
    "\n",
    "def 횟수_to_int(df_train, df_val):\n",
    "    for col in [col for col in df_train.columns if '횟수' in col]:\n",
    "        df_train[col] = df_train[col].replace({'6회 이상': '6회'})\n",
    "        df_val[col] = df_val[col].replace({'6회 이상': '6회'})\n",
    "\n",
    "        #### 일단 0으로 채움\n",
    "        df_train[col] = df_train[col].fillna('0')\n",
    "        df_val[col] = df_val[col].fillna('0')\n",
    "        ####\n",
    "\n",
    "        \n",
    "        df_train[col] = df_train[col].str[0].astype(int)\n",
    "        df_val[col] = df_val[col].str[0].astype(int)\n",
    "\n",
    "    return df_train, df_val\n",
    "\n",
    "\n",
    "def 임신_IVF(df_train, df_val):\n",
    "    for col in [col for col in df_train.columns if '횟수' in col]:\n",
    "        df_train[col] = df_train[col].replace({'6회 이상': '6회'})\n",
    "        df_val[col] = df_val[col].replace({'6회 이상': '6회'})\n",
    "        mode_value = df_train[col].mode()[0]\n",
    "\n",
    "        df_train[col] = df_train[col].fillna(mode_value)\n",
    "        df_val[col] = df_val[col].fillna(mode_value)\n",
    "\n",
    "        # 문자열의 첫 글자를 추출 후 int형으로 변환\n",
    "        df_train[col] = df_train[col].str[0].astype(int)\n",
    "        df_val[col] = df_val[col].str[0].astype(int)\n",
    "\n",
    "    df_train['임신_IVF'] = df_train['이전 총 임신 횟수'] - df_train['이전 IVF 시술 횟수']\n",
    "    df_val['임신_IVF'] = df_val['이전 총 임신 횟수'] - df_val['이전 IVF 시술 횟수']\n",
    "    # df_train = df_train.drop('이전 시술 횟수', axis=1)\n",
    "    return df_train, df_val\n",
    "\n",
    "def 난자기증자나이(df_train, df_test):\n",
    "    mapping = {\n",
    "        '만20세 이하': 20,\n",
    "        '만21-25세': 25,\n",
    "        '만26-30세': 30,\n",
    "        '만31-35세': 35,\n",
    "        '알 수 없음': 20,  # 만20세 이하와 동일하게 처리\n",
    "    }\n",
    "    df_train['난자 기증자 나이'] = df_train['난자 기증자 나이'].replace(mapping)\n",
    "    df_test['난자 기증자 나이'] = df_test['난자 기증자 나이'].replace(mapping)\n",
    "    return df_train, df_test\n",
    "\n",
    "\n",
    "def 단일배아이식여부(df_train, df_val):\n",
    "    df_train['단일 배아 이식 여부'] = df_train['단일 배아 이식 여부'].fillna(0)\n",
    "    df_val['단일 배아 이식 여부'] = df_val['단일 배아 이식 여부'].fillna(0)\n",
    "    return df_train, df_val\n",
    "\n",
    "\n",
    "def 이전_총_임신_성공_횟수(train, test):\n",
    "    train['이전 총 임신 횟수'] = train['이전 총 임신 횟수'].fillna(train['이전 총 임신 횟수'].mode()[0])\n",
    "    test['이전 총 임신 횟수'] = test['이전 총 임신 횟수'].fillna(train['이전 총 임신 횟수'].mode()[0])\n",
    "\n",
    "    train['이전 총 임신 성공 횟수'] = train['이전 총 임신 성공 횟수'].fillna(train['이전 총 임신 성공 횟수'].mode()[0])\n",
    "    test['이전 총 임신 성공 횟수'] = test['이전 총 임신 성공 횟수'].fillna(train['이전 총 임신 성공 횟수'].mode()[0])\n",
    "\n",
    "def 독립범주로보기(train, test):\n",
    "    cols = ['총 생성 배아 수', '저장된 배아 수', '채취된 신선 난자 수', '수정 시도된 난자 수']\n",
    "    for col in cols:\n",
    "        train[col] = train[col].fillna('NAN')\n",
    "        test[col] = test[col].fillna('NAN')\n",
    "\n",
    "\n",
    "    \n",
    "def label_encoding(train, test, cols):\n",
    "    encoder = LabelEncoder()\n",
    "    for col in cols:\n",
    "        train[col] = encoder.fit_transform(train[col])\n",
    "        test[col] = encoder.transform(test[col])\n",
    "    return train, test\n",
    "\n",
    "def type_to_category(train, test, cols):\n",
    "    train[cols] = train[cols].astype('category')\n",
    "    test[cols] = test[cols].astype('category')\n",
    "    return train, test\n",
    "\n",
    "def impute_nan(train, test):\n",
    "    cat_cols_with_na = [\n",
    "        '이전 총 임신 횟수',\n",
    "        '이전 총 임신 성공 횟수',\n",
    "\n",
    "        '총 생성 배아 수', ## 여기부터 100% DI\n",
    "        '저장된 배아 수',\n",
    "        '채취된 신선 난자 수',\n",
    "        '수정 시도된 난자 수'\n",
    "    ]\n",
    "\n",
    "    numeric_cols_with_na = [\n",
    "        '이식된 배아 수', ## only DI\n",
    "        '미세주입(ICSI) 배아 이식 수',\n",
    "        '배아 이식 후 경과일',\n",
    "    ]\n",
    "    cols_to_impute= cat_cols_with_na + numeric_cols_with_na\n",
    "    \n",
    "    for col in cols_to_impute:\n",
    "        train[col] = train[col].fillna(0)\n",
    "        test[col] = test[col].fillna(0)\n",
    "\n",
    "    return train, test\n",
    "\n",
    "\n",
    "def num_feature_scailing(train, test, seed=777):\n",
    "    cat_cols = [col for col in train.columns if pd.api.types.is_categorical_dtype(train[col])]\n",
    "    numeric_cols = [col for col in train.columns if col not in cat_cols and col != '임신 성공 확률']\n",
    "    # bin_cols 들도 동일하게 스케일링\n",
    "\n",
    "\n",
    "    arr_train = train[numeric_cols].to_numpy()  # DataFrame -> NumPy\n",
    "    arr_train = arr_train.astype(np.float32)\n",
    "    arr_test = test[numeric_cols].to_numpy()\n",
    "    arr_test = arr_test.astype(np.float32)\n",
    "\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "    noise = (\n",
    "        np.random.default_rng(0)\n",
    "        .normal(0.0, 1e-5, arr_train.shape)\n",
    "        .astype(arr_train.dtype)\n",
    "    )\n",
    "    preprocessing = QuantileTransformer(\n",
    "        n_quantiles=max(min(len(train[numeric_cols]) // 30, 1000), 10),\n",
    "        output_distribution='normal',\n",
    "        subsample=10**9,\n",
    "    ).fit(arr_train + noise)\n",
    "\n",
    "    train[numeric_cols] = preprocessing.transform(arr_train)\n",
    "    test[numeric_cols] = preprocessing.transform(arr_test)\n",
    "    return train, test\n",
    "\n",
    "\n",
    "def check_cols(train,val):\n",
    "\n",
    "    cols_without_na = train.columns\n",
    "    categorical_columns = ['환자 시술 당시 나이', '시술 유형', '세부 시술 유형', '이전 IVF 시술 횟수',\n",
    "       '이전 DI 시술 횟수', '이전 총 임신 횟수', '이전 총 임신 성공 횟수', '총 생성 배아 수', '저장된 배아 수',\n",
    "       '해동된 배아 수', '채취된 신선 난자 수', '수정 시도된 난자 수', '난자 출처', '정자 출처', '난자 기증자 나이',\n",
    "       '정자 기증자 나이']\n",
    "\n",
    "    numeric_columns = ['이식된 배아 수','미세주입(ICSI) 배아 이식 수','배아 이식 후 경과일']\n",
    "\n",
    "    bool_features = ['배란 자극 시술 여부', '단일 배아 이식 여부', '불임 원인 - 난관 질환',\n",
    "    '불임 원인 - 배란 장애', '불임 원인 - 남성 요인', '불임 원인 - 자궁내막증', '불임 원인 - 불명확', '해동 난자 사용 여부',\n",
    "    '신선 난자 사용 여부', '동결 배아 사용 여부', '신선 배아 사용 여부', '기증 배아 사용 여부', '착상 전 PGD 시행 여부', '착상 전 PGS 시행 여부']\n",
    "    \n",
    "    categorical_columns_cleaned = [col for col in categorical_columns if col in cols_without_na]\n",
    "    numeric_columns_cleaned = [col for col in numeric_columns if col in cols_without_na]\n",
    "    bool_features_cleaned = [col for col in bool_features if col in cols_without_na]\n",
    "    \n",
    "    # print(\"Categorical columns:\", categorical_columns_cleaned, \"\\n count\", len(categorical_columns_cleaned))\n",
    "    # print(\"Numeric columns:\", numeric_columns_cleaned, \"\\n count\", len(numeric_columns_cleaned))\n",
    "    # print(\"Boolean columns:\", bool_features_cleaned, \"\\n count\", len(bool_features_cleaned))\n",
    "\n",
    "    return categorical_columns_cleaned, numeric_columns_cleaned, bool_features_cleaned\n",
    "\n",
    "\n",
    "def drop_single_value_columns(df_train, df_test):\n",
    "    cols_to_drop = [col for col in df_train.columns if df_train[col].nunique() == 1]\n",
    "    return df_train.drop(columns=cols_to_drop), df_test.drop(columns=cols_to_drop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1850157/641393060.py:180: DeprecationWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, pd.CategoricalDtype) instead\n",
      "  cat_cols = [col for col in train.columns if pd.api.types.is_categorical_dtype(train[col])]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "수치형 변수: 21개 \n",
      "['배란 자극 시술 여부', '단일 배아 이식 여부', '불임 원인 - 난관 질환', '불임 원인 - 배란 장애', '불임 원인 - 남성 요인', '불임 원인 - 자궁내막증', '불임 원인 - 불명확', '이전 IVF 시술 횟수', '이전 DI 시술 횟수', '이전 총 임신 횟수', '이전 총 임신 성공 횟수', '이식된 배아 수', '미세주입(ICSI) 배아 이식 수', '해동 난자 사용 여부', '신선 난자 사용 여부', '동결 배아 사용 여부', '신선 배아 사용 여부', '기증 배아 사용 여부', '착상 전 PGD 시행 여부', '착상 전 PGS 시행 여부', '배아 이식 후 경과일']\n",
      "범주형 변수: 11개 \n",
      "['환자 시술 당시 나이', '총 생성 배아 수', '저장된 배아 수', '해동된 배아 수', '채취된 신선 난자 수', '수정 시도된 난자 수', '난자 출처', '정자 출처', '난자 기증자 나이', '정자 기증자 나이', '시술유형_통합']\n",
      "(126244, 33) (54412, 32)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1850157/1834065377.py:56: DeprecationWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, pd.CategoricalDtype) instead\n",
      "  cat_cols = [col for col in train.columns if pd.api.types.is_categorical_dtype(train[col])]\n"
     ]
    }
   ],
   "source": [
    "def all_process(train, val):\n",
    "    # train, val = drop_cols_with_na(train, val)\n",
    "\n",
    "    # 기본 전처리 단계\n",
    "    train, val = 횟수_to_int(train, val)\n",
    "\n",
    "    train, val = 시술유형(train, val)\n",
    "    # train, val = 임신_IVF(train, val)\n",
    "\n",
    "    train, val = 단일배아이식여부(train, val)\n",
    "    이전_총_임신_성공_횟수(train, val)\n",
    "    독립범주로보기(train, val)\n",
    "\n",
    "    \n",
    "    cols_to_encoding = [\n",
    "        \"환자 시술 당시 나이\",\n",
    "        # \"클리닉 내 총 시술 횟수\",\n",
    "        # \"IVF 시술 횟수\",\n",
    "        # \"DI 시술 횟수\",\n",
    "        # \"총 임신 횟수\",\n",
    "        # \"IVF 임신 횟수\",\n",
    "        # \"DI 임신 횟수\",\n",
    "        # \"총 출산 횟수\",\n",
    "        # \"IVF 출산 횟수\",\n",
    "        # \"DI 출산 횟수\",\n",
    "        \"난자 출처\",\n",
    "        \"정자 출처\",\n",
    "        \"난자 기증자 나이\",\n",
    "        \"정자 기증자 나이\",\n",
    "        '시술유형_통합',\n",
    "\n",
    "        '해동된 배아 수', # 원래 int였는데 범주형으로 바뀜\n",
    "        '총 생성 배아 수',\n",
    "        '저장된 배아 수', \n",
    "        '채취된 신선 난자 수', \n",
    "        '수정 시도된 난자 수'\n",
    "        \n",
    "\n",
    "    ]\n",
    "    \n",
    "    train, val = label_encoding(train, val, cols=cols_to_encoding)\n",
    "    train, val = type_to_category(train, val, cols=cols_to_encoding)\n",
    "\n",
    "    train, val = impute_nan(train, val)\n",
    "    train, val = num_feature_scailing(train, val)\n",
    "\n",
    "    train, val = drop_single_value_columns(train, val)\n",
    "\n",
    "    return train, val\n",
    "\n",
    "train = pd.read_csv(train_path).drop(columns=[\"ID\"])\n",
    "test = pd.read_csv(test_path).drop(columns=[\"ID\"])\n",
    "\n",
    "train, test = all_process(train, test)\n",
    "\n",
    "cat_cols = [col for col in train.columns if pd.api.types.is_categorical_dtype(train[col])]\n",
    "numeric_cols = [col for col in train.columns if col not in cat_cols and col != '임신 성공 확률']\n",
    "\n",
    "print(f'수치형 변수: {len(numeric_cols)}개 \\n{numeric_cols}')\n",
    "print(f'범주형 변수: {len(cat_cols)}개 \\n{cat_cols}')\n",
    "print(train.shape, test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## FTT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_seed(seed: int = 42):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)  # if using multi-GPU\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1850157/641393060.py:180: DeprecationWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, pd.CategoricalDtype) instead\n",
      "  cat_cols = [col for col in train.columns if pd.api.types.is_categorical_dtype(train[col])]\n",
      "/tmp/ipykernel_1850157/641393060.py:180: DeprecationWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, pd.CategoricalDtype) instead\n",
      "  cat_cols = [col for col in train.columns if pd.api.types.is_categorical_dtype(train[col])]\n",
      "/tmp/ipykernel_1850157/166562470.py:38: DeprecationWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, pd.CategoricalDtype) instead\n",
      "  categorical_cols = [col for col in fold_train.columns if pd.api.types.is_categorical_dtype(fold_train[col])]\n",
      "/home/ubuntu/.local/lib/python3.12/site-packages/pytorch_widedeep/preprocessing/tab_preprocessor.py:364: UserWarning: Continuous columns will not be normalised\n",
      "  warnings.warn(\"Continuous columns will not be normalised\")\n",
      "  0%|          | 0/99 [00:00<?, ?it/s]/opt/miniconda/lib/python3.12/multiprocessing/popen_fork.py:66: DeprecationWarning: This process (pid=1850157) is multi-threaded, use of fork() may lead to deadlocks in the child.\n",
      "  self.pid = os.fork()\n",
      "epoch 1: 100%|██████████| 99/99 [00:08<00:00, 11.90it/s, loss=0.868, metrics={'acc': 0.6566}]\n",
      "valid: 100%|██████████| 25/25 [00:04<00:00,  5.18it/s, loss=0.737, metrics={'acc': 0.7305}]\n",
      "epoch 2: 100%|██████████| 99/99 [00:11<00:00,  8.66it/s, loss=0.718, metrics={'acc': 0.7288}]\n",
      "valid: 100%|██████████| 25/25 [00:06<00:00,  4.10it/s, loss=0.687, metrics={'acc': 0.7193}]\n",
      "epoch 3: 100%|██████████| 99/99 [00:09<00:00, 10.83it/s, loss=0.693, metrics={'acc': 0.7366}]\n",
      "valid: 100%|██████████| 25/25 [00:05<00:00,  4.93it/s, loss=0.675, metrics={'acc': 0.7261}]\n",
      "epoch 4: 100%|██████████| 99/99 [00:08<00:00, 11.87it/s, loss=0.686, metrics={'acc': 0.7378}]\n",
      "valid: 100%|██████████| 25/25 [00:05<00:00,  4.81it/s, loss=0.668, metrics={'acc': 0.7384}]\n",
      "epoch 5: 100%|██████████| 99/99 [00:07<00:00, 13.56it/s, loss=0.677, metrics={'acc': 0.7409}]\n",
      "valid: 100%|██████████| 25/25 [00:04<00:00,  5.47it/s, loss=0.666, metrics={'acc': 0.7425}]\n",
      "epoch 6: 100%|██████████| 99/99 [00:08<00:00, 12.00it/s, loss=0.675, metrics={'acc': 0.7411}]\n",
      "valid: 100%|██████████| 25/25 [00:04<00:00,  5.18it/s, loss=0.664, metrics={'acc': 0.7441}]\n",
      "epoch 7: 100%|██████████| 99/99 [00:09<00:00, 10.87it/s, loss=0.674, metrics={'acc': 0.7422}]\n",
      "valid: 100%|██████████| 25/25 [00:06<00:00,  3.96it/s, loss=0.664, metrics={'acc': 0.7464}]\n",
      "epoch 8: 100%|██████████| 99/99 [00:08<00:00, 11.29it/s, loss=0.672, metrics={'acc': 0.7418}]\n",
      "valid: 100%|██████████| 25/25 [00:04<00:00,  5.16it/s, loss=0.663, metrics={'acc': 0.7444}]\n",
      "epoch 9: 100%|██████████| 99/99 [00:10<00:00,  9.10it/s, loss=0.67, metrics={'acc': 0.7409}] \n",
      "valid: 100%|██████████| 25/25 [00:05<00:00,  4.55it/s, loss=0.662, metrics={'acc': 0.7467}]\n",
      "epoch 10: 100%|██████████| 99/99 [00:09<00:00, 10.56it/s, loss=0.669, metrics={'acc': 0.7438}]\n",
      "valid: 100%|██████████| 25/25 [00:03<00:00,  6.94it/s, loss=0.661, metrics={'acc': 0.7466}]\n",
      "epoch 11: 100%|██████████| 99/99 [00:08<00:00, 11.26it/s, loss=0.669, metrics={'acc': 0.7414}]\n",
      "valid: 100%|██████████| 25/25 [00:04<00:00,  5.54it/s, loss=0.66, metrics={'acc': 0.7483}] \n",
      "epoch 12: 100%|██████████| 99/99 [00:09<00:00, 10.40it/s, loss=0.668, metrics={'acc': 0.7422}]\n",
      "valid: 100%|██████████| 25/25 [00:04<00:00,  5.57it/s, loss=0.659, metrics={'acc': 0.7459}]\n",
      "epoch 13: 100%|██████████| 99/99 [00:07<00:00, 13.54it/s, loss=0.668, metrics={'acc': 0.744}] \n",
      "valid: 100%|██████████| 25/25 [00:04<00:00,  6.07it/s, loss=0.659, metrics={'acc': 0.7483}]\n",
      "epoch 14: 100%|██████████| 99/99 [00:07<00:00, 12.71it/s, loss=0.665, metrics={'acc': 0.7435}]\n",
      "valid: 100%|██████████| 25/25 [00:03<00:00,  6.40it/s, loss=0.659, metrics={'acc': 0.7509}]\n",
      "epoch 15: 100%|██████████| 99/99 [00:09<00:00, 10.91it/s, loss=0.665, metrics={'acc': 0.7435}]\n",
      "valid: 100%|██████████| 25/25 [00:05<00:00,  4.67it/s, loss=0.658, metrics={'acc': 0.7518}]\n",
      "epoch 16: 100%|██████████| 99/99 [00:08<00:00, 11.39it/s, loss=0.664, metrics={'acc': 0.7448}]\n",
      "valid: 100%|██████████| 25/25 [00:03<00:00,  6.67it/s, loss=0.658, metrics={'acc': 0.7514}]\n",
      "epoch 17: 100%|██████████| 99/99 [00:08<00:00, 11.92it/s, loss=0.665, metrics={'acc': 0.743}] \n",
      "valid: 100%|██████████| 25/25 [00:03<00:00,  6.63it/s, loss=0.657, metrics={'acc': 0.7504}]\n",
      "epoch 18: 100%|██████████| 99/99 [00:08<00:00, 12.32it/s, loss=0.664, metrics={'acc': 0.7446}]\n",
      "valid: 100%|██████████| 25/25 [00:03<00:00,  6.57it/s, loss=0.657, metrics={'acc': 0.7522}]\n",
      "epoch 19: 100%|██████████| 99/99 [00:08<00:00, 12.17it/s, loss=0.663, metrics={'acc': 0.7445}]\n",
      "valid: 100%|██████████| 25/25 [00:03<00:00,  6.90it/s, loss=0.658, metrics={'acc': 0.7535}]\n",
      "epoch 20: 100%|██████████| 99/99 [00:07<00:00, 12.86it/s, loss=0.662, metrics={'acc': 0.7437}]\n",
      "valid: 100%|██████████| 25/25 [00:03<00:00,  6.52it/s, loss=0.657, metrics={'acc': 0.7535}]\n",
      "epoch 21: 100%|██████████| 99/99 [00:09<00:00, 10.96it/s, loss=0.662, metrics={'acc': 0.745}] \n",
      "valid: 100%|██████████| 25/25 [00:04<00:00,  5.25it/s, loss=0.657, metrics={'acc': 0.7561}]\n",
      "epoch 22: 100%|██████████| 99/99 [00:08<00:00, 12.37it/s, loss=0.662, metrics={'acc': 0.7451}]\n",
      "valid: 100%|██████████| 25/25 [00:04<00:00,  5.51it/s, loss=0.657, metrics={'acc': 0.7539}]\n",
      "epoch 23: 100%|██████████| 99/99 [00:07<00:00, 13.06it/s, loss=0.661, metrics={'acc': 0.7455}]\n",
      "valid: 100%|██████████| 25/25 [00:04<00:00,  5.33it/s, loss=0.656, metrics={'acc': 0.7544}]\n",
      "epoch 24: 100%|██████████| 99/99 [00:08<00:00, 12.34it/s, loss=0.66, metrics={'acc': 0.748}]  \n",
      "valid: 100%|██████████| 25/25 [00:04<00:00,  5.68it/s, loss=0.657, metrics={'acc': 0.7538}]\n",
      "epoch 25: 100%|██████████| 99/99 [00:07<00:00, 12.41it/s, loss=0.66, metrics={'acc': 0.7464}] \n",
      "valid: 100%|██████████| 25/25 [00:03<00:00,  7.34it/s, loss=0.656, metrics={'acc': 0.7548}]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Epoch: 20. Best val_loss: 0.65658\n",
      "Model weights restored to best epoch: 25\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "predict: 100%|██████████| 25/25 [00:03<00:00,  6.42it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Seed[333] Fold 1 | SCORE: 0.5880548 | WBS: 0.7659362 | F1: 0.4101734\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/54 [00:00<?, ?it/s]/opt/miniconda/lib/python3.12/multiprocessing/popen_fork.py:66: DeprecationWarning: This process (pid=1850157) is multi-threaded, use of fork() may lead to deadlocks in the child.\n",
      "  self.pid = os.fork()\n",
      "predict: 100%|██████████| 54/54 [00:04<00:00, 11.91it/s]\n",
      "/tmp/ipykernel_1850157/641393060.py:180: DeprecationWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, pd.CategoricalDtype) instead\n",
      "  cat_cols = [col for col in train.columns if pd.api.types.is_categorical_dtype(train[col])]\n",
      "/tmp/ipykernel_1850157/641393060.py:180: DeprecationWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, pd.CategoricalDtype) instead\n",
      "  cat_cols = [col for col in train.columns if pd.api.types.is_categorical_dtype(train[col])]\n",
      "/tmp/ipykernel_1850157/166562470.py:38: DeprecationWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, pd.CategoricalDtype) instead\n",
      "  categorical_cols = [col for col in fold_train.columns if pd.api.types.is_categorical_dtype(fold_train[col])]\n",
      "/home/ubuntu/.local/lib/python3.12/site-packages/pytorch_widedeep/preprocessing/tab_preprocessor.py:364: UserWarning: Continuous columns will not be normalised\n",
      "  warnings.warn(\"Continuous columns will not be normalised\")\n",
      "  0%|          | 0/99 [00:00<?, ?it/s]/opt/miniconda/lib/python3.12/multiprocessing/popen_fork.py:66: DeprecationWarning: This process (pid=1850157) is multi-threaded, use of fork() may lead to deadlocks in the child.\n",
      "  self.pid = os.fork()\n",
      "epoch 1: 100%|██████████| 99/99 [00:08<00:00, 12.20it/s, loss=0.872, metrics={'acc': 0.6475}]\n",
      "valid: 100%|██████████| 25/25 [00:06<00:00,  4.15it/s, loss=0.726, metrics={'acc': 0.7377}]\n",
      "epoch 2:  11%|█         | 11/99 [00:05<00:42,  2.06it/s, loss=0.749, metrics={'acc': 0.7282}]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 216\u001b[0m\n\u001b[1;32m    203\u001b[0m trainer \u001b[38;5;241m=\u001b[39m Trainer(\n\u001b[1;32m    204\u001b[0m     model\u001b[38;5;241m=\u001b[39mmodel, \n\u001b[1;32m    205\u001b[0m     objective\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbinary\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;66;03m### 이거를 regression으로 바꾸는 게 나은 것인지를 모르겠음\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    212\u001b[0m     seed\u001b[38;5;241m=\u001b[39mseed                     \u001b[38;5;66;03m# 랜덤 시드 설정 (기본값: 1)\u001b[39;00m\n\u001b[1;32m    213\u001b[0m )\n\u001b[1;32m    215\u001b[0m \u001b[38;5;66;03m# 학습: validation 데이터는 별도의 인자로 전달\u001b[39;00m\n\u001b[0;32m--> 216\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    217\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX_train\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# ✅ Training 데이터를 딕셔너리로 \u001b[39;49;00m\n\u001b[1;32m    218\u001b[0m \u001b[43m    \u001b[49m\u001b[43mn_epochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m100\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m####\u001b[39;49;00m\n\u001b[1;32m    219\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1024\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m    220\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX_val\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mX_val\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# ✅ Validation 데이터를 딕셔너리로 전달\u001b[39;49;00m\n\u001b[1;32m    221\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    225\u001b[0m \u001b[38;5;66;03m# 모델 학습 후 Validation 예측 코드:\u001b[39;00m\n\u001b[1;32m    226\u001b[0m \u001b[38;5;66;03m# 이미 확률이 계산되어 있는 컬럼을 사용합니다.\u001b[39;00m\n\u001b[1;32m    227\u001b[0m valid_probs \u001b[38;5;241m=\u001b[39m trainer\u001b[38;5;241m.\u001b[39mpredict_proba(X_wide\u001b[38;5;241m=\u001b[39mX_wide_valid, X_tab\u001b[38;5;241m=\u001b[39mX_tab_valid)[:,\u001b[38;5;241m1\u001b[39m] \u001b[38;5;66;03m####\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.12/site-packages/pytorch_widedeep/utils/general_utils.py:62\u001b[0m, in \u001b[0;36malias.<locals>.decorator.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     60\u001b[0m         kwargs[original_name] \u001b[38;5;241m=\u001b[39m kwargs\u001b[38;5;241m.\u001b[39mpop(alt_name)\n\u001b[1;32m     61\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[0;32m---> 62\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.12/site-packages/pytorch_widedeep/utils/general_utils.py:62\u001b[0m, in \u001b[0;36malias.<locals>.decorator.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     60\u001b[0m         kwargs[original_name] \u001b[38;5;241m=\u001b[39m kwargs\u001b[38;5;241m.\u001b[39mpop(alt_name)\n\u001b[1;32m     61\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[0;32m---> 62\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.12/site-packages/pytorch_widedeep/utils/general_utils.py:62\u001b[0m, in \u001b[0;36malias.<locals>.decorator.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     60\u001b[0m         kwargs[original_name] \u001b[38;5;241m=\u001b[39m kwargs\u001b[38;5;241m.\u001b[39mpop(alt_name)\n\u001b[1;32m     61\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[0;32m---> 62\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.12/site-packages/pytorch_widedeep/training/trainer.py:470\u001b[0m, in \u001b[0;36mTrainer.fit\u001b[0;34m(self, X_wide, X_tab, X_text, X_img, X_train, X_val, val_split, target, n_epochs, validation_freq, batch_size, train_dataloader, eval_dataloader, feature_importance_sample_size, finetune, **kwargs)\u001b[0m\n\u001b[1;32m    462\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcallback_container\u001b[38;5;241m.\u001b[39mon_train_begin(\n\u001b[1;32m    463\u001b[0m     {\n\u001b[1;32m    464\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbatch_size\u001b[39m\u001b[38;5;124m\"\u001b[39m: batch_size,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    467\u001b[0m     }\n\u001b[1;32m    468\u001b[0m )\n\u001b[1;32m    469\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(n_epochs):\n\u001b[0;32m--> 470\u001b[0m     epoch_logs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_train_epoch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepoch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    471\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m eval_loader \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m epoch \u001b[38;5;241m%\u001b[39m validation_freq \u001b[38;5;241m==\u001b[39m (\n\u001b[1;32m    472\u001b[0m         validation_freq \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    473\u001b[0m     ):\n\u001b[1;32m    474\u001b[0m         epoch_logs, on_epoch_end_metric \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_eval_epoch(\n\u001b[1;32m    475\u001b[0m             eval_loader, epoch_logs\n\u001b[1;32m    476\u001b[0m         )\n",
      "File \u001b[0;32m~/.local/lib/python3.12/site-packages/pytorch_widedeep/training/trainer.py:905\u001b[0m, in \u001b[0;36mTrainer._train_epoch\u001b[0;34m(self, train_loader, epoch)\u001b[0m\n\u001b[1;32m    903\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m batch_idx, (data, targett) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(t, train_loader):\n\u001b[1;32m    904\u001b[0m     t\u001b[38;5;241m.\u001b[39mset_description(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mepoch \u001b[39m\u001b[38;5;132;01m%i\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m (epoch \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m))\n\u001b[0;32m--> 905\u001b[0m     train_score, train_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_train_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtargett\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_idx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    906\u001b[0m     print_loss_and_metric(t, train_loss, train_score)\n\u001b[1;32m    907\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcallback_container\u001b[38;5;241m.\u001b[39mon_batch_end(batch\u001b[38;5;241m=\u001b[39mbatch_idx)\n",
      "File \u001b[0;32m~/.local/lib/python3.12/site-packages/pytorch_widedeep/training/trainer.py:937\u001b[0m, in \u001b[0;36mTrainer._train_step\u001b[0;34m(self, data, target, batch_idx)\u001b[0m\n\u001b[1;32m    933\u001b[0m y \u001b[38;5;241m=\u001b[39m to_device(y, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[1;32m    935\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m--> 937\u001b[0m y_pred \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    939\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mis_tabnet:\n\u001b[1;32m    940\u001b[0m     loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mloss_fn(y_pred[\u001b[38;5;241m0\u001b[39m], y) \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlambda_sparse \u001b[38;5;241m*\u001b[39m y_pred[\u001b[38;5;241m1\u001b[39m]\n",
      "File \u001b[0;32m~/.local/lib/python3.12/site-packages/torch/nn/modules/module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.12/site-packages/torch/nn/modules/module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/.local/lib/python3.12/site-packages/pytorch_widedeep/models/wide_deep.py:212\u001b[0m, in \u001b[0;36mWideDeep.forward\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m    210\u001b[0m     deep \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_deephead(X, wide_out)\n\u001b[1;32m    211\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 212\u001b[0m     deep \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_forward_deep\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwide_out\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    214\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menforce_positive:\n\u001b[1;32m    215\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menf_pos(deep)\n",
      "File \u001b[0;32m~/.local/lib/python3.12/site-packages/pytorch_widedeep/models/wide_deep.py:314\u001b[0m, in \u001b[0;36mWideDeep._forward_deep\u001b[0;34m(self, X, wide_out)\u001b[0m\n\u001b[1;32m    312\u001b[0m         wide_out\u001b[38;5;241m.\u001b[39madd_(tab_out)\n\u001b[1;32m    313\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 314\u001b[0m         wide_out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_forward_component\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    315\u001b[0m \u001b[43m            \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdeeptabular\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdeeptabular\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwide_out\u001b[49m\n\u001b[1;32m    316\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    318\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdeeptext \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    319\u001b[0m     wide_out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_component(X, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdeeptext, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdeeptext\u001b[39m\u001b[38;5;124m\"\u001b[39m, wide_out)\n",
      "File \u001b[0;32m~/.local/lib/python3.12/site-packages/pytorch_widedeep/models/wide_deep.py:380\u001b[0m, in \u001b[0;36mWideDeep._forward_component\u001b[0;34m(self, X, component, component_type, wide_out)\u001b[0m\n\u001b[1;32m    376\u001b[0m     component_out \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39madd(  \u001b[38;5;66;03m# type: ignore[call-overload]\u001b[39;00m\n\u001b[1;32m    377\u001b[0m         \u001b[38;5;241m*\u001b[39m[cp(X[component_type][i]) \u001b[38;5;28;01mfor\u001b[39;00m i, cp \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(component)]\n\u001b[1;32m    378\u001b[0m     )\n\u001b[1;32m    379\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 380\u001b[0m     component_out \u001b[38;5;241m=\u001b[39m \u001b[43mcomponent\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m[\u001b[49m\u001b[43mcomponent_type\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    382\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m wide_out\u001b[38;5;241m.\u001b[39madd_(component_out)\n",
      "File \u001b[0;32m~/.local/lib/python3.12/site-packages/torch/nn/modules/module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.12/site-packages/torch/nn/modules/module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/.local/lib/python3.12/site-packages/torch/nn/modules/container.py:250\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    248\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[1;32m    249\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[0;32m--> 250\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    251\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[0;32m~/.local/lib/python3.12/site-packages/torch/nn/modules/module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.12/site-packages/torch/nn/modules/module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/.local/lib/python3.12/site-packages/pytorch_widedeep/models/tabular/transformers/ft_transformer.py:320\u001b[0m, in \u001b[0;36mFTTransformer.forward\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    318\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, X: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m    319\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_embeddings(X)\n\u001b[0;32m--> 320\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoder\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    321\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwith_cls_token:\n\u001b[1;32m    322\u001b[0m         x \u001b[38;5;241m=\u001b[39m x[:, \u001b[38;5;241m0\u001b[39m, :]\n",
      "File \u001b[0;32m~/.local/lib/python3.12/site-packages/torch/nn/modules/module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.12/site-packages/torch/nn/modules/module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/.local/lib/python3.12/site-packages/torch/nn/modules/container.py:250\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    248\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[1;32m    249\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[0;32m--> 250\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    251\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[0;32m~/.local/lib/python3.12/site-packages/torch/nn/modules/module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.12/site-packages/torch/nn/modules/module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/.local/lib/python3.12/site-packages/pytorch_widedeep/models/tabular/transformers/_encoders.py:137\u001b[0m, in \u001b[0;36mFTTransformerEncoder.forward\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    135\u001b[0m     x \u001b[38;5;241m=\u001b[39m X \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mattn(X)\n\u001b[1;32m    136\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 137\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mattn_normadd\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mattn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    138\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mff_normadd(x, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mff)\n",
      "File \u001b[0;32m~/.local/lib/python3.12/site-packages/torch/nn/modules/module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.12/site-packages/torch/nn/modules/module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/.local/lib/python3.12/site-packages/pytorch_widedeep/models/tabular/transformers/_attention_layers.py:52\u001b[0m, in \u001b[0;36mNormAdd.forward\u001b[0;34m(self, X, sublayer)\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, X: Tensor, sublayer: nn\u001b[38;5;241m.\u001b[39mModule) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m---> 52\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m X \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout(\u001b[43msublayer\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mln\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[0;32m~/.local/lib/python3.12/site-packages/torch/nn/modules/module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.12/site-packages/torch/nn/modules/module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/.local/lib/python3.12/site-packages/pytorch_widedeep/models/tabular/transformers/_attention_layers.py:248\u001b[0m, in \u001b[0;36mLinearAttentionLinformer.forward\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    246\u001b[0m attn_weights \u001b[38;5;241m=\u001b[39m scores\u001b[38;5;241m.\u001b[39msoftmax(dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m    247\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mattn_weights \u001b[38;5;241m=\u001b[39m attn_weights\n\u001b[0;32m--> 248\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[43meinsum\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mb h s k, b h k d -> b h s d\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdropout\u001b[49m\u001b[43m(\u001b[49m\u001b[43mattn_weights\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mv\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    249\u001b[0m output \u001b[38;5;241m=\u001b[39m einops\u001b[38;5;241m.\u001b[39mrearrange(output, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb h s d -> b s (h d)\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    251\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mout_proj \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/.local/lib/python3.12/site-packages/torch/functional.py:407\u001b[0m, in \u001b[0;36meinsum\u001b[0;34m(*args)\u001b[0m\n\u001b[1;32m    402\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m einsum(equation, \u001b[38;5;241m*\u001b[39m_operands)\n\u001b[1;32m    404\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(operands) \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m2\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m opt_einsum\u001b[38;5;241m.\u001b[39menabled:\n\u001b[1;32m    405\u001b[0m     \u001b[38;5;66;03m# the path for contracting 0 or 1 time(s) is already optimized\u001b[39;00m\n\u001b[1;32m    406\u001b[0m     \u001b[38;5;66;03m# or the user has disabled using opt_einsum\u001b[39;00m\n\u001b[0;32m--> 407\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_VF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43meinsum\u001b[49m\u001b[43m(\u001b[49m\u001b[43mequation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moperands\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# type: ignore[attr-defined]\u001b[39;00m\n\u001b[1;32m    409\u001b[0m path \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    410\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m opt_einsum\u001b[38;5;241m.\u001b[39mis_available():\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import time\n",
    "start_time = time.time()\n",
    "\n",
    "\n",
    "\n",
    "# 교차 검증 설정: seed_list를 [333, 777], n_splits=5\n",
    "seed_list = [333, 777]\n",
    "n_splits = 5\n",
    "\n",
    "total_score, total_wbs_score, total_f1_score = [], [], []\n",
    "test_preds = []\n",
    "\n",
    "\n",
    "# 교차 검증 시작\n",
    "for seed in seed_list:\n",
    "    set_seed(seed)\n",
    "    # train, test 불러오기\n",
    "    train = pd.read_csv(train_path).drop(columns=[\"ID\"])\n",
    "    test = pd.read_csv(test_path).drop(columns=[\"ID\"])\n",
    "\n",
    "    kf = KFold(n_splits=n_splits, shuffle=True, random_state=seed)\n",
    "    scores, wbs_scores, f1_scores = [], [], []\n",
    "    \n",
    "    for fold, (train_idx, valid_idx) in enumerate(kf.split(train)):\n",
    "        # Fold 데이터 생성\n",
    "        fold_train, fold_valid = train.iloc[train_idx], train.iloc[valid_idx]\n",
    "        fold_train2 = fold_train.copy()\n",
    "        fold_test = test.copy()  # test 데이터는 별도 사용\n",
    "        \n",
    "        # 전처리 \n",
    "        fold_train, fold_valid = all_process(fold_train, fold_valid)\n",
    "        _, fold_test = all_process(fold_train2, fold_test)\n",
    "\n",
    "        # 범주형, 연속형 열 구분분\n",
    "        categorical_cols = [col for col in fold_train.columns if pd.api.types.is_categorical_dtype(fold_train[col])]\n",
    "        continuous_cols = [col for col in fold_train.columns if col not in cat_cols and col != '임신 성공 확률']\n",
    "\n",
    "\n",
    "        # Wide 부분: 원-핫 인코딩 (crossed_cols 옵션도 사용할 수 있음)\n",
    "        wide_cols = categorical_cols  # Wide 모델에는 범주형 변수를 원-핫 인코딩으로 처리\n",
    "        crossed_cols = []  # 필요시 두 개 이상의 컬럼을 교차시켜 상호작용 feature 생성\n",
    "\n",
    "        # WidePreprocessor (원-핫 인코딩, crossed_cols 사용 가능)\n",
    "        wide_preprocessor = WidePreprocessor(wide_cols=wide_cols, crossed_cols=crossed_cols)\n",
    "\n",
    "        # Deep 부분: 임베딩 + 연속형 변수 처리\n",
    "        tab_preprocessor = TabPreprocessor(embed_cols=categorical_cols, continuous_cols=continuous_cols)\n",
    "\n",
    "        # 전처리: 각 Fold 별로 Wide & Deep 데이터 생성\n",
    "        X_wide_train = wide_preprocessor.fit_transform(fold_train)\n",
    "        X_wide_valid = wide_preprocessor.transform(fold_valid)\n",
    "        X_wide_test = wide_preprocessor.transform(fold_test)\n",
    "        X_tab_train = tab_preprocessor.fit_transform(fold_train)\n",
    "        X_tab_valid = tab_preprocessor.transform(fold_valid)\n",
    "        X_tab_test = tab_preprocessor.transform(fold_test)\n",
    "\n",
    "        # Target 값: 정수형 (0,1)\n",
    "        y_train = fold_train['임신 성공 확률'].astype(int).values\n",
    "        y_valid = fold_valid['임신 성공 확률'].astype(int).values\n",
    "\n",
    "        # Wide 모델: 입력 차원은 원-핫 인코딩된 피처 수####\n",
    "        wide_model = Wide(input_dim=int(X_wide_train.max().item()), pred_dim=1)\n",
    "\n",
    "        \n",
    "        # Deep 모델: FTTransformer 사용\n",
    "        tab_model = FTTransformer(\n",
    "            column_idx=tab_preprocessor.column_idx,  # 각 컬럼의 인덱스 정보\n",
    "            cat_embed_input=tab_preprocessor.cat_embed_input,  # (컬럼명, 고유값 수, 임베딩 차원)\n",
    "\n",
    "            continuous_cols=continuous_cols,  # 연속형 변수 리스트\n",
    "\n",
    "            # ▶ 범주형 임베딩 관련 설정\n",
    "            cat_embed_dropout=0.0,  # 범주형 임베딩 드롭아웃\n",
    "            # 후보 값: 0.0 (기본), 0.1, 0.2, 0.3\n",
    "\n",
    "\n",
    "            cat_embed_activation=None,  # 범주형 임베딩 활성화 함수\n",
    "            # 후보 값: None (기본), 'relu', 'leaky_relu', 'tanh', 'gelu'\n",
    "\n",
    "\n",
    "            # ▶ 연속형 변수 관련 설정\n",
    "            cont_norm_layer=\"batchnorm\",  # 연속형 변수 정규화 방식\n",
    "            # 후보 값: None (사용 안함), 'batchnorm' (기본 추천), 'layernorm'\n",
    "\n",
    "            embed_continuous_method=\"standard\",  # 연속형 변수 임베딩 방식\n",
    "            # 후보 값: 'standard' (기본), 'periodic', 'piecewise'\n",
    "\n",
    "            cont_embed_dropout=0.0,  # 연속형 임베딩 드롭아웃\n",
    "            # 후보 값: 0.0 (기본), 0.1, 0.2, 0.3\n",
    "\n",
    "            cont_embed_activation='relu',  # 연속형 임베딩 활성화 함수\n",
    "            # 후보 값: None (기본), 'relu', 'leaky_relu', 'tanh', 'gelu'\n",
    "\n",
    "            quantization_setup=None,  # 'piecewise' 방식에서 구간 경계 지정\n",
    "            # 후보 값: None (기본), {\"age\": [20, 30, 40], \"income\": [10000, 30000, 60000]}\n",
    "\n",
    "            # ▶ 기타 임베딩 관련\n",
    "            full_embed_dropout=False,  # 전체 임베딩을 dropout할지 여부\n",
    "            # 후보 값: False (기본), True (더 강한 정규화)\n",
    "\n",
    "            # ▶ FTTransformer 구조 설정\n",
    "            input_dim=64,  # 임베딩 차원 수 (카테고리 + 연속형 임베딩 포함)\n",
    "            # 후보 값: 32, 64 (기본), 128\n",
    "\n",
    "\n",
    "            n_heads=8,  # Attention 헤드 수\n",
    "            # 후보 값: 4, 8 (기본), 16\n",
    "\n",
    "\n",
    "            n_blocks=4,  # Transformer block 수\n",
    "            # 후보 값: 2 (얕은 모델), 4 (기본), 6 (깊은 모델)\n",
    "\n",
    "            attn_dropout=0.2,  # Attention dropout\n",
    "            # 후보 값: 0.0, 0.1, 0.2 (기본), 0.3\n",
    "\n",
    "####\n",
    "\n",
    "            transformer_activation=\"reglu\",  # Transformer 내부 활성화 함수\n",
    "            # 후보 값: 'relu', 'gelu', 'leaky_relu', 'tanh', 'geglu', 'reglu' (기본)\n",
    "\n",
    "            # ▶ MLP 설정 (선택 사항)\n",
    "            mlp_hidden_dims=[64, 32],  # MLP 은닉층 크기\n",
    "            # 후보 값: [64, 32] (기본), [128, 64], [256, 128], None (사용 안 함)\n",
    "\n",
    "            mlp_activation=\"relu\",  # MLP 활성화 함수\n",
    "            # 후보 값: 'relu' (기본), 'leaky_relu', 'tanh', 'gelu'\n",
    "\n",
    "            mlp_dropout=0.1,  # MLP dropout\n",
    "            # 후보 값: 0.0, 0.1 (기본), 0.3\n",
    "\n",
    "            mlp_batchnorm=False,  # MLP에 배치 정규화 적용 여부\n",
    "            # 후보 값: False (기본), True\n",
    "\n",
    "            mlp_batchnorm_last=False,  # MLP 마지막 층에도 BN 적용 여부\n",
    "            # 후보 값: False (기본), True\n",
    "\n",
    "        )\n",
    "        \n",
    "        # Wide & Deep 모델 결합\n",
    "        model = WideDeep(wide=wide_model, deeptabular=tab_model)\n",
    "        \n",
    "        # 옵티마이저 및 학습률 스케줄러 설정\n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr=0.0001)  # Set your desired learning rate here\n",
    "        \n",
    "\n",
    "        # ✅ Training 데이터를 딕셔너리 형태로 생성\n",
    "        X_train = {\n",
    "            \"X_wide\": X_wide_train,\n",
    "            \"X_tab\": X_tab_train,\n",
    "            \"target\": y_train\n",
    "        }\n",
    "\n",
    "        # ✅ Validation 데이터를 딕셔너리로 전달 (X_val 방식 사용)\n",
    "        X_val = {\n",
    "            \"X_wide\": X_wide_valid,\n",
    "            \"X_tab\": X_tab_valid,\n",
    "            \"target\": y_valid\n",
    "        }\n",
    "        \n",
    "        # EarlyStopping 콜백 설정 (patience=5, min_delta=0.001)\n",
    "        early_stopping = EarlyStopping(\n",
    "            monitor=\"val_loss\", \n",
    "            patience=5, \n",
    "            min_delta=0.001, \n",
    "            verbose=1\n",
    "        )\n",
    "        \n",
    "        # 현재 날짜/시간을 포함한 파일 이름 생성\n",
    "        timestamp = datetime.datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "        model_path = f\"saved_models/best_model_{timestamp}.pt\"\n",
    "        \n",
    "        # ✅ 2. ModelCheckpoint: 최상의 모델을 자동 저장\n",
    "        model_checkpoint = ModelCheckpoint(\n",
    "            filepath=model_path,  # 모델 저장 경로\n",
    "            monitor=\"val_loss\",        # 감시할 지표 ('val_loss' 또는 'val_acc')\n",
    "            save_best_only=True        # 가장 좋은 성능의 모델만 저장\n",
    "        )\n",
    "\n",
    "\n",
    "\n",
    "        # 클래스 불균형을 반영하기 위한 변수\n",
    "        y_train_binary = (y_train > 0.5).astype(int)\n",
    "        positive_count = y_train_binary.sum()\n",
    "        negative_count = len(y_train_binary) - positive_count\n",
    "        \n",
    "        pos_weight = torch.tensor(negative_count / positive_count)  # 기본\n",
    "        \n",
    "        # sqrt 적용:\n",
    "        # pos_weight = torch.sqrt(torch.tensor(negative_count / positive_count))\n",
    "        \n",
    "        # # log 적용:\n",
    "        # pos_weight = torch.log1p(torch.tensor(negative_count / positive_count))\n",
    "        # ✅ custom loss: BCEWithLogitsLoss (pos_weight 사용 안 함)\n",
    "        custom_loss = nn.BCEWithLogitsLoss(pos_weight=pos_weight) # 클래스 비율을 반영\n",
    "\n",
    "        # Trainer 생성: objective \"binary\"로 설정, 평가 지표로 Accuracy, AUROC, F1Score 사용\n",
    "        trainer = Trainer(\n",
    "            model=model, \n",
    "            objective=\"binary\", ### 이거를 regression으로 바꾸는 게 나은 것인지를 모르겠음\n",
    "            custom_loss_function=custom_loss,  # 기본 손실 함수 사용 (필요 시 사용자 정의 함수 가능)\n",
    "            optimizers=optimizer,       # 옵티마이저 (기본값: Adam)\n",
    "            initializers=None,          # 가중치 초기화 없음 (기본값)\n",
    "            callbacks=[early_stopping, model_checkpoint],  # 조기 종료 (3 에포크 동안 개선 없으면 종료)\n",
    "            metrics=[Accuracy()],\n",
    "            verbose=0,                  # 학습 로그 출력 (기본값: 1)\n",
    "            seed=seed                     # 랜덤 시드 설정 (기본값: 1)\n",
    "        )\n",
    "        \n",
    "        # 학습: validation 데이터는 별도의 인자로 전달\n",
    "        trainer.fit(\n",
    "            X_train=X_train,  # ✅ Training 데이터를 딕셔너리로 \n",
    "            n_epochs=100,  ####\n",
    "            batch_size=1024, \n",
    "            X_val=X_val  # ✅ Validation 데이터를 딕셔너리로 전달\n",
    "        )\n",
    "        \n",
    "        \n",
    "        \n",
    "        # 모델 학습 후 Validation 예측 코드:\n",
    "        # 이미 확률이 계산되어 있는 컬럼을 사용합니다.\n",
    "        valid_probs = trainer.predict_proba(X_wide=X_wide_valid, X_tab=X_tab_valid)[:,1] ####\n",
    "        # valid_probs = trainer.predict(X_wide=X_wide_valid, X_tab=X_tab_valid)\n",
    "        # valid_probs = np.clip(valid_probs, 0, 1) ##### regression 일때\n",
    "        valid_pred = (valid_probs > 0.5).astype(int)\n",
    "\n",
    "        # DataFrame 으로 변환\n",
    "        valid_probs_df = pd.DataFrame({'prob': valid_probs})\n",
    "            \n",
    "        \n",
    "\n",
    "        # 실제 정답 \n",
    "        y_valid = fold_valid['임신 성공 확률'].values.astype(int)\n",
    "        \n",
    "\n",
    "        \n",
    "        # 대회 평가 지표\n",
    "        fold_score = competition_metric(y_valid, valid_probs)\n",
    "        fold_wbs = weighted_brier_score(y_valid, valid_probs)\n",
    "        fold_f1 = f1_score(y_valid, valid_probs)\n",
    "\n",
    "        \n",
    "        print(f\"Seed[{seed:<3}] Fold {fold + 1} | SCORE: {fold_score:.7f} | WBS: {fold_wbs:.7f} | F1: {fold_f1:.7f}\")\n",
    "\n",
    "        \n",
    "        scores.append(fold_score)\n",
    "        wbs_scores.append(fold_wbs)\n",
    "        f1_scores.append(fold_f1)\n",
    "        \n",
    "        total_score.append(fold_score)\n",
    "        total_wbs_score.append(fold_wbs)\n",
    "        total_f1_score.append(f1_scores)\n",
    "        \n",
    "        # Test 데이터 예측 (각 fold의 모델로 예측한 결과 저장)\n",
    "        test_pred = trainer.predict_proba(X_wide=X_wide_test, X_tab=X_tab_test)[:,1]\n",
    "        # test_pred = trainer.predict(X_wide=X_wide_test, X_tab=X_tab_test)\n",
    "        # test_pred = np.clip(test_pred, 0, 1) ##### regression 일때\n",
    "        test_preds.append(test_pred)\n",
    "    \n",
    "    # Fold 별 평균 성능 출력\n",
    "    avg_score = np.mean(scores)\n",
    "    avg_wbs = np.mean(wbs_scores)\n",
    "    avg_f1 = np.mean(f1_scores)\n",
    "    \n",
    "    print(\"-\" * 80)\n",
    "    print(f\"Seed[{seed:<3}] Average Metrics | SCORE: {avg_score:.7f} | WBS: {avg_wbs:.7f} | F1: {avg_f1:.7f}\")\n",
    "    print(\"-\" * 80)\n",
    "\n",
    "# 전체 Validation 평균 성능 출력\n",
    "val_score = np.mean(total_score)\n",
    "val_wbs = np.mean(total_wbs_score)\n",
    "val_f1 = np.mean(total_f1_score)\n",
    "\n",
    "\n",
    "print(\"-\" * 80)\n",
    "print(f\"Validation Average Metrics | SCORE: {val_score:.7f} | WBS: {val_wbs:.7f} | F1: {val_f1:.7f}\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "finish_time = time.time()\n",
    "total_time = finish_time - start_time \n",
    "\n",
    "print(total_time)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Validation Average Metrics | SCORE: 0.6339700 | WBS: 0.7970266 | F1: 0.4709135"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 열 제거: 0.3477212\n",
    "- 열 모두 살리고 0으로 채우기 : 0.4411126 (object: binary)\n",
    "- 열 모두 살리고 0으로 채우기 : 0.4041905 (object: regression)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp_prediction = pd.DataFrame({'FTT': np.mean(test_preds, axis=0)})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TabFastFormer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_path = f'../data/train.csv'\n",
    "test_path = f'../data/test.csv'\n",
    "sample_path = f'../data/sample_submission.csv'\n",
    "\n",
    "\n",
    "train = pd.read_csv(train_path).drop(columns=[\"ID\"])\n",
    "test = pd.read_csv(test_path).drop(columns=[\"ID\"])\n",
    "print(train.shape, test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def drop_cols_with_na(train_df, val_df):\n",
    "    # 나중에 결측치 대체하면서 반영할 예정\n",
    "\n",
    "    cat_cols_with_na = [\n",
    "        '이전 총 임신 횟수',\n",
    "        '이전 총 임신 성공 횟수',\n",
    "\n",
    "        '총 생성 배아 수', ## 여기부터 100% DI\n",
    "        '저장된 배아 수',\n",
    "        '채취된 신선 난자 수',\n",
    "        '수정 시도된 난자 수'\n",
    "    ]\n",
    "\n",
    "    numeric_cols_with_na = [\n",
    "        '이식된 배아 수', ## only DI\n",
    "        '미세주입(ICSI) 배아 이식 수',\n",
    "        '배아 이식 후 경과일',\n",
    "    ]\n",
    "    train_df = train_df.drop(columns=cat_cols_with_na)\n",
    "    train_df = train_df.drop(columns=numeric_cols_with_na)\n",
    "    val_df = val_df.drop(columns=cat_cols_with_na)\n",
    "    val_df = val_df.drop(columns=numeric_cols_with_na)\n",
    "    return train_df, val_df\n",
    "\n",
    "\n",
    "def 시술유형(train, test):\n",
    "    train['세부 시술 유형'] = train['세부 시술 유형'].fillna(\"Unknown\")\n",
    "    test['세부 시술 유형'] = test['세부 시술 유형'].fillna(\"Unknown\")\n",
    "\n",
    "    def categorize_procedure(proc):\n",
    "        tokens = [token.strip() for token in proc.split(\",\") if token.strip() and not token.strip().isdigit()]\n",
    "        # 우선순위에 따른 범주화\n",
    "        if tokens.count(\"Unknown\") >= 1:\n",
    "            return \"Unknown\"\n",
    "        if tokens.count(\"AH\") >= 1:\n",
    "            return \"AH\"\n",
    "        if tokens.count(\"BLASTOCYST\") >= 1:\n",
    "            return \"BLASTOCYST\"\n",
    "        if tokens.count(\"ICSI\") >= 2 or tokens.count(\"IVF\") >= 2:\n",
    "            return \"2ICSI_2IVF\"\n",
    "        if tokens.count(\"IVF\") >= 1 and tokens.count(\"ICSI\") >= 1:\n",
    "            return \"IVF_ICSI\"\n",
    "        if tokens == \"ICSI\":\n",
    "            return \"ICSI\"\n",
    "        if tokens == \"IVF\":\n",
    "            return \"IVF\"\n",
    "        return \",\".join(tokens) if tokens else None\n",
    "\n",
    "    for df in [train, test]:\n",
    "        df['세부 시술 유형'] = df['세부 시술 유형'].str.replace(\" / \", \",\")\n",
    "        df['세부 시술 유형'] = df['세부 시술 유형'].str.replace(\":\", \",\")\n",
    "        df['세부 시술 유형'] = df['세부 시술 유형'].str.replace(\" \", \"\")\n",
    "\n",
    "    counts = train['세부 시술 유형'].value_counts()\n",
    "    allowed_categories = counts[counts >= 100].index.tolist()\n",
    "\n",
    "    # allowed_categories에 속하지 않는 값은 \"Unknown\"으로 대체\n",
    "    train.loc[~train['세부 시술 유형'].isin(allowed_categories), '세부 시술 유형'] = \"Unknown\"\n",
    "    test.loc[~test['세부 시술 유형'].isin(allowed_categories), '세부 시술 유형'] = \"Unknown\"\n",
    "\n",
    "    train['세부 시술 유형'] = train['세부 시술 유형'].apply(categorize_procedure)\n",
    "    test['세부 시술 유형'] = test['세부 시술 유형'].apply(categorize_procedure)\n",
    "\n",
    "    train['시술유형_통합'] = train['시술 유형'].astype(str) + '_' + train['세부 시술 유형'].astype(str)\n",
    "    test['시술유형_통합'] = test['시술 유형'].astype(str) + '_' + test['세부 시술 유형'].astype(str)\n",
    "\n",
    "    drop_cols = ['시술 유형', '세부 시술 유형']\n",
    "    train = train.drop(drop_cols, axis=1)\n",
    "    test = test.drop(drop_cols, axis=1)\n",
    "\n",
    "    return train, test\n",
    "\n",
    "def 횟수_to_int(df_train, df_val):\n",
    "    for col in [col for col in df_train.columns if '횟수' in col]:\n",
    "        df_train[col] = df_train[col].replace({'6회 이상': '6회'})\n",
    "        df_val[col] = df_val[col].replace({'6회 이상': '6회'})\n",
    "\n",
    "        #### 일단 0으로 채움\n",
    "        df_train[col] = df_train[col].fillna('0')\n",
    "        df_val[col] = df_val[col].fillna('0')\n",
    "        ####\n",
    "\n",
    "        \n",
    "        df_train[col] = df_train[col].str[0].astype(int)\n",
    "        df_val[col] = df_val[col].str[0].astype(int)\n",
    "\n",
    "    return df_train, df_val\n",
    "\n",
    "\n",
    "def 임신_IVF(df_train, df_val):\n",
    "    for col in [col for col in df_train.columns if '횟수' in col]:\n",
    "        df_train[col] = df_train[col].replace({'6회 이상': '6회'})\n",
    "        df_val[col] = df_val[col].replace({'6회 이상': '6회'})\n",
    "        mode_value = df_train[col].mode()[0]\n",
    "\n",
    "        df_train[col] = df_train[col].fillna(mode_value)\n",
    "        df_val[col] = df_val[col].fillna(mode_value)\n",
    "\n",
    "        # 문자열의 첫 글자를 추출 후 int형으로 변환\n",
    "        df_train[col] = df_train[col].str[0].astype(int)\n",
    "        df_val[col] = df_val[col].str[0].astype(int)\n",
    "\n",
    "    df_train['임신_IVF'] = df_train['이전 총 임신 횟수'] - df_train['이전 IVF 시술 횟수']\n",
    "    df_val['임신_IVF'] = df_val['이전 총 임신 횟수'] - df_val['이전 IVF 시술 횟수']\n",
    "    # df_train = df_train.drop('이전 시술 횟수', axis=1)\n",
    "    return df_train, df_val\n",
    "\n",
    "def 난자기증자나이(df_train, df_test):\n",
    "    mapping = {\n",
    "        '만20세 이하': 20,\n",
    "        '만21-25세': 25,\n",
    "        '만26-30세': 30,\n",
    "        '만31-35세': 35,\n",
    "        '알 수 없음': 20,  # 만20세 이하와 동일하게 처리\n",
    "    }\n",
    "    df_train['난자 기증자 나이'] = df_train['난자 기증자 나이'].replace(mapping)\n",
    "    df_test['난자 기증자 나이'] = df_test['난자 기증자 나이'].replace(mapping)\n",
    "    return df_train, df_test\n",
    "\n",
    "\n",
    "def 단일배아이식여부(df_train, df_val):\n",
    "    df_train['단일 배아 이식 여부'] = df_train['단일 배아 이식 여부'].fillna(0)\n",
    "    df_val['단일 배아 이식 여부'] = df_val['단일 배아 이식 여부'].fillna(0)\n",
    "    return df_train, df_val\n",
    "\n",
    "\n",
    "def 이전_총_임신_성공_횟수(train, test):\n",
    "    train['이전 총 임신 횟수'] = train['이전 총 임신 횟수'].fillna(train['이전 총 임신 횟수'].mode()[0])\n",
    "    test['이전 총 임신 횟수'] = test['이전 총 임신 횟수'].fillna(train['이전 총 임신 횟수'].mode()[0])\n",
    "\n",
    "    train['이전 총 임신 성공 횟수'] = train['이전 총 임신 성공 횟수'].fillna(train['이전 총 임신 성공 횟수'].mode()[0])\n",
    "    test['이전 총 임신 성공 횟수'] = test['이전 총 임신 성공 횟수'].fillna(train['이전 총 임신 성공 횟수'].mode()[0])\n",
    "\n",
    "def 독립범주로보기(train, test):\n",
    "    cols = ['총 생성 배아 수', '저장된 배아 수', '채취된 신선 난자 수', '수정 시도된 난자 수']\n",
    "    for col in cols:\n",
    "        train[col] = train[col].fillna('NAN')\n",
    "        test[col] = test[col].fillna('NAN')\n",
    "\n",
    "\n",
    "    \n",
    "def label_encoding(train, test, cols):\n",
    "    encoder = LabelEncoder()\n",
    "    for col in cols:\n",
    "        train[col] = encoder.fit_transform(train[col])\n",
    "        test[col] = encoder.transform(test[col])\n",
    "    return train, test\n",
    "\n",
    "def type_to_category(train, test, cols):\n",
    "    train[cols] = train[cols].astype('category')\n",
    "    test[cols] = test[cols].astype('category')\n",
    "    return train, test\n",
    "\n",
    "def impute_nan(train, test):\n",
    "    cat_cols_with_na = [\n",
    "        '이전 총 임신 횟수',\n",
    "        '이전 총 임신 성공 횟수',\n",
    "\n",
    "        '총 생성 배아 수', ## 여기부터 100% DI\n",
    "        '저장된 배아 수',\n",
    "        '채취된 신선 난자 수',\n",
    "        '수정 시도된 난자 수'\n",
    "    ]\n",
    "\n",
    "    numeric_cols_with_na = [\n",
    "        '이식된 배아 수', ## only DI\n",
    "        '미세주입(ICSI) 배아 이식 수',\n",
    "        '배아 이식 후 경과일',\n",
    "    ]\n",
    "    cols_to_impute= cat_cols_with_na + numeric_cols_with_na\n",
    "    \n",
    "    for col in cols_to_impute:\n",
    "        train[col] = train[col].fillna(0)\n",
    "        test[col] = test[col].fillna(0)\n",
    "\n",
    "    return train, test\n",
    "\n",
    "\n",
    "def num_feature_scailing(train, test, seed=777):\n",
    "    cat_cols = [col for col in train.columns if pd.api.types.is_categorical_dtype(train[col])]\n",
    "    numeric_cols = [col for col in train.columns if col not in cat_cols and col != '임신 성공 확률']\n",
    "    # bin_cols 들도 동일하게 스케일링\n",
    "\n",
    "\n",
    "    arr_train = train[numeric_cols].to_numpy()  # DataFrame -> NumPy\n",
    "    arr_train = arr_train.astype(np.float32)\n",
    "    arr_test = test[numeric_cols].to_numpy()\n",
    "    arr_test = arr_test.astype(np.float32)\n",
    "\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "    noise = (\n",
    "        np.random.default_rng(0)\n",
    "        .normal(0.0, 1e-5, arr_train.shape)\n",
    "        .astype(arr_train.dtype)\n",
    "    )\n",
    "    preprocessing = QuantileTransformer(\n",
    "        n_quantiles=max(min(len(train[numeric_cols]) // 30, 1000), 10),\n",
    "        output_distribution='normal',\n",
    "        subsample=10**9,\n",
    "    ).fit(arr_train + noise)\n",
    "\n",
    "    train[numeric_cols] = preprocessing.transform(arr_train)\n",
    "    test[numeric_cols] = preprocessing.transform(arr_test)\n",
    "    return train, test\n",
    "\n",
    "\n",
    "def check_cols(train,val):\n",
    "\n",
    "    cols_without_na = train.columns\n",
    "    categorical_columns = ['환자 시술 당시 나이', '시술 유형', '세부 시술 유형', '이전 IVF 시술 횟수',\n",
    "       '이전 DI 시술 횟수', '이전 총 임신 횟수', '이전 총 임신 성공 횟수', '총 생성 배아 수', '저장된 배아 수',\n",
    "       '해동된 배아 수', '채취된 신선 난자 수', '수정 시도된 난자 수', '난자 출처', '정자 출처', '난자 기증자 나이',\n",
    "       '정자 기증자 나이']\n",
    "\n",
    "    numeric_columns = ['이식된 배아 수','미세주입(ICSI) 배아 이식 수','배아 이식 후 경과일']\n",
    "\n",
    "    bool_features = ['배란 자극 시술 여부', '단일 배아 이식 여부', '불임 원인 - 난관 질환',\n",
    "    '불임 원인 - 배란 장애', '불임 원인 - 남성 요인', '불임 원인 - 자궁내막증', '불임 원인 - 불명확', '해동 난자 사용 여부',\n",
    "    '신선 난자 사용 여부', '동결 배아 사용 여부', '신선 배아 사용 여부', '기증 배아 사용 여부', '착상 전 PGD 시행 여부', '착상 전 PGS 시행 여부']\n",
    "    \n",
    "    categorical_columns_cleaned = [col for col in categorical_columns if col in cols_without_na]\n",
    "    numeric_columns_cleaned = [col for col in numeric_columns if col in cols_without_na]\n",
    "    bool_features_cleaned = [col for col in bool_features if col in cols_without_na]\n",
    "    \n",
    "    # print(\"Categorical columns:\", categorical_columns_cleaned, \"\\n count\", len(categorical_columns_cleaned))\n",
    "    # print(\"Numeric columns:\", numeric_columns_cleaned, \"\\n count\", len(numeric_columns_cleaned))\n",
    "    # print(\"Boolean columns:\", bool_features_cleaned, \"\\n count\", len(bool_features_cleaned))\n",
    "\n",
    "    return categorical_columns_cleaned, numeric_columns_cleaned, bool_features_cleaned\n",
    "\n",
    "\n",
    "def drop_single_value_columns(df_train, df_test):\n",
    "    cols_to_drop = [col for col in df_train.columns if df_train[col].nunique() == 1]\n",
    "    return df_train.drop(columns=cols_to_drop), df_test.drop(columns=cols_to_drop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def all_process(train, val):\n",
    "    # train, val = drop_cols_with_na(train, val)\n",
    "\n",
    "    # 기본 전처리 단계\n",
    "    train, val = 횟수_to_int(train, val)\n",
    "\n",
    "    train, val = 시술유형(train, val)\n",
    "    # train, val = 임신_IVF(train, val)\n",
    "\n",
    "    train, val = 단일배아이식여부(train, val)\n",
    "    이전_총_임신_성공_횟수(train, val)\n",
    "    독립범주로보기(train, val)\n",
    "    \n",
    "    cols_to_encoding = [\n",
    "        \"환자 시술 당시 나이\",\n",
    "        # \"클리닉 내 총 시술 횟수\",\n",
    "        # \"IVF 시술 횟수\",\n",
    "        # \"DI 시술 횟수\",\n",
    "        # \"총 임신 횟수\",\n",
    "        # \"IVF 임신 횟수\",\n",
    "        # \"DI 임신 횟수\",\n",
    "        # \"총 출산 횟수\",\n",
    "        # \"IVF 출산 횟수\",\n",
    "        # \"DI 출산 횟수\",\n",
    "        \"난자 출처\",\n",
    "        \"정자 출처\",\n",
    "        \"난자 기증자 나이\",\n",
    "        \"정자 기증자 나이\",\n",
    "        '시술유형_통합',\n",
    "\n",
    "        '해동된 배아 수', # 원래 int였는데 범주형으로 바뀜\n",
    "        '총 생성 배아 수',\n",
    "        '저장된 배아 수', \n",
    "        '채취된 신선 난자 수', \n",
    "        '수정 시도된 난자 수'\n",
    "        \n",
    "\n",
    "    ]\n",
    "    \n",
    "    train, val = label_encoding(train, val, cols=cols_to_encoding)\n",
    "    train, val = type_to_category(train, val, cols=cols_to_encoding)\n",
    "\n",
    "    train, val = impute_nan(train, val)\n",
    "    train, val = num_feature_scailing(train, val)\n",
    "\n",
    "    train, val = drop_single_value_columns(train, val)\n",
    "\n",
    "    return train, val\n",
    "\n",
    "train = pd.read_csv(train_path).drop(columns=[\"ID\"])\n",
    "test = pd.read_csv(test_path).drop(columns=[\"ID\"])\n",
    "\n",
    "train, test = all_process(train, test)\n",
    "\n",
    "cat_cols = [col for col in train.columns if pd.api.types.is_categorical_dtype(train[col])]\n",
    "numeric_cols = [col for col in train.columns if col not in cat_cols and col != '임신 성공 확률']\n",
    "\n",
    "print(f'수치형 변수: {len(numeric_cols)}개 \\n{numeric_cols}')\n",
    "print(f'범주형 변수: {len(cat_cols)}개 \\n{cat_cols}')\n",
    "print(train.shape, test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "start_time = time.time()\n",
    "\n",
    "\n",
    "\n",
    "# 교차 검증 설정: seed_list를 [333] 하나만 사용, n_splits=3\n",
    "seed_list = [333]\n",
    "n_splits = 5\n",
    "\n",
    "total_score, total_wbs_score, total_f1_score = [], [], []\n",
    "test_preds = []\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# 교차 검증 시작\n",
    "for seed in seed_list:\n",
    "    set_seed(seed)\n",
    "    # train, test 불러오기\n",
    "    train = pd.read_csv(train_path).drop(columns=[\"ID\"])\n",
    "    test = pd.read_csv(test_path).drop(columns=[\"ID\"])\n",
    "\n",
    "    kf = KFold(n_splits=5, shuffle=True, random_state=seed)\n",
    "    scores, wbs_scores, f1_scores = [], [], []\n",
    "    \n",
    "    for fold, (train_idx, valid_idx) in enumerate(kf.split(train)):\n",
    "        # Fold 데이터 생성\n",
    "        fold_train, fold_valid = train.iloc[train_idx], train.iloc[valid_idx]\n",
    "        fold_train2 = fold_train.copy()\n",
    "        fold_test = test.copy()  # test 데이터는 별도 사용\n",
    "        \n",
    "        # 전처리 \n",
    "        fold_train, fold_valid = all_process(fold_train, fold_valid)\n",
    "        _, fold_test = all_process(fold_train2, fold_test)\n",
    "\n",
    "        # 범주형, 연속형 열 구분분\n",
    "        categorical_cols = [col for col in fold_train.columns if pd.api.types.is_categorical_dtype(fold_train[col])]\n",
    "        continuous_cols = [col for col in fold_train.columns if col not in cat_cols and col != '임신 성공 확률']\n",
    "\n",
    "\n",
    "        # Wide 부분: 원-핫 인코딩 (crossed_cols 옵션도 사용할 수 있음)\n",
    "        wide_cols = categorical_cols  # Wide 모델에는 범주형 변수를 원-핫 인코딩으로 처리\n",
    "        crossed_cols = []  # 필요시 두 개 이상의 컬럼을 교차시켜 상호작용 feature 생성\n",
    "\n",
    "        # WidePreprocessor (원-핫 인코딩, crossed_cols 사용 가능)\n",
    "        wide_preprocessor = WidePreprocessor(wide_cols=wide_cols, crossed_cols=crossed_cols)\n",
    "\n",
    "        # Deep 부분: 임베딩 + 연속형 변수 처리\n",
    "        tab_preprocessor = TabPreprocessor(embed_cols=categorical_cols, continuous_cols=continuous_cols)\n",
    "\n",
    "        # 전처리: 각 Fold 별로 Wide & Deep 데이터 생성\n",
    "        X_wide_train = wide_preprocessor.fit_transform(fold_train)\n",
    "        X_wide_valid = wide_preprocessor.transform(fold_valid)\n",
    "        X_wide_test = wide_preprocessor.transform(fold_test)\n",
    "        X_tab_train = tab_preprocessor.fit_transform(fold_train)\n",
    "        X_tab_valid = tab_preprocessor.transform(fold_valid)\n",
    "        X_tab_test = tab_preprocessor.transform(fold_test)\n",
    "\n",
    "        # Target 값: 정수형 (0,1)\n",
    "        y_train = fold_train['임신 성공 확률'].astype(int).values\n",
    "        y_valid = fold_valid['임신 성공 확률'].astype(int).values\n",
    "\n",
    "        # Wide 모델: 입력 차원은 원-핫 인코딩된 피처 수####\n",
    "        wide_model = Wide(input_dim=int(X_wide_train.max().item()), pred_dim=1)\n",
    "\n",
    "\n",
    "        \n",
    "        # Deep 모델: TabFastFormer 사용\n",
    "        tab_model = TabFastFormer(\n",
    "            column_idx=tab_preprocessor.column_idx,  # 각 컬럼의 인덱스 정보\n",
    "            cat_embed_input=tab_preprocessor.cat_embed_input,  # (컬럼명, 고유값 수, 임베딩 차원)\n",
    "\n",
    "            continuous_cols=continuous_cols,  # 연속형 변수 리스트\n",
    "\n",
    "            # ▶ 범주형 임베딩 설정\n",
    "            cat_embed_dropout=0.0,  # 범주형 임베딩 드롭아웃\n",
    "            # 후보 값: 0.0 (기본), 0.1, 0.2, 0.3\n",
    "\n",
    "            use_cat_bias=False,  # 범주형 임베딩에 bias 사용 여부\n",
    "            # 후보 값: False (기본), True\n",
    "\n",
    "            cat_embed_activation=None,  # 범주형 임베딩 활성화 함수\n",
    "            # 후보 값: None (기본), 'relu', 'tanh', 'leaky_relu', 'gelu'\n",
    "\n",
    "\n",
    "            # ▶ 연속형 변수 설정\n",
    "            cont_norm_layer='batchnorm',  # 연속형 변수 정규화 방식\n",
    "            # 후보 값: None, 'batchnorm', 'layernorm'\n",
    "\n",
    "            embed_continuous_method='standard',  # 연속형 변수 임베딩 방식\n",
    "            # 후보 값: 'standard' (기본), 'piecewise', 'periodic'\n",
    "\n",
    "            cont_embed_dropout=0.0,  # 연속형 임베딩 드롭아웃\n",
    "            # 후보 값: 0.0 (기본), 0.1, 0.3\n",
    "\n",
    "            cont_embed_activation='relu',  # 연속형 임베딩 활성화 함수\n",
    "            # 후보 값: None (기본), 'relu', 'leaky_relu', 'gelu'\n",
    "\n",
    "            quantization_setup=None,  # piecewise 임베딩 시 구간 설정\n",
    "            # 후보 값: None (기본), {'age': [20, 30, 40], 'income': [10000, 30000]}\n",
    "\n",
    "\n",
    "            full_embed_dropout=False,  # 전체 임베딩 dropout 여부\n",
    "            # 후보 값: False (기본), True\n",
    "\n",
    "            # ▶ FastFormer 구조 설정\n",
    "            input_dim=32,  # 임베딩 차원\n",
    "            # 후보 값: 32 (기본), 64, 128\n",
    "\n",
    "            n_heads=8,  # 어텐션 헤드 개수\n",
    "            # 후보 값: 4, 8 (기본), 16\n",
    "\n",
    "            use_bias=False,  # QKV에 bias 적용 여부\n",
    "            # 후보 값: False (기본), True\n",
    "\n",
    "            n_blocks=4,  # FastFormer 블록 수\n",
    "            # 후보 값: 2, 4 (기본), 6\n",
    "\n",
    "            attn_dropout=0.1,  # 어텐션 드롭아웃\n",
    "            # 후보 값: 0.0, 0.1 (기본), 0.2\n",
    "\n",
    "\n",
    "            transformer_activation=\"relu\",  # Transformer 내부 활성화 함수\n",
    "            # 후보 값: 'relu' (기본), 'gelu', 'geglu', 'reglu', 'tanh', 'leaky_relu'\n",
    "\n",
    "            # ▶ MLP 추가 설정 (선택적)\n",
    "            mlp_hidden_dims=[64, 32],  # MLP 은닉층 크기\n",
    "            # 후보 값: [64, 32] (기본), [128, 64], None (사용 안 함)\n",
    "\n",
    "            mlp_activation=\"relu\",  # MLP 활성화 함수\n",
    "            # 후보 값: 'relu' (기본), 'gelu', 'leaky_relu', 'tanh'\n",
    "\n",
    "            mlp_dropout=0.1,  # MLP dropout\n",
    "            # 후보 값: 0.0, 0.1 (기본), 0.3\n",
    "\n",
    "            mlp_batchnorm=False,  # MLP에 배치 정규화 사용 여부\n",
    "            # 후보 값: False (기본), True\n",
    "\n",
    "            mlp_batchnorm_last=False,  # 마지막 층에도 배치 정규화\n",
    "            # 후보 값: False (기본), True\n",
    "\n",
    "            mlp_linear_first=True  # MLP 연산 순서\n",
    "            # 후보 값: True (기본), False\n",
    "        )\n",
    "        \n",
    "        # Wide & Deep 모델 결합\n",
    "        model = WideDeep(wide=wide_model, deeptabular=tab_model)\n",
    "        \n",
    "        # 옵티마이저 및 학습률 스케줄러 설정\n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr=0.0001)  # Set your desired learning rate here\n",
    "        \n",
    "\n",
    "        # ✅ Training 데이터를 딕셔너리 형태로 생성\n",
    "        X_train = {\n",
    "            \"X_wide\": X_wide_train,\n",
    "            \"X_tab\": X_tab_train,\n",
    "            \"target\": y_train\n",
    "        }\n",
    "\n",
    "        # ✅ Validation 데이터를 딕셔너리로 전달 (X_val 방식 사용)\n",
    "        X_val = {\n",
    "            \"X_wide\": X_wide_valid,\n",
    "            \"X_tab\": X_tab_valid,\n",
    "            \"target\": y_valid\n",
    "        }\n",
    "        \n",
    "        # EarlyStopping 콜백 설정 (patience=5, min_delta=0.001)\n",
    "        early_stopping = EarlyStopping(\n",
    "            monitor=\"val_loss\", \n",
    "            patience=5, \n",
    "            min_delta=0.001, \n",
    "            verbose=1\n",
    "        )\n",
    "        \n",
    "        # 현재 날짜/시간을 포함한 파일 이름 생성\n",
    "        timestamp = datetime.datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "        model_path = f\"saved_models/best_model_{timestamp}.pt\"\n",
    "        \n",
    "        # ✅ 2. ModelCheckpoint: 최상의 모델을 자동 저장\n",
    "        model_checkpoint = ModelCheckpoint(\n",
    "            filepath=model_path,  # 모델 저장 경로\n",
    "            monitor=\"val_loss\",        # 감시할 지표 ('val_loss' 또는 'val_acc')\n",
    "            save_best_only=True        # 가장 좋은 성능의 모델만 저장\n",
    "        )\n",
    "\n",
    "\n",
    "\n",
    "        # 클래스 불균형을 반영하기 위한 변수\n",
    "        y_train_binary = (y_train > 0.5).astype(int)\n",
    "        positive_count = y_train_binary.sum()\n",
    "        negative_count = len(y_train_binary) - positive_count\n",
    "        \n",
    "        pos_weight = torch.tensor(negative_count / positive_count)  # 기본\n",
    "        \n",
    "        # sqrt 적용:\n",
    "        # pos_weight = torch.sqrt(torch.tensor(negative_count / positive_count))\n",
    "        \n",
    "        # # log 적용:\n",
    "        # pos_weight = torch.log1p(torch.tensor(negative_count / positive_count))\n",
    "        # ✅ custom loss: BCEWithLogitsLoss \n",
    "        custom_loss = nn.BCEWithLogitsLoss(pos_weight=pos_weight) # 클래스 비율을 반영\n",
    "\n",
    "        # Trainer 생성: objective \"binary\"로 설정, 평가 지표로 Accuracy, AUROC, F1Score 사용\n",
    "        trainer = Trainer(\n",
    "            model=model, \n",
    "            objective=\"binary\", ### 이거를 regression으로 바꾸는 게 나은 것인지를 모르겠음\n",
    "            custom_loss_function=custom_loss,  # 기본 손실 함수 사용 (필요 시 사용자 정의 함수 가능)\n",
    "            optimizers=optimizer,       # 옵티마이저 (기본값: Adam)\n",
    "            initializers=None,          # 가중치 초기화 없음 (기본값)\n",
    "            callbacks=[early_stopping, model_checkpoint],  # 조기 종료 (3 에포크 동안 개선 없으면 종료)\n",
    "            metrics=[Accuracy()],\n",
    "            verbose=1,                  # 학습 로그 출력 (기본값: 1)\n",
    "            seed=seed                     # 랜덤 시드 설정 (기본값: 1)\n",
    "        )\n",
    "        \n",
    "        # 학습: validation 데이터는 별도의 인자로 전달\n",
    "        trainer.fit(\n",
    "            X_train=X_train,  # ✅ Training 데이터를 딕셔너리로 \n",
    "            n_epochs=100,  ####\n",
    "            batch_size=1024, \n",
    "            X_val=X_val  # ✅ Validation 데이터를 딕셔너리로 전달\n",
    "        )\n",
    "        \n",
    "        \n",
    "        \n",
    "        # 모델 학습 후 Validation 예측 코드:\n",
    "        # 이미 확률이 계산되어 있는 컬럼을 사용합니다.\n",
    "        valid_probs = trainer.predict_proba(X_wide=X_wide_valid, X_tab=X_tab_valid)[:,1] ####\n",
    "        # valid_probs = trainer.predict(X_wide=X_wide_valid, X_tab=X_tab_valid)\n",
    "        # valid_probs = np.clip(valid_probs, 0, 1) ##### regression 일때\n",
    "        valid_pred = (valid_probs > 0.5).astype(int)\n",
    "        \n",
    "        \n",
    "\n",
    "        # 실제 정답 \n",
    "        y_valid = fold_valid['임신 성공 확률'].values.astype(int)\n",
    "        \n",
    "\n",
    "        \n",
    "        # 대회 평가 지표\n",
    "        fold_score = competition_metric(y_valid, valid_probs)\n",
    "        fold_wbs = weighted_brier_score(y_valid, valid_probs)\n",
    "        fold_f1 = f1_score(y_valid, valid_probs)\n",
    "\n",
    "        \n",
    "        print(f\"Seed[{seed:<3}] Fold {fold + 1} | SCORE: {fold_score:.7f} | WBS: {fold_wbs:.7f} | F1: {fold_f1:.7f}\")\n",
    "\n",
    "        \n",
    "        scores.append(fold_score)\n",
    "        wbs_scores.append(fold_wbs)\n",
    "        f1_scores.append(fold_f1)\n",
    "        \n",
    "        total_score.append(fold_score)\n",
    "        total_wbs_score.append(fold_wbs)\n",
    "        total_f1_score.append(f1_scores)\n",
    "        \n",
    "        # Test 데이터 예측 (각 fold의 모델로 예측한 결과 저장)\n",
    "        test_pred = trainer.predict_proba(X_wide=X_wide_test, X_tab=X_tab_test)[:,1]\n",
    "        # test_pred = trainer.predict(X_wide=X_wide_test, X_tab=X_tab_test)\n",
    "        # test_pred = np.clip(test_pred, 0, 1) ##### regression 일때\n",
    "        test_preds.append(test_pred)\n",
    "    \n",
    "    # Fold 별 평균 성능 출력\n",
    "    avg_score = np.mean(scores)\n",
    "    avg_wbs = np.mean(wbs_scores)\n",
    "    avg_f1 = np.mean(f1_scores)\n",
    "    \n",
    "    print(\"-\" * 80)\n",
    "    print(f\"Seed[{seed:<3}] Average Metrics | SCORE: {avg_score:.7f} | WBS: {avg_wbs:.7f} | F1: {avg_f1:.7f}\")\n",
    "    print(\"-\" * 80)\n",
    "\n",
    "# 전체 Validation 평균 성능 출력\n",
    "val_score = np.mean(total_score)\n",
    "val_wbs = np.mean(total_wbs_score)\n",
    "val_f1 = np.mean(total_f1_score)\n",
    "\n",
    "\n",
    "print(\"-\" * 80)\n",
    "print(f\"Validation Average Metrics | SCORE: {val_score:.7f} | WBS: {val_wbs:.7f} | F1: {val_f1:.7f}\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "finish_time = time.time()\n",
    "total_time = finish_time - start_time \n",
    "\n",
    "print(total_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp_predictions['FastTabTransformer'] = np.mean(test_preds, axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SoftVoting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp_prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_prediction = (np.array(tmp_prediction['FTT']) + np.array(tmp_prediction['FastTabTransformer']))/2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>임신 성공 확률</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>TEST_00000</td>\n",
       "      <td>0.098759</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>TEST_00001</td>\n",
       "      <td>0.624725</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>TEST_00002</td>\n",
       "      <td>0.427147</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>TEST_00003</td>\n",
       "      <td>0.569354</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>TEST_00004</td>\n",
       "      <td>0.314667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54407</th>\n",
       "      <td>TEST_54407</td>\n",
       "      <td>0.710529</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54408</th>\n",
       "      <td>TEST_54408</td>\n",
       "      <td>0.000194</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54409</th>\n",
       "      <td>TEST_54409</td>\n",
       "      <td>0.632851</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54410</th>\n",
       "      <td>TEST_54410</td>\n",
       "      <td>0.890026</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54411</th>\n",
       "      <td>TEST_54411</td>\n",
       "      <td>0.357063</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>54412 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "               ID  임신 성공 확률\n",
       "0      TEST_00000  0.098759\n",
       "1      TEST_00001  0.624725\n",
       "2      TEST_00002  0.427147\n",
       "3      TEST_00003  0.569354\n",
       "4      TEST_00004  0.314667\n",
       "...           ...       ...\n",
       "54407  TEST_54407  0.710529\n",
       "54408  TEST_54408  0.000194\n",
       "54409  TEST_54409  0.632851\n",
       "54410  TEST_54410  0.890026\n",
       "54411  TEST_54411  0.357063\n",
       "\n",
       "[54412 rows x 2 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_submission = pd.read_csv(sample_path)\n",
    "\n",
    "sample_submission['임신 성공 확률'] = final_prediction\n",
    "\n",
    "sample_submission\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_submission.to_csv('./Submission/FTT_FastTabTrans_n_splits=5.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
