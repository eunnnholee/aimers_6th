{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-05T09:01:59.503541Z",
     "start_time": "2025-04-05T09:01:58.702342Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "from sklearn import metrics\n",
    "\n",
    "def f1_score(true_prob, pred_prob):\n",
    "    true_binary = (np.array(true_prob) > 0.5).astype(int)\n",
    "    pred_binary = (np.array(pred_prob) > 0.5).astype(int)\n",
    "    return metrics.f1_score(true_binary, pred_binary)\n",
    "\n",
    "def weighted_brier_score(true_prob, pred_prob, alpha=4):\n",
    "    weights = 1 + alpha * true_prob + np.abs(0.5 - true_prob) ** 2\n",
    "    brier = np.sum(weights * (true_prob - pred_prob) ** 2) / np.sum(weights)\n",
    "    adjusted_brier = max(0, 1 - brier)\n",
    "    return adjusted_brier\n",
    "\n",
    "def competition_metric(true_prob, pred_prob):\n",
    "    true_prob = np.array(true_prob)\n",
    "    pred_prob = np.array(pred_prob)\n",
    "\n",
    "    if true_prob.shape != pred_prob.shape:\n",
    "        raise ValueError(\"예측값과 정답값의 shape이 일치하지 않습니다.\")\n",
    "    if np.isnan(pred_prob).any():\n",
    "        raise ValueError(\"예측값에 NaN이 포함되어 있습니다.\")\n",
    "    if not ((0 <= pred_prob) & (pred_prob <= 1)).all():\n",
    "        raise ValueError(\"예측값이 0~1 범위를 벗어났습니다.\")\n",
    "    if not np.isfinite(pred_prob).all():\n",
    "        raise ValueError(\"예측값에 inf 또는 -inf가 포함되어 있습니다.\")\n",
    "\n",
    "    wbs = weighted_brier_score(true_prob, pred_prob)\n",
    "    f1 = f1_score(true_prob, pred_prob)\n",
    "    score = 0.5 * wbs + 0.5 * f1\n",
    "    return score"
   ],
   "id": "d406c4d14ae6f52",
   "outputs": [],
   "execution_count": 1
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-04-05T09:02:01.018680Z",
     "start_time": "2025-04-05T09:01:59.507540Z"
    }
   },
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "import math\n",
    "\n",
    "from sklearn.model_selection import StratifiedKFold, KFold, train_test_split\n",
    "from sklearn.metrics import roc_auc_score, mean_squared_error, accuracy_score\n",
    "\n",
    "import scipy\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch import Tensor\n",
    "from tabm_reference import Model, make_parameter_groups\n",
    "import rtdl_num_embeddings\n",
    "from tqdm import tqdm\n",
    "from typing import Literal, NamedTuple\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(action='ignore')\n",
    "\n",
    "from LG_Aimers_6th.cal_auc import calculate_auc\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-05T09:02:01.573265Z",
     "start_time": "2025-04-05T09:02:01.147126Z"
    }
   },
   "cell_type": "code",
   "source": [
    "train_path = '../offline_data/train_aimers_6th_offline.csv'\n",
    "test_path = '../offline_data/test_aimers_6th_offline.csv'\n",
    "sample_path = '../offline_data/sample_submission_aimers_6th_offline.csv'\n",
    "\n",
    "train = pd.read_csv(train_path, encoding='utf-8-sig').drop(columns=['ID'])\n",
    "test = pd.read_csv(test_path, encoding='utf-8-sig').drop(columns=['ID'])\n",
    "\n",
    "task_type = 'regression'\n",
    "\n",
    "print(train.shape, test.shape)"
   ],
   "id": "7fe6db4fa3d6801f",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(126244, 34) (54412, 33)\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-05T09:02:01.618474Z",
     "start_time": "2025-04-05T09:02:01.589267Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from sklearn.preprocessing import LabelEncoder, StandardScaler, QuantileTransformer, MultiLabelBinarizer\n",
    "from sklearn.impute import SimpleImputer\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "import warnings\n",
    "warnings.filterwarnings(action='ignore')\n",
    "\n",
    "def drop_cols_with_na(train_df, val_df):\n",
    "    # 나중에 결측치 대체하면서 반영할 예정\n",
    "\n",
    "    cat_cols_with_na = [\n",
    "        '이전 총 임신 횟수',\n",
    "        '이전 총 임신 성공 횟수',\n",
    "\n",
    "        '총 생성 배아 수', ## 여기부터 100% DI\n",
    "        '저장된 배아 수',\n",
    "        '채취된 신선 난자 수',\n",
    "        '수정 시도된 난자 수'\n",
    "    ]\n",
    "\n",
    "    numeric_cols_with_na = [\n",
    "        '이식된 배아 수', ## only DI\n",
    "        '미세주입(ICSI) 배아 이식 수',\n",
    "        '배아 이식 후 경과일',\n",
    "    ]\n",
    "    train_df = train_df.drop(columns=cat_cols_with_na)\n",
    "    train_df = train_df.drop(columns=numeric_cols_with_na)\n",
    "    val_df = val_df.drop(columns=cat_cols_with_na)\n",
    "    val_df = val_df.drop(columns=numeric_cols_with_na)\n",
    "    return train_df, val_df\n",
    "\n",
    "\n",
    "def 시술유형(train, test):\n",
    "    train['세부 시술 유형'] = train['세부 시술 유형'].fillna(\"Unknown\")\n",
    "    test['세부 시술 유형'] = test['세부 시술 유형'].fillna(\"Unknown\")\n",
    "\n",
    "    def categorize_procedure(proc):\n",
    "        tokens = [token.strip() for token in proc.split(\",\") if token.strip() and not token.strip().isdigit()]\n",
    "        # 우선순위에 따른 범주화\n",
    "        if tokens.count(\"Unknown\") >= 1:\n",
    "            return \"Unknown\"\n",
    "        if tokens.count(\"AH\") >= 1:\n",
    "            return \"AH\"\n",
    "        if tokens.count(\"BLASTOCYST\") >= 1:\n",
    "            return \"BLASTOCYST\"\n",
    "        if tokens.count(\"ICSI\") >= 2 or tokens.count(\"IVF\") >= 2:\n",
    "            return \"2ICSI_2IVF\"\n",
    "        if tokens.count(\"IVF\") >= 1 and tokens.count(\"ICSI\") >= 1:\n",
    "            return \"IVF_ICSI\"\n",
    "        if tokens == \"ICSI\":\n",
    "            return \"ICSI\"\n",
    "        if tokens == \"IVF\":\n",
    "            return \"IVF\"\n",
    "        return \",\".join(tokens) if tokens else None\n",
    "\n",
    "    for df in [train, test]:\n",
    "        df['세부 시술 유형'] = df['세부 시술 유형'].str.replace(\" / \", \",\")\n",
    "        df['세부 시술 유형'] = df['세부 시술 유형'].str.replace(\":\", \",\")\n",
    "        df['세부 시술 유형'] = df['세부 시술 유형'].str.replace(\" \", \"\")\n",
    "\n",
    "    counts = train['세부 시술 유형'].value_counts()\n",
    "    allowed_categories = counts[counts >= 100].index.tolist()\n",
    "\n",
    "    # allowed_categories에 속하지 않는 값은 \"Unknown\"으로 대체\n",
    "    train.loc[~train['세부 시술 유형'].isin(allowed_categories), '세부 시술 유형'] = \"Unknown\"\n",
    "    test.loc[~test['세부 시술 유형'].isin(allowed_categories), '세부 시술 유형'] = \"Unknown\"\n",
    "\n",
    "    train['세부 시술 유형'] = train['세부 시술 유형'].apply(categorize_procedure)\n",
    "    test['세부 시술 유형'] = test['세부 시술 유형'].apply(categorize_procedure)\n",
    "\n",
    "    train['시술유형_통합'] = train['시술 유형'].astype(str) + '_' + train['세부 시술 유형'].astype(str)\n",
    "    test['시술유형_통합'] = test['시술 유형'].astype(str) + '_' + test['세부 시술 유형'].astype(str)\n",
    "\n",
    "    drop_cols = ['시술 유형', '세부 시술 유형']\n",
    "    train = train.drop(drop_cols, axis=1)\n",
    "    test = test.drop(drop_cols, axis=1)\n",
    "\n",
    "    return train, test\n",
    "\n",
    "def 횟수_to_int(df_train, df_val):\n",
    "    for col in [col for col in df_train.columns if '횟수' in col]:\n",
    "        df_train[col] = df_train[col].replace({'6회 이상': '6회'})\n",
    "        df_val[col] = df_val[col].replace({'6회 이상': '6회'})\n",
    "\n",
    "        df_train[col] = df_train[col].str[0].astype(int)\n",
    "        df_val[col] = df_val[col].str[0].astype(int)\n",
    "\n",
    "    return df_train, df_val\n",
    "\n",
    "def 임신_IVF(df_train, df_val):\n",
    "    for col in [col for col in df_train.columns if '횟수' in col]:\n",
    "        df_train[col] = df_train[col].replace({'6회 이상': '6회'})\n",
    "        df_val[col] = df_val[col].replace({'6회 이상': '6회'})\n",
    "        mode_value = df_train[col].mode()[0]\n",
    "\n",
    "        df_train[col] = df_train[col].fillna(mode_value)\n",
    "        df_val[col] = df_val[col].fillna(mode_value)\n",
    "\n",
    "        # 문자열의 첫 글자를 추출 후 int형으로 변환\n",
    "        df_train[col] = df_train[col].str[0].astype(int)\n",
    "        df_val[col] = df_val[col].str[0].astype(int)\n",
    "\n",
    "    df_train['임신_IVF'] = df_train['이전 총 임신 횟수'] - df_train['이전 IVF 시술 횟수']\n",
    "    df_val['임신_IVF'] = df_val['이전 총 임신 횟수'] - df_val['이전 IVF 시술 횟수']\n",
    "    # df_train = df_train.drop('이전 시술 횟수', axis=1)\n",
    "    return df_train, df_val\n",
    "\n",
    "\n",
    "def 난자기증자나이(df_train, df_test):\n",
    "    mapping = {\n",
    "        '만20세 이하': 20,\n",
    "        '만21-25세': 25,\n",
    "        '만26-30세': 30,\n",
    "        '만31-35세': 35,\n",
    "        '알 수 없음': 20,  # 만20세 이하와 동일하게 처리\n",
    "    }\n",
    "    df_train['난자 기증자 나이'] = df_train['난자 기증자 나이'].replace(mapping)\n",
    "    df_test['난자 기증자 나이'] = df_test['난자 기증자 나이'].replace(mapping)\n",
    "    return df_train, df_test\n",
    "\n",
    "def 단일배아이식여부(df_train, df_val):\n",
    "    df_train['단일 배아 이식 여부'] = df_train['단일 배아 이식 여부'].fillna(0)\n",
    "    df_val['단일 배아 이식 여부'] = df_val['단일 배아 이식 여부'].fillna(0)\n",
    "    return df_train, df_val\n",
    "\n",
    "\n",
    "def 기증자정자와혼합된난자수(df_train, df_test):\n",
    "    df_train[\"기증자 정자와 혼합된 난자 수\"] = df_train[\"기증자 정자와 혼합된 난자 수\"].fillna(2)\n",
    "    df_test[\"기증자 정자와 혼합된 난자 수\"] = df_test[\"기증자 정자와 혼합된 난자 수\"].fillna(2)\n",
    "    return df_train, df_test\n",
    "\n",
    "def label_encoding(train, test, cols):\n",
    "    encoder = LabelEncoder()\n",
    "    for col in cols:\n",
    "        train[col] = encoder.fit_transform(train[col])\n",
    "        test[col] = encoder.transform(test[col])\n",
    "    return train, test\n",
    "\n",
    "def type_to_category(train, test, cols):\n",
    "    train[cols] = train[cols].astype('category')\n",
    "    test[cols] = test[cols].astype('category')\n",
    "    return train, test\n",
    "\n",
    "def impute_nan(train, test):\n",
    "\n",
    "    for col in cols_to_impute:\n",
    "        train[col] = train[col].fillna(0)\n",
    "        test[col] = test[col].fillna(0)\n",
    "\n",
    "    return train, test\n",
    "\n",
    "def num_feature_scailing(train, test, seed=777):\n",
    "    cat_cols = [col for col in train.columns if pd.api.types.is_categorical_dtype(train[col])]\n",
    "    numeric_cols = [col for col in train.columns if col not in cat_cols and col != '임신 성공 확률']\n",
    "    # bin_cols 들도 동일하게 스케일링\n",
    "\n",
    "    arr_train = train[numeric_cols].to_numpy()  # DataFrame -> NumPy\n",
    "    arr_train = arr_train.astype(np.float32)\n",
    "    arr_test = test[numeric_cols].to_numpy()\n",
    "    arr_test = arr_test.astype(np.float32)\n",
    "\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "    noise = (\n",
    "        np.random.default_rng(0)\n",
    "        .normal(0.0, 1e-5, arr_train.shape)\n",
    "        .astype(arr_train.dtype)\n",
    "    )\n",
    "    preprocessing = QuantileTransformer(\n",
    "        n_quantiles=max(min(len(train[numeric_cols]) // 30, 1000), 10),\n",
    "        output_distribution='normal',\n",
    "        subsample=10**9,\n",
    "    ).fit(arr_train + noise)\n",
    "\n",
    "    train[numeric_cols] = preprocessing.transform(arr_train)\n",
    "    test[numeric_cols] = preprocessing.transform(arr_test)\n",
    "    return train, test\n",
    "\n",
    "def drop_single_value_columns(df_train, df_test):\n",
    "    cols_to_drop = [col for col in df_train.columns if df_train[col].nunique() == 1]\n",
    "    return df_train.drop(columns=cols_to_drop), df_test.drop(columns=cols_to_drop)\n",
    "\n",
    "def all_process(train, val):\n",
    "    train, val = drop_cols_with_na(train, val)\n",
    "\n",
    "    # 기본 전처리 단계\n",
    "    train, val = 횟수_to_int(train, val)\n",
    "\n",
    "    train, val = 시술유형(train, val)\n",
    "    # train, val = 임신_IVF(train, val)\n",
    "\n",
    "    train, val = 단일배아이식여부(train, val)\n",
    "\n",
    "    cols_to_encoding = [\n",
    "        \"환자 시술 당시 나이\",\n",
    "        # \"클리닉 내 총 시술 횟수\",\n",
    "        # \"IVF 시술 횟수\",\n",
    "        # \"DI 시술 횟수\",\n",
    "        # \"총 임신 횟수\",\n",
    "        # \"IVF 임신 횟수\",\n",
    "        # \"DI 임신 횟수\",\n",
    "        # \"총 출산 횟수\",\n",
    "        # \"IVF 출산 횟수\",\n",
    "        # \"DI 출산 횟수\",\n",
    "        \"난자 출처\",\n",
    "        \"정자 출처\",\n",
    "        \"난자 기증자 나이\",\n",
    "        \"정자 기증자 나이\",\n",
    "        '시술유형_통합',\n",
    "\n",
    "        '해동된 배아 수', # 원래 int였는데 범주형으로 바뀜\n",
    "\n",
    "    ]\n",
    "    train, val = label_encoding(train, val, cols=cols_to_encoding)\n",
    "    train, val = type_to_category(train, val, cols=cols_to_encoding)\n",
    "\n",
    "    # train, val = impute_nan(train, val)\n",
    "    train, val = num_feature_scailing(train, val)\n",
    "\n",
    "    train, val = drop_single_value_columns(train, val)\n",
    "\n",
    "    return train, val\n"
   ],
   "id": "85640b72e45244de",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-05T09:02:02.824608Z",
     "start_time": "2025-04-05T09:02:01.634474Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# from preprocess_DL_offline import all_process\n",
    "\n",
    "train = pd.read_csv(train_path).drop(columns=['ID'])\n",
    "test = pd.read_csv(test_path).drop(columns=['ID'])\n",
    "\n",
    "train, test = all_process(train, test)\n",
    "\n",
    "train, val = train_test_split(train, test_size=0.2, random_state=42)\n",
    "\n",
    "cat_cols = [col for col in train.columns if pd.api.types.is_categorical_dtype(train[col])]\n",
    "numeric_cols = [col for col in train.columns if col not in cat_cols and col != '임신 성공 확률']\n",
    "\n",
    "print(f'수치형 변수: {len(numeric_cols)}개 \\n{numeric_cols}')\n",
    "print(f'범주형 변수: {len(cat_cols)}개 \\n{cat_cols}')\n",
    "print(train.shape, val.shape, test.shape)"
   ],
   "id": "934211c600af996f",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "수치형 변수: 16개 \n",
      "['배란 자극 시술 여부', '단일 배아 이식 여부', '불임 원인 - 난관 질환', '불임 원인 - 배란 장애', '불임 원인 - 남성 요인', '불임 원인 - 자궁내막증', '불임 원인 - 불명확', '이전 IVF 시술 횟수', '이전 DI 시술 횟수', '해동 난자 사용 여부', '신선 난자 사용 여부', '동결 배아 사용 여부', '신선 배아 사용 여부', '기증 배아 사용 여부', '착상 전 PGD 시행 여부', '착상 전 PGS 시행 여부']\n",
      "범주형 변수: 7개 \n",
      "['환자 시술 당시 나이', '해동된 배아 수', '난자 출처', '정자 출처', '난자 기증자 나이', '정자 기증자 나이', '시술유형_통합']\n",
      "(100995, 24) (25249, 24) (54412, 23)\n"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-05T09:02:02.856611Z",
     "start_time": "2025-04-05T09:02:02.841610Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Device\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)\n",
    "\n",
    "amp_dtype = (\n",
    "    torch.bfloat16\n",
    "    if torch.cuda.is_available() and torch.cuda.is_bf16_supported()\n",
    "    else torch.float16\n",
    "    if torch.cuda.is_available()\n",
    "    else None\n",
    ")\n",
    "# Changing False to True will result in faster training on compatible hardware.\n",
    "amp_enabled = False and amp_dtype is not None\n",
    "grad_scaler = torch.cuda.amp.GradScaler() if amp_dtype is torch.float16 else None  # type: ignore\n",
    "\n",
    "# torch.compile\n",
    "compile_model = False\n",
    "\n",
    "# fmt: off\n",
    "print(\n",
    "    f'Device:        {device.type.upper()}'\n",
    "    f'\\nAMP:           {amp_enabled} (dtype: {amp_dtype})'\n",
    "    f'\\ntorch.compile: {compile_model}'\n",
    ")\n",
    "# fmt: on"
   ],
   "id": "9d1098d1626afa2e",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n",
      "Device:        CUDA\n",
      "AMP:           False (dtype: torch.bfloat16)\n",
      "torch.compile: False\n"
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-05T09:02:03.764765Z",
     "start_time": "2025-04-05T09:02:02.873609Z"
    }
   },
   "cell_type": "code",
   "source": [
    "arch_type = 'tabm-mini'\n",
    "def get_feature_info(train):\n",
    "    n_num_features_ = len(numeric_cols)\n",
    "    cat_cardinalities_ = [train[col].nunique() for col in cat_cols]\n",
    "\n",
    "    return n_num_features_, cat_cardinalities_\n",
    "\n",
    "n_num_features, cat_cardinalities = get_feature_info(train)\n",
    "bins = rtdl_num_embeddings.compute_bins(torch.tensor(train[numeric_cols].values))\n",
    "\n",
    "model = Model(\n",
    "    n_num_features=n_num_features,\n",
    "    cat_cardinalities=cat_cardinalities,\n",
    "    n_classes=None,\n",
    "    backbone={\n",
    "        'type': 'MLP',\n",
    "        'n_blocks': 3 if bins is None else 2,\n",
    "        'd_block': 512,\n",
    "        'dropout': 0.1,\n",
    "    },\n",
    "    bins=bins,\n",
    "    num_embeddings=(\n",
    "        None\n",
    "        if bins is None\n",
    "        else {\n",
    "            'type': 'PiecewiseLinearEmbeddings',\n",
    "            'd_embedding': 16,\n",
    "            'activation': False,\n",
    "            'version': 'B',\n",
    "        }\n",
    "    ),\n",
    "    arch_type=arch_type,\n",
    "    k=32,\n",
    "    share_training_batches=True,\n",
    ").to(device)\n",
    "optimizer = torch.optim.AdamW(make_parameter_groups(model), lr=2e-3, weight_decay=3e-4)\n",
    "\n",
    "if compile_model:\n",
    "    # NOTE\n",
    "    # `torch.compile` is intentionally called without the `mode` argument\n",
    "    # (mode=\"reduce-overhead\" caused issues during training with torch==2.0.1).\n",
    "    model = torch.compile(model)\n",
    "    evaluation_mode = torch.no_grad\n",
    "else:\n",
    "    evaluation_mode = torch.inference_mode"
   ],
   "id": "20c8ee4b21d3459",
   "outputs": [],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-05T09:02:03.811782Z",
     "start_time": "2025-04-05T09:02:03.796769Z"
    }
   },
   "cell_type": "code",
   "source": [
    "@torch.autocast(device.type, enabled=amp_enabled, dtype=amp_dtype)  # type: ignore[code]\n",
    "def apply_model(part: str, idx: Tensor) -> Tensor:\n",
    "    return (\n",
    "        model(\n",
    "            data[part]['x_cont'][idx],\n",
    "            data[part]['x_cat'][idx] if 'x_cat' in data[part] else None,\n",
    "        )\n",
    "        .squeeze(-1)  # Remove the last dimension for regression tasks.\n",
    "        .float()\n",
    "    )\n",
    "\n",
    "\n",
    "base_loss_fn = F.mse_loss if task_type == 'regression' else F.cross_entropy\n",
    "\n",
    "\n",
    "def loss_fn(y_pred: Tensor, y_true: Tensor) -> Tensor:\n",
    "    # TabM produces k predictions. Each of them must be trained separately.\n",
    "    # (regression)     y_pred.shape == (batch_size, k)\n",
    "    # (classification) y_pred.shape == (batch_size, k, n_classes)\n",
    "    k = y_pred.shape[-1 if task_type == 'regression' else -2]\n",
    "    return base_loss_fn(\n",
    "        y_pred.flatten(0, 1),\n",
    "        y_true.repeat_interleave(k) if model.share_training_batches else y_true,\n",
    "    )\n",
    "\n",
    "\n",
    "@evaluation_mode()\n",
    "def evaluate(part: str) -> float:\n",
    "    model.eval()\n",
    "\n",
    "    # When using torch.compile, you may need to reduce the evaluation batch size.\n",
    "    eval_batch_size = 8096\n",
    "    y_pred: np.ndarray = (\n",
    "        torch.cat(\n",
    "            [\n",
    "                apply_model(part, idx)\n",
    "                for idx in torch.arange(len(data[part]['y']), device=device).split(\n",
    "                eval_batch_size\n",
    "            )\n",
    "            ]\n",
    "        )\n",
    "        .cpu()\n",
    "        .numpy()\n",
    "    )\n",
    "    if task_type == 'regression':\n",
    "        # Transform the predictions back to the original label space.\n",
    "        assert regression_label_stats is not None\n",
    "        y_pred = y_pred * regression_label_stats.std + regression_label_stats.mean\n",
    "\n",
    "    # Compute the mean of the k predictions.\n",
    "    if task_type != 'regression':\n",
    "        # For classification, the mean must be computed in the probabily space.\n",
    "        y_pred = scipy.special.softmax(y_pred, axis=-1)\n",
    "    y_pred = y_pred.mean(1)\n",
    "\n",
    "    y_true = data[part]['y'].cpu().numpy()\n",
    "    score = (\n",
    "        mean_squared_error(y_true, y_pred) ** 0.5\n",
    "        if task_type == 'regression'\n",
    "        else accuracy_score(y_true, y_pred.argmax(1))\n",
    "    )\n",
    "    return float(score)  # The higher -- the better.\n",
    "\n",
    "@torch.no_grad()\n",
    "@torch.autocast(device.type, enabled=amp_enabled, dtype=amp_dtype)  # type: ignore[code]\n",
    "def predict(part: str) -> np.ndarray:\n",
    "    model.eval()\n",
    "\n",
    "    eval_batch_size = 8096\n",
    "    y_pred: np.ndarray = (\n",
    "        torch.cat(\n",
    "            [\n",
    "                apply_model(part, idx)\n",
    "                for idx in torch.arange(len(data[part]['x_cont']), device=device).split(\n",
    "                eval_batch_size\n",
    "            )\n",
    "            ]\n",
    "        )\n",
    "        .cpu()\n",
    "        .numpy()\n",
    "    )\n",
    "\n",
    "    if task_type == 'regression':\n",
    "        # Transform the predictions back to the original label space.\n",
    "        assert regression_label_stats is not None\n",
    "        y_pred = y_pred * regression_label_stats.std + regression_label_stats.mean\n",
    "\n",
    "    if task_type != 'regression':\n",
    "        # For classification, apply softmax to get probabilities\n",
    "        y_pred = scipy.special.softmax(y_pred, axis=-1)\n",
    "\n",
    "    # Take mean over k predictions\n",
    "    return y_pred.mean(1)\n"
   ],
   "id": "78ec422a213acdb1",
   "outputs": [],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-05T09:06:15.361667Z",
     "start_time": "2025-04-05T09:02:03.828783Z"
    }
   },
   "cell_type": "code",
   "source": [
    "seed = 333\n",
    "\n",
    "torch.manual_seed(seed)\n",
    "np.random.seed(seed)\n",
    "random.seed(seed)\n",
    "\n",
    "train = pd.read_csv(train_path).drop(columns=['ID'])\n",
    "test = pd.read_csv(test_path).drop(columns=['ID'])\n",
    "\n",
    "test_preds = []\n",
    "val_scores = []\n",
    "val_mses = []\n",
    "\n",
    "skf = KFold(n_splits=5, shuffle=True, random_state=seed)\n",
    "for fold, (train_idx, valid_idx) in enumerate(skf.split(train)):\n",
    "    fold_train = train.iloc[train_idx].copy().reset_index(drop=True)\n",
    "    fold_valid = train.iloc[valid_idx].copy().reset_index(drop=True)\n",
    "    fold_train2 = fold_train.copy()\n",
    "    fold_test = test.copy()\n",
    "\n",
    "    fold_train, fold_valid = all_process(fold_train, fold_valid)\n",
    "    _, fold_test = all_process(fold_train2, fold_test)\n",
    "\n",
    "    cat_cols = [col for col in fold_train.columns if pd.api.types.is_categorical_dtype(fold_train[col])]\n",
    "    numeric_cols = [col for col in fold_train.columns if col not in cat_cols and col != '임신 성공 확률']\n",
    "\n",
    "    data_numpy = {\n",
    "        'train': {'x_cont': fold_train[numeric_cols].values, 'y': fold_train['임신 성공 확률'].values},\n",
    "        'val': {'x_cont': fold_valid[numeric_cols].values, 'y': fold_valid['임신 성공 확률'].values},\n",
    "        'test': {'x_cont': fold_test[numeric_cols].values},\n",
    "    }\n",
    "\n",
    "    if cat_cols is not None:\n",
    "        data_numpy['train']['x_cat'] = fold_train[cat_cols].values\n",
    "        data_numpy['val']['x_cat'] = fold_valid[cat_cols].values\n",
    "        data_numpy['test']['x_cat'] = fold_test[cat_cols].values\n",
    "\n",
    "    class RegressionLabelStats(NamedTuple):\n",
    "        mean: float\n",
    "        std: float\n",
    "\n",
    "    Y_train = data_numpy['train']['y'].copy()\n",
    "    if task_type == 'regression':\n",
    "        regression_label_stats = RegressionLabelStats(\n",
    "            Y_train.mean().item(), Y_train.std().item()\n",
    "        )\n",
    "        Y_train = (Y_train - regression_label_stats.mean) / regression_label_stats.std\n",
    "    else:\n",
    "        regression_label_stats = None\n",
    "\n",
    "\n",
    "    data = {}\n",
    "    for part in data_numpy:\n",
    "        data[part] = {}\n",
    "        for k, v in data_numpy[part].items():\n",
    "            tensor = torch.as_tensor(v, device=device)\n",
    "            if k == 'x_cat':\n",
    "                tensor = tensor.long()\n",
    "            data[part][k] = tensor\n",
    "\n",
    "    Y_train = torch.as_tensor(Y_train, device=device)\n",
    "    if task_type == 'regression':\n",
    "        for part in data:\n",
    "            if 'y' in data[part]:  # 'y'가 존재할 경우에만 변환\n",
    "                data[part]['y'] = data[part]['y'].float()\n",
    "        Y_train = Y_train.float()\n",
    "\n",
    "    n_num_features, cat_cardinalities = get_feature_info(fold_train)\n",
    "\n",
    "    arch_type = 'tabm-mini'\n",
    "    bins = rtdl_num_embeddings.compute_bins(torch.tensor(fold_train[numeric_cols].values))\n",
    "\n",
    "    model = Model(\n",
    "        n_num_features=n_num_features,\n",
    "        cat_cardinalities=cat_cardinalities,\n",
    "        n_classes=None,\n",
    "        backbone={\n",
    "            'type': 'MLP',\n",
    "            'n_blocks': 3 if bins is None else 2,\n",
    "            'd_block': 512,\n",
    "            'dropout': 0.1,\n",
    "        },\n",
    "        bins=bins,\n",
    "        num_embeddings=(\n",
    "            None\n",
    "            if bins is None\n",
    "            else {\n",
    "                'type': 'PiecewiseLinearEmbeddings',\n",
    "                'd_embedding': 16,\n",
    "                'activation': False,\n",
    "                'version': 'B',\n",
    "            }\n",
    "        ),\n",
    "        arch_type=arch_type,\n",
    "        k=32,\n",
    "        share_training_batches=True,\n",
    "    ).to(device)\n",
    "    optimizer = torch.optim.AdamW(make_parameter_groups(model), lr=2e-3, weight_decay=3e-4)\n",
    "    base_loss_fn = F.mse_loss\n",
    "\n",
    "\n",
    "    n_epochs = 1_000_000_000\n",
    "    patience = 16\n",
    "\n",
    "    train_size = len(fold_train)\n",
    "    batch_size = 256\n",
    "    epoch_size = math.ceil(train_size / batch_size)\n",
    "    best = {\n",
    "        'val': math.inf,\n",
    "        'epoch': -1,\n",
    "    }\n",
    "\n",
    "    patience = 16\n",
    "    remaining_patience = patience\n",
    "\n",
    "    for epoch in range(n_epochs):\n",
    "        batches = (\n",
    "            torch.randperm(train_size, device=device).split(batch_size)\n",
    "            if model.share_training_batches\n",
    "            else [\n",
    "                x.transpose(0, 1).flatten()\n",
    "                for x in torch.rand((model.k, train_size), device=device)\n",
    "                .argsort(dim=1)\n",
    "                .split(batch_size, dim=1)\n",
    "            ]\n",
    "        )\n",
    "        for batch_idx in batches:\n",
    "            model.train()\n",
    "            optimizer.zero_grad()\n",
    "            loss = loss_fn(apply_model('train', batch_idx), Y_train[batch_idx])\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        train_mse = evaluate('train')\n",
    "        val_mse = evaluate('val')\n",
    "\n",
    "        y_val_pred = predict('val')\n",
    "        y_val_pred = np.clip(y_val_pred, 0, 1)\n",
    "        valid_score = competition_metric(fold_valid['임신 성공 확률'], y_val_pred)\n",
    "\n",
    "        y_test_pred = predict('test')\n",
    "        y_test_pred = np.clip(y_test_pred, 0, 1)\n",
    "        test_preds.append(y_test_pred)\n",
    "\n",
    "        if val_mse < best['val']:\n",
    "            best = {'epoch': epoch+1, 'train': train_mse, 'val': val_mse, 'score': valid_score}\n",
    "            remaining_patience = patience\n",
    "        else:\n",
    "            remaining_patience -= 1\n",
    "\n",
    "        if remaining_patience < 0:\n",
    "            break\n",
    "\n",
    "        print(f'[epoch {epoch+1}] Train Loss {train_mse:.5f} Valid Loss {val_mse:.5f}, Valid Score {valid_score:.7f}')\n",
    "\n",
    "        val_mses.append(val_mse)\n",
    "\n",
    "    print(f'\\n[Fold {fold+1} Result] ')\n",
    "    print(best)\n",
    "\n",
    "avg_valid_mse = np.mean(val_mses)\n",
    "\n",
    "print('-'*80)\n",
    "print(f'[Seed {seed}] Average Valid MSE: {avg_valid_mse:.7f}')"
   ],
   "id": "62d4f75ee1192f72",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[epoch 1] Train Loss 0.40387 Valid Loss 0.40410, Valid Score 0.3550278\n",
      "[epoch 2] Train Loss 0.40295 Valid Loss 0.40322, Valid Score 0.3560861\n",
      "[epoch 3] Train Loss 0.40290 Valid Loss 0.40321, Valid Score 0.3592911\n",
      "[epoch 4] Train Loss 0.40232 Valid Loss 0.40269, Valid Score 0.3587443\n",
      "[epoch 5] Train Loss 0.40207 Valid Loss 0.40278, Valid Score 0.3590662\n",
      "[epoch 6] Train Loss 0.40189 Valid Loss 0.40274, Valid Score 0.3594453\n",
      "[epoch 7] Train Loss 0.40178 Valid Loss 0.40267, Valid Score 0.3571797\n",
      "[epoch 8] Train Loss 0.40149 Valid Loss 0.40240, Valid Score 0.3592536\n",
      "[epoch 9] Train Loss 0.40140 Valid Loss 0.40271, Valid Score 0.3797759\n",
      "[epoch 10] Train Loss 0.40140 Valid Loss 0.40280, Valid Score 0.3625159\n",
      "[epoch 11] Train Loss 0.40075 Valid Loss 0.40260, Valid Score 0.3681365\n",
      "[epoch 12] Train Loss 0.40051 Valid Loss 0.40267, Valid Score 0.3693438\n",
      "[epoch 13] Train Loss 0.40061 Valid Loss 0.40295, Valid Score 0.3614514\n",
      "[epoch 14] Train Loss 0.39995 Valid Loss 0.40262, Valid Score 0.3587696\n",
      "[epoch 15] Train Loss 0.40002 Valid Loss 0.40289, Valid Score 0.3629803\n",
      "[epoch 16] Train Loss 0.39957 Valid Loss 0.40288, Valid Score 0.3574132\n",
      "[epoch 17] Train Loss 0.39896 Valid Loss 0.40301, Valid Score 0.3749902\n",
      "[epoch 18] Train Loss 0.39813 Valid Loss 0.40300, Valid Score 0.3676806\n",
      "[epoch 19] Train Loss 0.39819 Valid Loss 0.40317, Valid Score 0.3751434\n",
      "[epoch 20] Train Loss 0.39721 Valid Loss 0.40315, Valid Score 0.3740321\n",
      "[epoch 21] Train Loss 0.39746 Valid Loss 0.40339, Valid Score 0.3660919\n",
      "[epoch 22] Train Loss 0.39647 Valid Loss 0.40318, Valid Score 0.3738658\n",
      "[epoch 23] Train Loss 0.39640 Valid Loss 0.40341, Valid Score 0.3685108\n",
      "[epoch 24] Train Loss 0.39558 Valid Loss 0.40349, Valid Score 0.3703612\n",
      "\n",
      "[Fold 1 Result] \n",
      "{'epoch': 8, 'train': 0.4014940826970461, 'val': 0.4023970781557699, 'score': 0.35925364018844924}\n",
      "[epoch 1] Train Loss 0.40358 Valid Loss 0.40107, Valid Score 0.3580796\n",
      "[epoch 2] Train Loss 0.40340 Valid Loss 0.40122, Valid Score 0.3569227\n",
      "[epoch 3] Train Loss 0.40311 Valid Loss 0.40118, Valid Score 0.3631741\n",
      "[epoch 4] Train Loss 0.40306 Valid Loss 0.40129, Valid Score 0.3604899\n",
      "[epoch 5] Train Loss 0.40273 Valid Loss 0.40119, Valid Score 0.3604102\n",
      "[epoch 6] Train Loss 0.40234 Valid Loss 0.40085, Valid Score 0.3602642\n",
      "[epoch 7] Train Loss 0.40255 Valid Loss 0.40119, Valid Score 0.3577470\n",
      "[epoch 8] Train Loss 0.40260 Valid Loss 0.40127, Valid Score 0.3561244\n",
      "[epoch 9] Train Loss 0.40209 Valid Loss 0.40086, Valid Score 0.3641498\n",
      "[epoch 10] Train Loss 0.40167 Valid Loss 0.40110, Valid Score 0.3808882\n",
      "[epoch 11] Train Loss 0.40115 Valid Loss 0.40090, Valid Score 0.3733688\n",
      "[epoch 12] Train Loss 0.40116 Valid Loss 0.40086, Valid Score 0.3750124\n",
      "[epoch 13] Train Loss 0.40082 Valid Loss 0.40074, Valid Score 0.3765609\n",
      "[epoch 14] Train Loss 0.40071 Valid Loss 0.40109, Valid Score 0.3739320\n",
      "[epoch 15] Train Loss 0.40044 Valid Loss 0.40083, Valid Score 0.3608640\n",
      "[epoch 16] Train Loss 0.40019 Valid Loss 0.40088, Valid Score 0.3611553\n",
      "[epoch 17] Train Loss 0.39962 Valid Loss 0.40100, Valid Score 0.3621922\n",
      "[epoch 18] Train Loss 0.39954 Valid Loss 0.40113, Valid Score 0.3601518\n",
      "[epoch 19] Train Loss 0.39899 Valid Loss 0.40132, Valid Score 0.3621430\n",
      "[epoch 20] Train Loss 0.39871 Valid Loss 0.40130, Valid Score 0.3616837\n",
      "[epoch 21] Train Loss 0.39798 Valid Loss 0.40127, Valid Score 0.3757857\n",
      "[epoch 22] Train Loss 0.39789 Valid Loss 0.40125, Valid Score 0.3643097\n",
      "[epoch 23] Train Loss 0.39683 Valid Loss 0.40159, Valid Score 0.3886644\n",
      "[epoch 24] Train Loss 0.39647 Valid Loss 0.40162, Valid Score 0.3750666\n",
      "[epoch 25] Train Loss 0.39661 Valid Loss 0.40212, Valid Score 0.3660794\n",
      "[epoch 26] Train Loss 0.39540 Valid Loss 0.40139, Valid Score 0.3821510\n",
      "[epoch 27] Train Loss 0.39504 Valid Loss 0.40174, Valid Score 0.3762276\n",
      "[epoch 28] Train Loss 0.39447 Valid Loss 0.40182, Valid Score 0.3741582\n",
      "[epoch 29] Train Loss 0.39402 Valid Loss 0.40176, Valid Score 0.3693263\n",
      "\n",
      "[Fold 2 Result] \n",
      "{'epoch': 13, 'train': 0.40081812668477507, 'val': 0.40074082852992204, 'score': 0.3765608616467209}\n",
      "[epoch 1] Train Loss 0.40317 Valid Loss 0.40410, Valid Score 0.3562550\n",
      "[epoch 2] Train Loss 0.40305 Valid Loss 0.40438, Valid Score 0.3635627\n",
      "[epoch 3] Train Loss 0.40248 Valid Loss 0.40403, Valid Score 0.3677023\n",
      "[epoch 4] Train Loss 0.40195 Valid Loss 0.40381, Valid Score 0.3661917\n",
      "[epoch 5] Train Loss 0.40197 Valid Loss 0.40407, Valid Score 0.3842202\n",
      "[epoch 6] Train Loss 0.40193 Valid Loss 0.40393, Valid Score 0.3726950\n",
      "[epoch 7] Train Loss 0.40182 Valid Loss 0.40395, Valid Score 0.3579237\n",
      "[epoch 8] Train Loss 0.40145 Valid Loss 0.40380, Valid Score 0.3753019\n",
      "[epoch 9] Train Loss 0.40141 Valid Loss 0.40363, Valid Score 0.3589491\n",
      "[epoch 10] Train Loss 0.40161 Valid Loss 0.40394, Valid Score 0.3572925\n",
      "[epoch 11] Train Loss 0.40044 Valid Loss 0.40357, Valid Score 0.3649691\n",
      "[epoch 12] Train Loss 0.40026 Valid Loss 0.40373, Valid Score 0.3704989\n",
      "[epoch 13] Train Loss 0.39989 Valid Loss 0.40386, Valid Score 0.3837265\n",
      "[epoch 14] Train Loss 0.39977 Valid Loss 0.40397, Valid Score 0.3716729\n",
      "[epoch 15] Train Loss 0.39962 Valid Loss 0.40367, Valid Score 0.3653221\n",
      "[epoch 16] Train Loss 0.39917 Valid Loss 0.40405, Valid Score 0.3813481\n",
      "[epoch 17] Train Loss 0.39892 Valid Loss 0.40449, Valid Score 0.3917824\n",
      "[epoch 18] Train Loss 0.39869 Valid Loss 0.40375, Valid Score 0.3626671\n",
      "[epoch 19] Train Loss 0.39824 Valid Loss 0.40389, Valid Score 0.3819682\n",
      "[epoch 20] Train Loss 0.39775 Valid Loss 0.40445, Valid Score 0.3914158\n",
      "[epoch 21] Train Loss 0.39750 Valid Loss 0.40390, Valid Score 0.3719630\n",
      "[epoch 22] Train Loss 0.39690 Valid Loss 0.40414, Valid Score 0.3719214\n",
      "[epoch 23] Train Loss 0.39658 Valid Loss 0.40446, Valid Score 0.3800387\n",
      "[epoch 24] Train Loss 0.39634 Valid Loss 0.40411, Valid Score 0.3719630\n",
      "[epoch 25] Train Loss 0.39599 Valid Loss 0.40456, Valid Score 0.3879908\n",
      "[epoch 26] Train Loss 0.39535 Valid Loss 0.40516, Valid Score 0.3962830\n",
      "[epoch 27] Train Loss 0.39509 Valid Loss 0.40452, Valid Score 0.3865839\n",
      "\n",
      "[Fold 3 Result] \n",
      "{'epoch': 11, 'train': 0.4004410874736112, 'val': 0.4035653762206813, 'score': 0.36496907715979726}\n",
      "[epoch 1] Train Loss 0.40298 Valid Loss 0.40388, Valid Score 0.3567389\n",
      "[epoch 2] Train Loss 0.40295 Valid Loss 0.40403, Valid Score 0.3577286\n",
      "[epoch 3] Train Loss 0.40255 Valid Loss 0.40350, Valid Score 0.3587666\n",
      "[epoch 4] Train Loss 0.40236 Valid Loss 0.40366, Valid Score 0.3551170\n",
      "[epoch 5] Train Loss 0.40200 Valid Loss 0.40337, Valid Score 0.3578882\n",
      "[epoch 6] Train Loss 0.40228 Valid Loss 0.40384, Valid Score 0.3817010\n",
      "[epoch 7] Train Loss 0.40209 Valid Loss 0.40372, Valid Score 0.3825763\n",
      "[epoch 8] Train Loss 0.40129 Valid Loss 0.40319, Valid Score 0.3685124\n",
      "[epoch 9] Train Loss 0.40142 Valid Loss 0.40330, Valid Score 0.3582358\n",
      "[epoch 10] Train Loss 0.40112 Valid Loss 0.40306, Valid Score 0.3599386\n",
      "[epoch 11] Train Loss 0.40092 Valid Loss 0.40312, Valid Score 0.3685028\n",
      "[epoch 12] Train Loss 0.40090 Valid Loss 0.40328, Valid Score 0.3687702\n",
      "[epoch 13] Train Loss 0.40064 Valid Loss 0.40370, Valid Score 0.3826976\n",
      "[epoch 14] Train Loss 0.39985 Valid Loss 0.40317, Valid Score 0.3767501\n",
      "[epoch 15] Train Loss 0.39963 Valid Loss 0.40310, Valid Score 0.3687248\n",
      "[epoch 16] Train Loss 0.39954 Valid Loss 0.40356, Valid Score 0.3649289\n",
      "[epoch 17] Train Loss 0.39890 Valid Loss 0.40335, Valid Score 0.3823961\n",
      "[epoch 18] Train Loss 0.39916 Valid Loss 0.40348, Valid Score 0.3616257\n",
      "[epoch 19] Train Loss 0.39844 Valid Loss 0.40354, Valid Score 0.3797304\n",
      "[epoch 20] Train Loss 0.39798 Valid Loss 0.40331, Valid Score 0.3707085\n",
      "[epoch 21] Train Loss 0.39743 Valid Loss 0.40351, Valid Score 0.3681662\n",
      "[epoch 22] Train Loss 0.39700 Valid Loss 0.40338, Valid Score 0.3757698\n",
      "[epoch 23] Train Loss 0.39645 Valid Loss 0.40373, Valid Score 0.3685223\n",
      "[epoch 24] Train Loss 0.39581 Valid Loss 0.40362, Valid Score 0.3879245\n",
      "[epoch 25] Train Loss 0.39571 Valid Loss 0.40366, Valid Score 0.3765880\n",
      "[epoch 26] Train Loss 0.39456 Valid Loss 0.40405, Valid Score 0.3821042\n",
      "\n",
      "[Fold 4 Result] \n",
      "{'epoch': 10, 'train': 0.40112492287511137, 'val': 0.40305654107112154, 'score': 0.35993864665390146}\n",
      "[epoch 1] Train Loss 0.40343 Valid Loss 0.40492, Valid Score 0.3560957\n",
      "[epoch 2] Train Loss 0.40259 Valid Loss 0.40452, Valid Score 0.3624515\n",
      "[epoch 3] Train Loss 0.40226 Valid Loss 0.40414, Valid Score 0.3560909\n",
      "[epoch 4] Train Loss 0.40251 Valid Loss 0.40461, Valid Score 0.3587126\n",
      "[epoch 5] Train Loss 0.40176 Valid Loss 0.40427, Valid Score 0.3577060\n",
      "[epoch 6] Train Loss 0.40154 Valid Loss 0.40389, Valid Score 0.3600488\n",
      "[epoch 7] Train Loss 0.40165 Valid Loss 0.40445, Valid Score 0.3600816\n",
      "[epoch 8] Train Loss 0.40150 Valid Loss 0.40456, Valid Score 0.3546074\n",
      "[epoch 9] Train Loss 0.40100 Valid Loss 0.40456, Valid Score 0.3861655\n",
      "[epoch 10] Train Loss 0.40066 Valid Loss 0.40422, Valid Score 0.3605070\n",
      "[epoch 11] Train Loss 0.40015 Valid Loss 0.40427, Valid Score 0.3735570\n",
      "[epoch 12] Train Loss 0.40036 Valid Loss 0.40441, Valid Score 0.3785902\n",
      "[epoch 13] Train Loss 0.40007 Valid Loss 0.40408, Valid Score 0.3607088\n",
      "[epoch 14] Train Loss 0.39966 Valid Loss 0.40477, Valid Score 0.3699234\n",
      "[epoch 15] Train Loss 0.39911 Valid Loss 0.40433, Valid Score 0.3618842\n",
      "[epoch 16] Train Loss 0.39881 Valid Loss 0.40465, Valid Score 0.3843058\n",
      "[epoch 17] Train Loss 0.39852 Valid Loss 0.40427, Valid Score 0.3631630\n",
      "[epoch 18] Train Loss 0.39786 Valid Loss 0.40460, Valid Score 0.3767365\n",
      "[epoch 19] Train Loss 0.39851 Valid Loss 0.40500, Valid Score 0.3526814\n",
      "[epoch 20] Train Loss 0.39724 Valid Loss 0.40495, Valid Score 0.3820044\n",
      "[epoch 21] Train Loss 0.39687 Valid Loss 0.40470, Valid Score 0.3651409\n",
      "[epoch 22] Train Loss 0.39616 Valid Loss 0.40487, Valid Score 0.3706698\n",
      "\n",
      "[Fold 5 Result] \n",
      "{'epoch': 6, 'train': 0.40154160471184047, 'val': 0.4038912816474544, 'score': 0.36004880811508266}\n",
      "--------------------------------------------------------------------------------\n",
      "[Seed 333] Average Valid MSE: 0.4031844\n"
     ]
    }
   ],
   "execution_count": 9
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-05T09:08:05.646908Z",
     "start_time": "2025-04-05T09:08:05.604906Z"
    }
   },
   "cell_type": "code",
   "source": [
    "submission = pd.read_csv(sample_path)\n",
    "\n",
    "submission['임신 성공 확률'] = np.mean(test_preds, axis=0)\n",
    "submission"
   ],
   "id": "da79f64aafb874fb",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "               ID  임신 성공 확률\n",
       "0      TEST_00000  0.083630\n",
       "1      TEST_00001  0.192079\n",
       "2      TEST_00002  0.225487\n",
       "3      TEST_00003  0.295571\n",
       "4      TEST_00004  0.269995\n",
       "...           ...       ...\n",
       "54407  TEST_54407  0.289648\n",
       "54408  TEST_54408  0.054458\n",
       "54409  TEST_54409  0.294988\n",
       "54410  TEST_54410  0.291498\n",
       "54411  TEST_54411  0.158053\n",
       "\n",
       "[54412 rows x 2 columns]"
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>임신 성공 확률</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>TEST_00000</td>\n",
       "      <td>0.083630</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>TEST_00001</td>\n",
       "      <td>0.192079</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>TEST_00002</td>\n",
       "      <td>0.225487</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>TEST_00003</td>\n",
       "      <td>0.295571</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>TEST_00004</td>\n",
       "      <td>0.269995</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54407</th>\n",
       "      <td>TEST_54407</td>\n",
       "      <td>0.289648</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54408</th>\n",
       "      <td>TEST_54408</td>\n",
       "      <td>0.054458</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54409</th>\n",
       "      <td>TEST_54409</td>\n",
       "      <td>0.294988</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54410</th>\n",
       "      <td>TEST_54410</td>\n",
       "      <td>0.291498</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54411</th>\n",
       "      <td>TEST_54411</td>\n",
       "      <td>0.158053</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>54412 rows × 2 columns</p>\n",
       "</div>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 14
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-05T09:09:00.482332Z",
     "start_time": "2025-04-05T09:09:00.424231Z"
    }
   },
   "cell_type": "code",
   "source": "submission.to_csv(f'./Submission/TabM_{seed}.csv', index=False)",
   "id": "60ebcf8f803b5d65",
   "outputs": [],
   "execution_count": 16
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "6396aca59e1b5c8d"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
