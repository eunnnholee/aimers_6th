{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 라이브러리 설치",
   "id": "5dd7a13cab58dd07"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-04T08:27:10.305744Z",
     "start_time": "2025-04-04T08:27:10.290731Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# conda install -c conda-forge faiss-gpu\n",
    "\n",
    "# conda 가상환경 상에서 설치 (로컬로 돌릴때)"
   ],
   "id": "6574b27e2f56ef2c",
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "36aa081e2784f2e7"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Import",
   "id": "d816fc12aa1f272c"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-04T13:40:59.614940Z",
     "start_time": "2025-04-04T13:40:56.922228Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import sys\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "# 현재 작업 디렉토리(Eunhak)에서 tabular_dl_tabr 경로 추가\n",
    "project_path = os.path.join(os.getcwd(), \"tabular_dl_tabr\")\n",
    "if project_path not in sys.path:\n",
    "    sys.path.insert(0, project_path)\n",
    "\n",
    "\n",
    "project_dir = Path(r\"C:\\workspace\\LG_Aimers_6th\\Eunhak\\tabular_dl_tabr\")\n",
    "os.environ['PROJECT_DIR'] = str(project_dir)\n",
    "\n",
    "# 경로가 존재하지 않으면 생성\n",
    "if not project_dir.exists():\n",
    "    project_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "import math\n",
    "import scipy\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "import delu\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "from bin.tabr import Model\n",
    "from LG_Aimers_6th.cal_auc import calculate_auc"
   ],
   "id": "a7699c4dd3ad12c9",
   "outputs": [],
   "execution_count": 1
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-04-04T13:41:19.791876Z",
     "start_time": "2025-04-04T13:41:18.549619Z"
    }
   },
   "source": [
    "train_path = f'../data/train.csv'\n",
    "test_path = f'../data/test.csv'\n",
    "sample_path = f'../data/sample_submission.csv'\n",
    "\n",
    "# 학습/평가 데이터 로드\n",
    "train = pd.read_csv(train_path).drop(columns=['ID'])\n",
    "test = pd.read_csv(test_path).drop(columns=['ID']) # test에는 target이 없음\n",
    "\n",
    "print(train.shape, test.shape)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(256351, 68) (90067, 67)\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-04T13:41:29.393667Z",
     "start_time": "2025-04-04T13:41:22.931847Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from preprocess_DL import all_process\n",
    "\n",
    "train = pd.read_csv(train_path).drop(columns=['ID'])\n",
    "test = pd.read_csv(test_path).drop(columns=['ID'])\n",
    "\n",
    "train, test = all_process(train, test)\n",
    "\n",
    "print(train.shape, test.shape)"
   ],
   "id": "d931c6a8384f4e1f",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(256351, 66) (90067, 65)\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-04T13:41:31.303091Z",
     "start_time": "2025-04-04T13:41:31.212077Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def get_cols(df_train, target_col='임신 성공 여부'):\n",
    "    cat_cols = [col for col in df_train.columns if pd.api.types.is_categorical_dtype(df_train[col])]\n",
    "    numeric_cols = [col for col in df_train.columns if col not in cat_cols and col != '임신 성공 여부']\n",
    "\n",
    "    num_cols = []\n",
    "    bin_cols = []\n",
    "    for col in numeric_cols:\n",
    "        if df_train[col].nunique() == 2:\n",
    "            bin_cols.append(col)\n",
    "        else:\n",
    "            num_cols.append(col)\n",
    "\n",
    "    return num_cols, bin_cols, cat_cols\n",
    "\n",
    "num_cols, bin_cols, cat_cols = get_cols(train)\n",
    "cat_cardinalities = [train[col].nunique() for col in cat_cols]\n",
    "\n",
    "print(f'수치형 변수: {len(num_cols)}개 \\n{num_cols}')\n",
    "print(f'이진형 변수: {len(bin_cols)}개 \\n{bin_cols}')\n",
    "print(f'범주형 변수: {len(cat_cols)}개 \\n{cat_cols}')"
   ],
   "id": "9dbb80b9a10a8558",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "수치형 변수: 32개 \n",
      "['임신 시도 또는 마지막 임신 경과 연수', '클리닉 내 총 시술 횟수', 'IVF 시술 횟수', 'DI 시술 횟수', '총 임신 횟수', 'IVF 임신 횟수', 'DI 임신 횟수', '총 출산 횟수', 'IVF 출산 횟수', 'DI 출산 횟수', '총 생성 배아 수', '미세주입된 난자 수', '미세주입에서 생성된 배아 수', '이식된 배아 수', '미세주입 배아 이식 수', '저장된 배아 수', '미세주입 후 저장된 배아 수', '해동된 배아 수', '해동 난자 수', '수집된 신선 난자 수', '저장된 신선 난자 수', '혼합된 난자 수', '파트너 정자와 혼합된 난자 수', '기증자 정자와 혼합된 난자 수', '난자 혼합 경과일', '배아 이식 경과일', '배아 해동 경과일', '시술_임신', '배아생성이유_기증용', '배아생성이유_난자 저장용', '배아생성이유_배아 저장용', '배아생성이유_현재 시술용']\n",
      "이진형 변수: 25개 \n",
      "['배란 자극 여부', '단일 배아 이식 여부', '착상 전 유전 검사 사용 여부', '착상 전 유전 진단 사용 여부', '남성 주 불임 원인', '남성 부 불임 원인', '여성 주 불임 원인', '여성 부 불임 원인', '부부 주 불임 원인', '부부 부 불임 원인', '불명확 불임 원인', '불임 원인 - 난관 질환', '불임 원인 - 남성 요인', '불임 원인 - 배란 장애', '불임 원인 - 자궁경부 문제', '불임 원인 - 자궁내막증', '불임 원인 - 정자 농도', '불임 원인 - 정자 운동성', '불임 원인 - 정자 형태', '동결 배아 사용 여부', '신선 배아 사용 여부', '기증 배아 사용 여부', '대리모 여부', 'PGD 시술 여부', 'PGS 시술 여부']\n",
      "범주형 변수: 8개 \n",
      "['시술 시기 코드', '시술 당시 나이', '배란 유도 유형', '난자 출처', '정자 출처', '난자 기증자 나이', '정자 기증자 나이', '시술유형_통합']\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-04T13:41:32.183020Z",
     "start_time": "2025-04-04T13:41:32.170019Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def build_dataset_from_dfs(train_df, valid_df, test_df, num_cols, bin_cols, cat_cols, target_col='임신 성공 여부'):\n",
    "    data = {}\n",
    "    data['X_num'] = {\n",
    "        'train': torch.tensor(train_df[num_cols].values, dtype=torch.float32),\n",
    "        'val':   torch.tensor(valid_df[num_cols].values, dtype=torch.float32),\n",
    "        'test':  torch.tensor(test_df[num_cols].values, dtype=torch.float32),\n",
    "    }\n",
    "    data['X_bin'] = {\n",
    "        'train': torch.tensor(train_df[bin_cols].values, dtype=torch.float32),\n",
    "        'val':   torch.tensor(valid_df[bin_cols].values, dtype=torch.float32),\n",
    "        'test':  torch.tensor(test_df[bin_cols].values, dtype=torch.float32),\n",
    "    }\n",
    "    if cat_cols:\n",
    "        data['X_cat'] = {\n",
    "            'train': torch.tensor(train_df[cat_cols].values, dtype=torch.long),\n",
    "            'val':   torch.tensor(valid_df[cat_cols].values, dtype=torch.long),\n",
    "            'test':  torch.tensor(test_df[cat_cols].values, dtype=torch.long),\n",
    "        }\n",
    "    else:\n",
    "        data['X_cat'] = None\n",
    "    data['Y'] = {\n",
    "        'train': torch.tensor(train_df[target_col].values, dtype=torch.long),\n",
    "        'val':   torch.tensor(valid_df[target_col].values, dtype=torch.long),\n",
    "        # test 데이터에는 타깃이 없을 수 있습니다.\n",
    "    }\n",
    "    return data\n",
    "\n",
    "def move_data_to_device(data, device):\n",
    "    # data는 dict 형식: 예) {'X_num': {'train': tensor, 'val': tensor, ...}, ...}\n",
    "    for key in data:\n",
    "        if data[key] is None:\n",
    "            continue\n",
    "        if isinstance(data[key], dict):\n",
    "            for part in data[key]:\n",
    "                data[key][part] = data[key][part].to(device)\n",
    "        else:\n",
    "            data[key] = data[key].to(device)\n",
    "    return data\n",
    "\n",
    "\n",
    "class MyDataset:\n",
    "    def __init__(self, data, n_num_features, n_bin_features, cat_cardinalities, is_regression=False, is_multiclass=True):\n",
    "        self.data = data\n",
    "        self._n_num_features = n_num_features\n",
    "        self._n_bin_features = n_bin_features\n",
    "        self._cat_cardinalities = cat_cardinalities\n",
    "        self.is_regression = is_regression\n",
    "        self.is_multiclass = is_multiclass\n",
    "\n",
    "    @property\n",
    "    def n_num_features(self):\n",
    "        return self._n_num_features\n",
    "\n",
    "    @property\n",
    "    def n_bin_features(self):\n",
    "        return self._n_bin_features\n",
    "\n",
    "    def cat_cardinalities(self):\n",
    "        return self._cat_cardinalities\n",
    "\n",
    "    @property\n",
    "    def Y(self):\n",
    "        return self.data['Y']\n",
    "\n",
    "    def size(self, part: str) -> int:\n",
    "        # target이 있는 경우 사용\n",
    "        if part in self.data['Y']:\n",
    "            return self.data['Y'][part].shape[0]\n",
    "        else:\n",
    "            return self.data['X_num'][part].shape[0]\n",
    "\n",
    "# data_dict = build_dataset_from_dfs(train, valid, test, num_cols, bin_cols, cat_cols, target_col='임신 성공 여부')\n",
    "# data_dict = move_data_to_device(data_dict, device)\n",
    "#\n",
    "# dataset = MyDataset(data_dict, n_num_features=len(num_cols), n_bin_features=len(bin_cols), cat_cardinalities=cat_cardinalities)"
   ],
   "id": "cf40ff609df1c64b",
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-04T13:41:33.110558Z",
     "start_time": "2025-04-04T13:41:33.096560Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def get_Xy(part: str, idx: torch.Tensor = None) -> tuple[dict, torch.Tensor]:\n",
    "    batch = (\n",
    "        { key[2:]: dataset.data[key][part] for key in dataset.data if key.startswith('X_') },\n",
    "        dataset.data['Y'][part] if 'Y' in dataset.data and part in dataset.data['Y'] else None\n",
    "    )\n",
    "    if idx is None:\n",
    "        return batch\n",
    "    else:\n",
    "        return (\n",
    "            {k: v[idx] for k, v in batch[0].items()},\n",
    "            batch[1][idx] if batch[1] is not None else None\n",
    "        )\n",
    "\n",
    "# train_size = dataset.size('train')\n",
    "# train_indices = torch.arange(train_size, device=device)"
   ],
   "id": "691e8030d5feac88",
   "outputs": [],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-04T13:42:21.499628Z",
     "start_time": "2025-04-04T13:42:21.485628Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def apply_model(part: str, idx: torch.Tensor, is_train: bool) -> torch.Tensor:\n",
    "    x, y = get_Xy(part, idx)\n",
    "    candidate_indices = train_indices\n",
    "    if is_train:\n",
    "        # training part: 후보에서 현재 배치 제거\n",
    "        candidate_indices = candidate_indices[~torch.isin(candidate_indices, idx)]\n",
    "    # 후보 데이터: 조건에 따라 전체 train 또는 선택된 인덱스 사용\n",
    "    candidate_x, candidate_y = get_Xy('train', None if candidate_indices.equal(train_indices) else candidate_indices)\n",
    "    return model(\n",
    "        x_=x,\n",
    "        y=y if is_train else None,\n",
    "        candidate_x_=candidate_x,\n",
    "        candidate_y=candidate_y,\n",
    "        context_size=5,\n",
    "        is_train=is_train,\n",
    "    ).squeeze(-1)\n"
   ],
   "id": "582b27aec832b9a",
   "outputs": [],
   "execution_count": 7
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "seed = 333\n",
    "\n",
    "torch.manual_seed(seed)\n",
    "delu.random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "random.seed(seed)\n",
    "\n",
    "train = pd.read_csv(train_path).drop(columns=['ID'])\n",
    "test = pd.read_csv(test_path).drop(columns=['ID'])\n",
    "\n",
    "test_preds = []\n",
    "val_aucs = []\n",
    "\n",
    "skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=seed)\n",
    "for fold, (train_idx, valid_idx) in enumerate(skf.split(train, train['임신 성공 여부'])):\n",
    "    fold_train = train.iloc[train_idx].copy().reset_index(drop=True)\n",
    "    fold_valid = train.iloc[valid_idx].copy().reset_index(drop=True)\n",
    "    fold_train2 = fold_train.copy()\n",
    "    fold_test = test.copy()\n",
    "\n",
    "    fold_train, fold_valid = all_process(fold_train, fold_valid)\n",
    "    _, fold_test = all_process(fold_train2, fold_test)\n",
    "\n",
    "    num_cols, bin_cols, cat_cols = get_cols(fold_train)\n",
    "    cat_cardinalities = [fold_train[col].nunique() for col in cat_cols]\n",
    "\n",
    "    data_dict = build_dataset_from_dfs(\n",
    "        fold_train, fold_valid, fold_test,\n",
    "        num_cols, bin_cols, cat_cols, target_col='임신 성공 여부'\n",
    "    )\n",
    "    data_dict = move_data_to_device(data_dict, device)\n",
    "    dataset = MyDataset(data_dict, n_num_features=len(num_cols), n_bin_features=len(bin_cols), cat_cardinalities=cat_cardinalities)\n",
    "\n",
    "    train_size = dataset.size('train')\n",
    "    train_indices = torch.arange(train_size, device=device)\n",
    "\n",
    "    model = Model(\n",
    "        n_num_features=len(num_cols),\n",
    "        n_bin_features=len(bin_cols),\n",
    "        cat_cardinalities=cat_cardinalities,\n",
    "        n_classes=2,\n",
    "        num_embeddings=None,      # 임베딩 사용하지 않을 경우 None\n",
    "        d_main=64,\n",
    "        d_multiplier=2.0,\n",
    "        encoder_n_blocks=2,\n",
    "        predictor_n_blocks=2,\n",
    "        mixer_normalization=True,\n",
    "        context_dropout=0.1,\n",
    "        dropout0=0.1,\n",
    "        dropout1='dropout0',      # 'dropout0' 문자열을 지정하면 내부에서 dropout0 값이 사용됩니다.\n",
    "        normalization=\"BatchNorm1d\",\n",
    "        activation=\"ReLU\",\n",
    "        memory_efficient=False,\n",
    "        candidate_encoding_batch_size=None,\n",
    "    ).to(device)\n",
    "\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "    criterion = nn.BCEWithLogitsLoss()\n",
    "\n",
    "    num_epochs = 100000\n",
    "    batch_size = 2048\n",
    "\n",
    "    patience = 10\n",
    "    best_val_loss = float('inf')\n",
    "    early_stop_counter = 0\n",
    "\n",
    "    checkpoint_path = \"best_model_TabR.pth\"\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        # 학습 데이터 인덱스 섞기\n",
    "        shuffled_indices = train_indices[torch.randperm(train_size)]\n",
    "        num_batches = math.ceil(train_size / batch_size)\n",
    "        epoch_loss = 0.0\n",
    "        for i in range(num_batches):\n",
    "            idx = shuffled_indices[i * batch_size : (i + 1) * batch_size]\n",
    "            outputs = apply_model('train', idx, is_train=True)\n",
    "\n",
    "            # 해당 인덱스의 타깃\n",
    "            _, y_batch = get_Xy('train', idx)\n",
    "\n",
    "            y_batch = y_batch.float()\n",
    "            loss = criterion(outputs.squeeze(), y_batch.squeeze()) # squeeze해서 shape 맞추기 (예: (batch_size,))\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            epoch_loss += loss.item() * idx.numel()\n",
    "\n",
    "        avg_loss = epoch_loss / train_size\n",
    "\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            val_indices = torch.arange(dataset.size('val'), device=device)\n",
    "            outputs_val = apply_model('val', val_indices, is_train=False)\n",
    "            _, y_val = get_Xy('val', val_indices)\n",
    "\n",
    "            val_loss = criterion(outputs_val.squeeze(), y_val.float().squeeze()).item() # validation loss 계산\n",
    "\n",
    "            outputs_val = torch.sigmoid(outputs_val)\n",
    "            outputs_val_np = outputs_val.detach().cpu().numpy().squeeze()\n",
    "            y_val_np = y_val.detach().cpu().numpy().squeeze()\n",
    "\n",
    "            val_auc = roc_auc_score(y_val_np, outputs_val_np)\n",
    "\n",
    "        print(f\"[Epoch {epoch+1}] Train Loss: {avg_loss:.4f}, Valid Loss: {val_loss:.4f}, Valid AUC: {val_auc:.6f}\")\n",
    "\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            early_stop_counter = 0\n",
    "            best = {'epoch': epoch+1, 'val_loss': val_loss, 'val_auc': val_auc}\n",
    "            torch.save(model.state_dict(), checkpoint_path)\n",
    "        else:\n",
    "            early_stop_counter += 1\n",
    "            if early_stop_counter >= patience:\n",
    "                break\n",
    "\n",
    "    model.load_state_dict(torch.load(checkpoint_path))\n",
    "    torch.cuda.empty_cache()\n",
    "    print(f'\\n[Fold{fold+1} Result]')\n",
    "    print(best)\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        val_indices = torch.arange(dataset.size('val'), device=device)\n",
    "        val_pred = apply_model('val', val_indices, is_train=False)\n",
    "        val_pred = torch.sigmoid(val_pred)\n",
    "        val_pred_np = val_pred.detach().cpu().numpy().squeeze()\n",
    "\n",
    "        _, y_val = get_Xy('val', val_indices)\n",
    "        y_val_np = y_val.detach().cpu().numpy().squeeze()\n",
    "\n",
    "        val_auc = roc_auc_score(val_pred_np, y_val_np)\n",
    "        val_aucs.append(val_auc)\n",
    "        print(f'[Seed {seed} Fold {fold+1}] Test AUC: {val_auc}')\n",
    "\n",
    "        test_indices = torch.arange(dataset.size('test'), device=device)\n",
    "        test_pred = apply_model('test', test_indices, is_train=False)\n",
    "        test_pred = torch.sigmoid(test_pred)\n",
    "        test_pred_np = test_pred.detach().cpu().numpy().squeeze()\n",
    "\n",
    "        test_preds.append(test_pred_np)\n",
    "\n",
    "final_score = np.mean(val_aucs, axis=0)\n",
    "\n",
    "print(f'[Seed {seed}] Final Valid AUC: {final_score:.7f}')"
   ],
   "id": "7a2e4dcbc4842d7b",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-04T08:32:16.993905Z",
     "start_time": "2025-04-04T08:32:16.978904Z"
    }
   },
   "cell_type": "code",
   "source": [
    "submission = pd.read_csv(sample_path)\n",
    "\n",
    "submission['probability'] = np.mean(test_preds, axis=0)\n",
    "submission"
   ],
   "id": "5a8014e0753892d0",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "submission.to_csv(f'./Submission/TabR_{seed}.csv', index=False)",
   "id": "a5f8ca582076b6ee"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
