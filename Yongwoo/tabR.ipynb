{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip -q install ipynbname\n",
    "!pip -q install pandas numpy ipynbname scikit-learn torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sqlite3\n",
    "import shutil\n",
    "import ipynbname\n",
    "import datetime\n",
    "import sys\n",
    "import os\n",
    "from sklearn.model_selection import StratifiedKFold, train_test_split\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.preprocessing import StandardScaler, FunctionTransformer, MultiLabelBinarizer\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "# import cudf\n",
    "# from cuml.neighbors import NearestNeighbors\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "orin_train_path = '../data/train.csv'\n",
    "train_path = '../data/custom_train_7.csv'\n",
    "test_path = '../data/custom_test_7.csv'\n",
    "sample_path = '../data/sample_submission.csv'\n",
    "\n",
    "# 1. 데이터 로드\n",
    "origin= pd.read_csv(orin_train_path).drop(columns=['ID'])\n",
    "train = pd.read_csv(train_path).drop(columns=['ID'])\n",
    "test = pd.read_csv(test_path).drop(columns=['ID'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def drop_columns(df):\n",
    "    cols = [\n",
    "        '불임 원인 - 여성 요인',  # 고유값 1\n",
    "        '불임 원인 - 정자 면역학적 요인',  # train, test 모두 '1'인 데이터 1개 >> 신뢰할 수 없음\n",
    "        '난자 해동 경과일',\n",
    "    ]\n",
    "    df = df.drop(cols, axis=1)\n",
    "    return df\n",
    "\n",
    "def 특정시술유형(train, test):\n",
    "    def categorize_procedure(proc):\n",
    "        tokens = [token.strip() for token in proc.split(\",\") if token.strip() and not token.strip().isdigit()]\n",
    "        # 우선순위에 따른 범주화\n",
    "        if tokens.count(\"Unknown\") >= 1:\n",
    "            return \"Unknown\"\n",
    "        if tokens.count(\"AH\") >= 1:\n",
    "            return \"AH\"\n",
    "        if tokens.count(\"BLASTOCYST\") >= 1:\n",
    "            return \"BLASTOCYST\"\n",
    "        if tokens.count(\"ICSI\") >= 2 or tokens.count(\"IVF\") >= 2:\n",
    "            return \"2ICSI_2IVF\"\n",
    "        if tokens.count(\"IVF\") >= 1 and tokens.count(\"ICSI\") >= 1:\n",
    "            return \"IVF_ICSI\"\n",
    "        if tokens == \"ICSI\":\n",
    "            return \"ICSI\"\n",
    "        if tokens == \"IVF\":\n",
    "            return \"IVF\"\n",
    "        return \",\".join(tokens) if tokens else None\n",
    "\n",
    "    for df in [train, test]:\n",
    "        df['특정 시술 유형'] = df['특정 시술 유형'].str.replace(\" / \", \",\")\n",
    "        df['특정 시술 유형'] = df['특정 시술 유형'].str.replace(\":\", \",\")\n",
    "        df['특정 시술 유형'] = df['특정 시술 유형'].str.replace(\" \", \"\")\n",
    "\n",
    "    counts = train['특정 시술 유형'].value_counts()\n",
    "    allowed_categories = counts[counts >= 100].index.tolist()\n",
    "\n",
    "    # allowed_categories에 속하지 않는 값은 \"Unknown\"으로 대체\n",
    "    train.loc[~train['특정 시술 유형'].isin(allowed_categories), '특정 시술 유형'] = \"Unknown\"\n",
    "    test.loc[~test['특정 시술 유형'].isin(allowed_categories), '특정 시술 유형'] = \"Unknown\"\n",
    "\n",
    "    train['특정 시술 유형'] = train['특정 시술 유형'].apply(categorize_procedure)\n",
    "    test['특정 시술 유형'] = test['특정 시술 유형'].apply(categorize_procedure)\n",
    "\n",
    "    train['시술유형_통합'] = train['시술 유형'].astype(str) + '_' + train['특정 시술 유형'].astype(str)\n",
    "    test['시술유형_통합'] = test['시술 유형'].astype(str) + '_' + test['특정 시술 유형'].astype(str)\n",
    "\n",
    "    drop_cols = ['시술 유형', '특정 시술 유형']\n",
    "    train = train.drop(drop_cols, axis=1)\n",
    "    test = test.drop(drop_cols, axis=1)\n",
    "\n",
    "    return train, test\n",
    "\n",
    "def 시술횟수(df_train):\n",
    "    for col in [col for col in df_train.columns if '횟수' in col]:\n",
    "        df_train[col] = df_train[col].replace({'6회 이상':'6회'})\n",
    "        df_train[col] = df_train[col].str[0].astype(int)\n",
    "    df_train['시술_임신'] = df_train['총 임신 횟수'] - df_train['총 시술 횟수']\n",
    "    df_train = df_train.drop('총 시술 횟수', axis=1)\n",
    "    return df_train\n",
    "\n",
    "def 배란유도유형(df_train, df_test):\n",
    "    mapping = {\n",
    "        '기록되지 않은 시행': 1,\n",
    "        '알 수 없음': 0,\n",
    "        '세트로타이드 (억제제)': 0,\n",
    "        '생식선 자극 호르몬': 0,\n",
    "    }\n",
    "    df_train['배란 유도 유형'] = df_train['배란 유도 유형'].replace(mapping)\n",
    "    df_test['배란 유도 유형'] = df_test['배란 유도 유형'].replace(mapping)\n",
    "    return df_train, df_test\n",
    "\n",
    "def 난자기증자나이(df_train, df_test):\n",
    "    mapping = {\n",
    "        '만20세 이하': 20,\n",
    "        '만21-25세': 25,\n",
    "        '만26-30세': 30,\n",
    "        '만31-35세': 35,\n",
    "        '알 수 없음': 20,  # 만20세 이하와 동일하게 처리\n",
    "    }\n",
    "    df_train['난자 기증자 나이'] = df_train['난자 기증자 나이'].replace(mapping)\n",
    "    df_test['난자 기증자 나이'] = df_test['난자 기증자 나이'].replace(mapping)\n",
    "    return df_train, df_test\n",
    "\n",
    "def 배아생성주요이유(df_train, df_test):\n",
    "    df_train['배아 생성 주요 이유'] = df_train['배아 생성 주요 이유'].fillna('DI')\n",
    "    df_test['배아 생성 주요 이유'] = df_test['배아 생성 주요 이유'].fillna('DI')\n",
    "\n",
    "    df_train['배아 생성 이유 리스트'] = df_train['배아 생성 주요 이유'].apply(lambda x: [reason.strip() for reason in x.split(',')])\n",
    "    df_test['배아 생성 이유 리스트'] = df_test['배아 생성 주요 이유'].apply(lambda x: [reason.strip() for reason in x.split(',')])\n",
    "\n",
    "    mlb = MultiLabelBinarizer()\n",
    "    train_one_hot = pd.DataFrame(\n",
    "        mlb.fit_transform(df_train['배아 생성 이유 리스트']),\n",
    "        columns=mlb.classes_,\n",
    "        index=df_train.index\n",
    "    )\n",
    "    train_one_hot.columns = ['배아생성이유_' + col for col in train_one_hot.columns]\n",
    "\n",
    "    test_one_hot = pd.DataFrame(\n",
    "        mlb.transform(df_test['배아 생성 이유 리스트']),\n",
    "        columns=mlb.classes_,\n",
    "        index=df_test.index\n",
    "    )\n",
    "    test_one_hot.columns = ['배아생성이유_' + col for col in test_one_hot.columns]\n",
    "\n",
    "    df_train = pd.concat([df_train, train_one_hot], axis=1)\n",
    "    df_test = pd.concat([df_test, test_one_hot], axis=1)\n",
    "\n",
    "    cols_to_drop = [\n",
    "        '배아 생성 주요 이유',\n",
    "        '배아 생성 이유 리스트',\n",
    "        '배아생성이유_연구용',\n",
    "        '배아생성이유_DI'\n",
    "    ]\n",
    "    df_train = df_train.drop(cols_to_drop, axis=1, errors='ignore')\n",
    "    df_test = df_test.drop(cols_to_drop, axis=1, errors='ignore')\n",
    "\n",
    "    cols = ['배아생성이유_기증용',\n",
    "            '배아생성이유_난자 저장용',\n",
    "            '배아생성이유_배아 저장용',\n",
    "            '배아생성이유_현재 시술용']\n",
    "\n",
    "    df_train[cols] = df_train[cols].div(df_train[cols].sum(axis=1).replace(0, np.nan), axis=0).fillna(0)\n",
    "    df_test[cols] = df_test[cols].div(df_test[cols].sum(axis=1).replace(0, np.nan), axis=0).fillna(0)\n",
    "\n",
    "    return df_train, df_test\n",
    "\n",
    "def 단일배아이식여부(df_train, df_val):\n",
    "    df_train['단일 배아 이식 여부'] = df_train['단일 배아 이식 여부'].fillna(0)\n",
    "    df_val['단일 배아 이식 여부'] = df_val['단일 배아 이식 여부'].fillna(0)\n",
    "    return df_train, df_val\n",
    "\n",
    "\n",
    "def 기증자정자와혼합된난자수(df_train, df_test):\n",
    "    df_train[\"기증자 정자와 혼합된 난자 수\"] = df_train[\"기증자 정자와 혼합된 난자 수\"].fillna(2)\n",
    "    df_test[\"기증자 정자와 혼합된 난자 수\"] = df_test[\"기증자 정자와 혼합된 난자 수\"].fillna(2)\n",
    "    return df_train, df_test\n",
    "\n",
    "def type_to_category(train, test, cols):\n",
    "    train[cols] = train[cols].astype('category')\n",
    "    test[cols] = test[cols].astype('category')\n",
    "    return train, test\n",
    "\n",
    "def impute_nan(train, test):\n",
    "    cols_to_impute = [\n",
    "        '임신 시도 또는 마지막 임신 경과 연수', # DI, IVF랑 관련 X\n",
    "    ]\n",
    "    imputer = SimpleImputer(strategy='mean')\n",
    "    train[cols_to_impute] = imputer.fit_transform(train[cols_to_impute])\n",
    "    test[cols_to_impute] = imputer.transform(test[cols_to_impute])\n",
    "\n",
    "    cols_to_impute = [\n",
    "        '난자 채취 경과일',\n",
    "        '난자 혼합 경과일',\n",
    "        '배아 이식 경과일',\n",
    "        '배아 해동 경과일',\n",
    "\n",
    "        '착상 전 유전 검사 사용 여부',\n",
    "        'PGD 시술 여부',\n",
    "        'PGS 시술 여부',\n",
    "\n",
    "        ### DI only\n",
    "        '착상 전 유전 진단 사용 여부',\n",
    "        '총 생성 배아 수',\n",
    "        '미세주입된 난자 수',\n",
    "        '미세주입에서 생성된 배아 수',\n",
    "        '이식된 배아 수',\n",
    "        '미세주입 배아 이식 수',\n",
    "        '저장된 배아 수',\n",
    "        '미세주입 후 저장된 배아 수',\n",
    "        '해동된 배아 수',\n",
    "        '해동 난자 수',\n",
    "        '수집된 신선 난자 수',\n",
    "        '저장된 신선 난자 수',\n",
    "        '혼합된 난자 수',\n",
    "        '파트너 정자와 혼합된 난자 수',\n",
    "        '기증자 정자와 혼합된 난자 수',\n",
    "        '동결 배아 사용 여부',\n",
    "        '신선 배아 사용 여부',\n",
    "        '기증 배아 사용 여부',\n",
    "        '대리모 여부',\n",
    "        ### DI\n",
    "    ]\n",
    "    train[cols_to_impute] = train[cols_to_impute].fillna(0)\n",
    "    test[cols_to_impute] = test[cols_to_impute].fillna(0)\n",
    "\n",
    "    return train, test\n",
    "\n",
    "def num_feature_scailing(train, test):\n",
    "    # cols_to_divide = [\n",
    "    #     '연간 소득',\n",
    "    #     '최대 신용한도',\n",
    "    #     '현재 대출 잔액',\n",
    "    #     '현재 미상환 신용액',\n",
    "    #     '월 상환 부채액',\n",
    "    # ]\n",
    "    # train[cols_to_divide] = train[cols_to_divide] / 100000\n",
    "    # test[cols_to_divide] = test[cols_to_divide] / 100000\n",
    "\n",
    "    cols_to_log = [\n",
    "        '총 생성 배아 수',\n",
    "    ]\n",
    "    log_transformer = FunctionTransformer(np.log1p, validate=True)\n",
    "    train[cols_to_log] = log_transformer.fit_transform(train[cols_to_log])\n",
    "    test[cols_to_log] = log_transformer.transform(test[cols_to_log])\n",
    "\n",
    "    # numeric_cols = train.select_dtypes(include=[\"number\"]).columns.tolist()\n",
    "    # cat_cols = [col for col in train.columns if pd.api.types.is_categorical_dtype(train[col])]\n",
    "    # cols_to_scale = [\n",
    "    #     col for col in numeric_cols\n",
    "    #     if col not in cat_cols and col != '임신 성공 여부'\n",
    "    # ]\n",
    "    # scaler = StandardScaler()\n",
    "    # train[cols_to_scale] = scaler.fit_transform(train[cols_to_scale])\n",
    "    # test[cols_to_scale] = scaler.transform(test[cols_to_scale])\n",
    "\n",
    "    return train, test\n",
    "\n",
    "def all_process(train, val):\n",
    "    # 기본 전처리 단계\n",
    "    train, val = drop_columns(train), drop_columns(val)\n",
    "    train, val = 특정시술유형(train, val)\n",
    "    train, val = 시술횟수(train), 시술횟수(val)\n",
    "\n",
    "    cols_to_encoding = [\n",
    "        \"시술 시기 코드\",\n",
    "        \"시술 당시 나이\",\n",
    "        \"배란 유도 유형\",\n",
    "        # \"클리닉 내 총 시술 횟수\",\n",
    "        # \"IVF 시술 횟수\",\n",
    "        # \"DI 시술 횟수\",\n",
    "        # \"총 임신 횟수\",\n",
    "        # \"IVF 임신 횟수\",\n",
    "        # \"DI 임신 횟수\",\n",
    "        # \"총 출산 횟수\",\n",
    "        # \"IVF 출산 횟수\",\n",
    "        # \"DI 출산 횟수\",\n",
    "        \"난자 출처\",\n",
    "        \"정자 출처\",\n",
    "        \"난자 기증자 나이\",\n",
    "        \"정자 기증자 나이\",\n",
    "        '시술유형_통합',\n",
    "    ]\n",
    "    train, val = type_to_category(train, val, cols=cols_to_encoding)\n",
    "\n",
    "    train, val = 단일배아이식여부(train, val)\n",
    "    train, val = 배란유도유형(train, val)\n",
    "    train, val = 배아생성주요이유(train, val)\n",
    "\n",
    "    train, val = impute_nan(train, val)\n",
    "    train, val = num_feature_scailing(train, val)\n",
    "\n",
    "    return train, val\n",
    "\n",
    "train = pd.read_csv(train_path).drop(columns=['ID'])\n",
    "test = pd.read_csv(test_path).drop(columns=['ID'])\n",
    "\n",
    "train, test = all_process(train, test)\n",
    "\n",
    "print(train.shape, test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train, _ = all_process(train, train)\n",
    "\n",
    "cat_cols = [col for col in train.columns if pd.api.types.is_categorical_dtype(train[col])]\n",
    "num_cols = train.select_dtypes(include=[\"number\"]).columns.tolist()\n",
    "num_cols = [\n",
    "    col for col in num_cols\n",
    "    if col not in cat_cols and col != '임신 성공 여부'\n",
    "]\n",
    "print(f'범주형 변수: \\n {cat_cols}')\n",
    "print(f'수치형 변수: \\n {num_cols}, {len(num_cols)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "## 4. TABR 모델 정의 (PyTorch)\n",
    "\n",
    "# TABR 모델은 **Transformer 인코더**를 기반으로 표 형태 데이터의 특성 간 상호작용을 학습하는 모델입니다. 각 행(row)은 하나의 샘플에 해당하며, 여러 열(column)이 하나의 \"피처 시퀀스\"로 간주됩니다. 모델 구조를 간략히 설명하면 다음과 같습니다:\n",
    "\n",
    "# - **입력 임베딩:** 범주형 변수는 정수 인코딩된 값을 임베딩 벡터로 변환하고, 수치형 변수는 1차원 값을 선형 투영하여 임베딩 차원으로 변환합니다. 결과적으로 모든 특징은 동일한 임베딩 차원 `d_model`의 벡터 표현을 갖게 됩니다. 추가로, 각 **특성(column)에 대한 고유한 임베딩**을 더해줘 모델이 각 벡터의 의미(어떤 컬럼인지)를 구분할 수 있도록 합니다. (이는 Transformer에서 위치 임베딩을 추가하는 것과 유사하며, 여기서는 열 위치/ID 임베딩으로 볼 수 있습니다.)\n",
    "# - **Transformer 인코더:** 임베딩된 특징 벡터 시퀀스를 입력으로 하여 여러 층의 Multi-Head Self-Attention과 피드포워드 층으로 구성된 Transformer 인코더를 통과시킵니다. Self-Attention 메커니즘을 통해 **특성 간의 복합 관계**를 학습합니다. (`n_heads`는 헤드 수, `n_layers`는 인코더 레이어 수)\n",
    "# - **출력 및 분류:** Transformer 인코더의 출력을 일렬로 평탄화하여 하나의 긴 벡터로 만들고, 최종적으로 **이진 분류를 위한 출력 노드**에 연결합니다. (이 예에서는 `임신 성공 여부` 예측이므로 sigmoid를 거치기 전의 로지스틱 출력 하나를 생성합니다.)\n",
    "\n",
    "# 이제 PyTorch `nn.Module`로 TABR 모델을 구현하겠습니다.\n",
    "\n",
    "# **모델 구현 고려사항:**\n",
    "# - `cat_dims`: 각 범주형 특성별 임베딩 필요 크기 (고유값 개수 + 1).\n",
    "# - `num_numeric`: 수치형 특성 개수 (추가된 KNN 특성 포함). 각 수치형 특성마다 하나의 작은 `nn.Linear(1 -> d_model)` 층을 사용합니다.\n",
    "# - 모든 임베딩/투영 결과 벡터의 차원은 `embed_dim = d_model`로 통일합니다.\n",
    "# - Feature(컬럼) 임베딩: `self.feature_embeds = nn.Embedding(total_features, embed_dim)`를 통해 전체 특징 수 만큼의 임베딩 벡터를 정의합니다. 입력 시퀀스 순서에 따라 해당 벡터를 더합니다.\n",
    "# - Transformer 인코더: `nn.TransformerEncoder`를 사용하여 지정한 층 수만큼 반복 적용합니다.\n",
    "# - 최종 분류 층: `nn.Linear(total_features * embed_dim, 1)`로 flatten된 모든 특징의 정보를 받아 이진 분류 출력 산출.\n",
    "\n",
    "# python\n",
    "\n",
    "# TABR 모델 정의\n",
    "class TABRModel(nn.Module):\n",
    "    def __init__(self, cat_dims, num_numeric, embed_dim=32, n_heads=4, n_layers=2, dropout=0.1):\n",
    "        \"\"\"\n",
    "        cat_dims: 각 범주형 특성의 임베딩 카테고리 개수 리스트 (고유값 + 1)\n",
    "        num_numeric: 수치형 특성 개수\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        # 범주형 특성 임베딩 레이어 생성 (각 범주형 컬럼마다 별도 Embedding)\n",
    "        self.cat_embeds = nn.ModuleList([\n",
    "            nn.Embedding(num_categories, embed_dim)\n",
    "            for num_categories in cat_dims\n",
    "        ])\n",
    "        # 수치형 특성 투영 레이어 생성 (각 수치형 컬럼마다 별도 Linear)\n",
    "        self.num_proj = nn.ModuleList([\n",
    "            \\\n",
    "            nn.Linear(1, embed_dim)\n",
    "            for _ in range(num_numeric)\n",
    "        ])\n",
    "\n",
    "        # 전체 특징 개수 (범주형 + 수치형)\n",
    "        self.total_features = len(cat_dims) + num_numeric\n",
    "        # 특징 위치 임베딩 (특정 열(column) 전용 임베딩)\n",
    "        self.feature_embeds = nn.Embedding(self.total_features, embed_dim)\n",
    "        # Transformer 인코더 설정\n",
    "        encoder_layer = nn.TransformerEncoderLayer(\n",
    "            d_model=embed_dim, nhead=n_heads, dim_feedforward=embed_dim*4, dropout=dropout\n",
    "        )\n",
    "        self.transformer = nn.TransformerEncoder(encoder_layer, num_layers=n_layers)\n",
    "        # 최종 분류를 위한 출력 층\n",
    "        self.classifier = nn.Linear(self.total_features * embed_dim, 1)\n",
    "\n",
    "    def forward(self, x_cat, x_num):\n",
    "        \"\"\"\n",
    "        x_cat: LongTensor [batch_size, n_cat_features] - 범주형 입력 (정수 코드)\n",
    "        x_num: FloatTensor [batch_size, n_num_features] - 수치형 입력 (실수)\n",
    "        \"\"\"\n",
    "        batch_size = x_cat.size(0)\n",
    "        # 1. 범주형 임베딩: 각 범주형 컬럼별 임베딩 벡터\n",
    "        cat_tokens = []  # 각 토큰: [batch, embed_dim]\n",
    "        for i, embed_layer in enumerate(self.cat_embeds):\n",
    "            cat_tok = embed_layer(x_cat[:, i])    # i번째 범주형 컬럼 -> 임베딩\n",
    "            cat_tokens.append(cat_tok)\n",
    "        # 2. 수치형 투영: 각 수치형 컬럼별 Linear 투영\n",
    "        num_tokens = []\n",
    "        for j, linear_layer in enumerate(self.num_proj):\n",
    "            # x_num의 j번째 컬럼 (batch_size,) -> (batch_size,1)로 shape 변환 후 Linear\n",
    "            num_val = x_num[:, j].unsqueeze(1)\n",
    "            num_tok = linear_layer(num_val)\n",
    "            num_tokens.append(num_tok)\n",
    "        # 3. 모든 특징 토큰 결합 (범주형 + 수치형 순서 연결)\n",
    "        tokens = cat_tokens + num_tokens   # 리스트 합치기\n",
    "        # tokens 리스트 길이는 total_features, 각 원소 shape [batch_size, embed_dim]\n",
    "        # 리스트를 스택하여 shape: [batch_size, total_features, embed_dim]\n",
    "        x = torch.stack(tokens, dim=1)\n",
    "        # 4. 특징 위치 임베딩 추가\n",
    "        seq_indices = torch.arange(self.total_features, device=x.device)  # [0,1,...,total_features-1]\n",
    "        seq_emb = self.feature_embeds(seq_indices)  # [total_features, embed_dim]\n",
    "        # 배치 차원으로 broadcast 위해 unsqueeze 후 더하기\n",
    "        x = x + seq_emb.unsqueeze(0)  # [1, seq_len, embed_dim] -> broadcasting add\n",
    "        # 5. Transformer 인코더 적용\n",
    "        # PyTorch Transformer는 입력 shape: [seq_len, batch, embed_dim]을 기대하므로 전치(transpose)\n",
    "        x = x.permute(1, 0, 2)  # [seq_len, batch_size, embed_dim]\n",
    "        x = self.transformer(x)  # 인코더 통과\n",
    "        x = x.permute(1, 0, 2)   # [batch_size, seq_len, embed_dim]로 복귀\n",
    "        # 6. 토큰별 출력 평탄화 (flatten)\n",
    "        x = x.reshape(batch_size, -1)  # [batch_size, seq_len*embed_dim]\n",
    "        # 7. 최종 이진 분류 출력\n",
    "        out = self.classifier(x)  # [batch_size, 1] 로지스틱 출력 (시그모이드 전)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_splits = 5\n",
    "seed_lst = [333]\n",
    "\n",
    "for seed in seed_lst:\n",
    "    skf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=seed)\n",
    "    train = pd.read_csv(train_path).drop(columns=['ID'])\n",
    "    test = pd.read_csv(test_path).drop(columns=['ID'])\n",
    "\n",
    "    fold_roc_auc_scores = []\n",
    "\n",
    "    for fold, (train_idx, val_idx) in enumerate(skf.split(train, train['임신 성공 여부'])):\n",
    "        print(f\"\\n----- [{seed}, 폴드 {fold}] -----\")\n",
    "\n",
    "        # 데이터 분할 및 전처리\n",
    "        train_df = train.iloc[train_idx].reset_index(drop=True)\n",
    "        val_df = train.iloc[val_idx].reset_index(drop=True)\n",
    "\n",
    "        train_proc, val_proc = all_process(train_df.copy(), val_df.copy())\n",
    "\n",
    "        y_train = train_proc['임신 성공 여부'].copy()\n",
    "        y_val = val_proc['임신 성공 여부'].copy()\n",
    "        X_train = train_proc.drop('임신 성공 여부', axis=1)\n",
    "        X_val = val_proc.drop('임신 성공 여부', axis=1)\n",
    "\n",
    "        # 범주형 정수 인코딩\n",
    "        cat_dims = []\n",
    "        for col in cat_cols:\n",
    "            X_train[col] = X_train[col].astype('category')\n",
    "            train_categories = X_train[col].cat.categories\n",
    "            X_val[col] = pd.Categorical(X_val[col], categories=train_categories)\n",
    "            X_train[col] = X_train[col].cat.codes.astype(int)\n",
    "            X_val[col] = X_val[col].cat.codes.astype(int)\n",
    "            cat_dims.append(len(train_categories) + 1)\n",
    "\n",
    "        # 수치형 표준화\n",
    "        scaler = StandardScaler()\n",
    "        X_train[num_cols] = scaler.fit_transform(X_train[num_cols])\n",
    "        X_val[num_cols] = scaler.transform(X_val[num_cols])\n",
    "\n",
    "        # KNN 이웃 평균\n",
    "        m = 96\n",
    "        knn_model = NearestNeighbors(n_neighbors=m, algorithm='auto')\n",
    "        knn_model.fit(X_train.values)\n",
    "\n",
    "        neigh_idx_train = knn_model.kneighbors(X_train.values, return_distance=False)\n",
    "        neigh_idx_val = knn_model.kneighbors(X_val.values, return_distance=False)\n",
    "\n",
    "        neigh_idx_train_no_self = []\n",
    "        for i, neighbors in enumerate(neigh_idx_train):\n",
    "            neigh_idx = [n for n in neighbors if n != i][:m]\n",
    "            neigh_idx_train_no_self.append(neigh_idx)\n",
    "        neigh_idx_train_no_self = np.array(neigh_idx_train_no_self)\n",
    "\n",
    "        y_train_array = y_train.to_numpy()\n",
    "        knn_feat_train = y_train_array[neigh_idx_train_no_self].mean(axis=1)\n",
    "        knn_feat_val = y_train_array[neigh_idx_val].mean(axis=1)\n",
    "        X_train['knn_target_mean'] = knn_feat_train\n",
    "        X_val['knn_target_mean'] = knn_feat_val\n",
    "\n",
    "        # KNN 평균 표준화\n",
    "        num_cols_with_knn = num_cols + ['knn_target_mean']\n",
    "        scaler_knn = StandardScaler()\n",
    "        X_train[['knn_target_mean']] = scaler_knn.fit_transform(X_train[['knn_target_mean']])\n",
    "        X_val[['knn_target_mean']] = scaler_knn.transform(X_val[['knn_target_mean']])\n",
    "\n",
    "        # 텐서 변환\n",
    "        X_train_cat = torch.tensor(X_train[cat_cols].values, dtype=torch.long)\n",
    "        X_train_num = torch.tensor(X_train[num_cols_with_knn].values, dtype=torch.float32)\n",
    "        y_train_tensor = torch.tensor(y_train.values, dtype=torch.float32)\n",
    "\n",
    "        X_val_cat = torch.tensor(X_val[cat_cols].values, dtype=torch.long)\n",
    "        X_val_num = torch.tensor(X_val[num_cols_with_knn].values, dtype=torch.float32)\n",
    "        y_val_tensor = torch.tensor(y_val.values, dtype=torch.float32)\n",
    "\n",
    "        # DataLoader\n",
    "        batch_size = 4096\n",
    "        train_dataset = TensorDataset(X_train_cat, X_train_num, y_train_tensor)\n",
    "        val_dataset = TensorDataset(X_val_cat, X_val_num, y_val_tensor)\n",
    "        train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, pin_memory=True)\n",
    "        val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "        # 모델 정의 및 학습\n",
    "        model = TABRModel(cat_dims=cat_dims, num_numeric=len(num_cols_with_knn), embed_dim=32, n_heads=4, n_layers=2, dropout=0.1)\n",
    "        device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "        model.to(device)\n",
    "\n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "        criterion = nn.BCEWithLogitsLoss()\n",
    "\n",
    "        early_stopping_patience = 3\n",
    "        patience_counter = 0\n",
    "        best_loss = float('inf')\n",
    "        best_epoch = -1\n",
    "\n",
    "        num_epochs = 100\n",
    "        for epoch in range(1, num_epochs + 1):\n",
    "            model.train()\n",
    "            running_loss = 0.0\n",
    "            for batch in train_loader:\n",
    "                cat_batch, num_batch, labels = batch\n",
    "                cat_batch = cat_batch.to(device)\n",
    "                num_batch = num_batch.to(device)\n",
    "                labels = labels.to(device)\n",
    "\n",
    "                optimizer.zero_grad()\n",
    "                outputs = model(cat_batch, num_batch).squeeze(1)\n",
    "                loss = criterion(outputs, labels)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                running_loss += loss.item() * labels.size(0)\n",
    "\n",
    "            avg_loss = running_loss / len(train_dataset)\n",
    "            print(f\"Fold {fold} - Epoch {epoch:02d}: Training Loss = {avg_loss:.4f}\")\n",
    "\n",
    "            # 검증\n",
    "            model.eval()\n",
    "            val_loss_total = 0.0\n",
    "            with torch.no_grad():\n",
    "                for batch in val_loader:\n",
    "                    cat_batch, num_batch, labels = batch\n",
    "                    cat_batch = cat_batch.to(device)\n",
    "                    num_batch = num_batch.to(device)\n",
    "                    labels = labels.to(device)\n",
    "                    outputs = model(cat_batch, num_batch).squeeze(1)\n",
    "                    loss = criterion(outputs, labels)\n",
    "                    val_loss_total += loss.item() * labels.size(0)\n",
    "\n",
    "            avg_val_loss = val_loss_total / len(val_dataset)\n",
    "            print(f\"Fold {fold} - Epoch {epoch:02d}: Validation Loss = {avg_val_loss:.4f}\")\n",
    "\n",
    "            if avg_val_loss < best_loss:\n",
    "                best_loss = avg_val_loss\n",
    "                best_epoch = epoch\n",
    "                torch.save({\n",
    "                    'model_state_dict': model.state_dict(),\n",
    "                    'knn_model': knn_model,\n",
    "                    'scaler_knn': scaler_knn,\n",
    "                }, f'data seed: 7, best_model_fold{fold}.pth')\n",
    "                patience_counter = 0\n",
    "            else:\n",
    "                patience_counter += 1\n",
    "\n",
    "            if patience_counter >= early_stopping_patience:\n",
    "                print(f\"Early stopping triggered at epoch {epoch}. No improvement for {early_stopping_patience} consecutive epochs.\")\n",
    "                break\n",
    "\n",
    "        print(f\"Fold {fold}: Best Validation Loss = {best_loss:.4f} at Epoch {best_epoch}\")\n",
    "        fold_roc_auc_scores.append(-best_loss)\n",
    "\n",
    "    # fold 루프 끝나고 평균 출력\n",
    "    valid_losses = [-v for v in fold_roc_auc_scores if v != float('inf')]\n",
    "    if valid_losses:\n",
    "        mean_val_loss = np.mean(valid_losses)\n",
    "        print(f\"\\nAverage Validation Loss across folds: {mean_val_loss:.4f}\")\n",
    "    else:\n",
    "        print(\"No valid Validation Loss values were computed across folds.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import numpy as np\n",
    "\n",
    "test_pred=[]\n",
    "\n",
    "# 0. 디바이스 설정\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "for fold_num in range(5):\n",
    "    # 1. 저장된 모델과 KNN 관련 객체 먼저 로드\n",
    "    checkpoint = torch.load(f'data seed: 7, best_model_fold{fold_num}.pth', map_location=device, weights_only=False)\n",
    "    knn_model = checkpoint['knn_model']\n",
    "    scaler_knn = checkpoint['scaler_knn']\n",
    "\n",
    "    # 2. 데이터 로드 및 전처리\n",
    "    test = pd.read_csv(test_path).drop(columns=['ID'])\n",
    "    train = pd.read_csv(train_path).drop(columns=['ID'])\n",
    "    train_proc, test_proc = all_process(train.copy(), test.copy())  # 전처리\n",
    "    X_test = test_proc.copy()\n",
    "    y_train_array = train_proc['임신 성공 여부'].to_numpy()\n",
    "\n",
    "    # 3. 범주형 정수 인코딩 (train 기준으로)\n",
    "    cat_dims = []\n",
    "    for col in cat_cols:\n",
    "        X_test[col] = X_test[col].astype('category')\n",
    "        train_cat = train_proc[col].astype('category')\n",
    "        train_categories = train_cat.cat.categories\n",
    "        X_test[col] = pd.Categorical(X_test[col], categories=train_categories)\n",
    "        test_codes = X_test[col].cat.codes.replace(-1, len(train_categories))  # unseen → 마지막 인덱스\n",
    "        X_test[col] = test_codes.astype(int)\n",
    "        cat_dims.append(len(train_categories) + 1)\n",
    "\n",
    "    # 4. 수치형 정규화 (train 기준)\n",
    "    scaler = StandardScaler()\n",
    "    X_test[num_cols] = scaler.fit(train_proc[num_cols]).transform(X_test[num_cols])\n",
    "\n",
    "    # 5. KNN 기반 특성 생성\n",
    "    neigh_idx_test = knn_model.kneighbors(X_test.values, n_neighbors=96, return_distance=False)\n",
    "    knn_feat_test = y_train_array[neigh_idx_test].mean(axis=1)\n",
    "    X_test['knn_target_mean'] = knn_feat_test\n",
    "    X_test[['knn_target_mean']] = scaler_knn.transform(X_test[['knn_target_mean']])\n",
    "\n",
    "    # 6. PyTorch 텐서 변환\n",
    "    X_test_cat = torch.tensor(X_test[cat_cols].values, dtype=torch.long)\n",
    "    X_test_num = torch.tensor(X_test[num_cols + ['knn_target_mean']].values, dtype=torch.float32)\n",
    "\n",
    "    # 7. DataLoader 생성\n",
    "    test_dataset = TensorDataset(X_test_cat, X_test_num)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=1024, shuffle=False)\n",
    "\n",
    "    # 8. 모델 정의 및 파라미터 로드\n",
    "    model = TABRModel(\n",
    "        cat_dims=cat_dims,\n",
    "        num_numeric=len(num_cols + ['knn_target_mean']),\n",
    "        embed_dim=32,\n",
    "        n_heads=4,\n",
    "        n_layers=2,\n",
    "        dropout=0.1\n",
    "    )\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "\n",
    "    # 9. 예측 수행\n",
    "    all_preds = []\n",
    "    with torch.no_grad():\n",
    "        for cat_batch, num_batch in test_loader:\n",
    "            cat_batch = cat_batch.to(device)\n",
    "            num_batch = num_batch.to(device)\n",
    "            outputs = model(cat_batch, num_batch).squeeze(1)\n",
    "            preds = torch.sigmoid(outputs)\n",
    "            all_preds.extend(preds.cpu().numpy())\n",
    "\n",
    "\n",
    "    test_pred.append(all_preds)\n",
    "    print(f\"{fold_num} is ended\")\n",
    "\n",
    "last_pred=np.mean(test_pred, axis=0)\n",
    "\n",
    "#data seed ( 1 / 7 )\n",
    "train, test = train_test_split(origin, test_size=0.2, random_state=7, stratify=origin['임신 성공 여부'])\n",
    "y_test=test['임신 성공 여부'].copy()\n",
    "auc = roc_auc_score(y_test, last_pred)\n",
    "print(f\"fold: {fold_num}, AUC: {auc:.6f}\")\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
