{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# WideDeep_BayesianWide"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 라이브러리 설치"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install numpy==1.26.0\n",
    "# !pip install pandas==2.2.2\n",
    "# !pip install scikit-learn==1.5.1\n",
    "# !pip install scipy==1.14.1\n",
    "# !pip install statsmodels==0.14.2\n",
    "# !pip install joblib==1.4.2\n",
    "# !pip install threadpoolctl==3.5.0\n",
    "# !pip install ipynbname"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 1. 기존의 패키지 정리\n",
    "# !pip uninstall -y torch torchvision torchaudio pytorch-lightning pytorch-tabular\n",
    "\n",
    "# # 2. 호환 가능한 버전으로 재설치\n",
    "# !pip install torch==2.0.1+cu118 --index-url https://download.pytorch.org/whl/cu118\n",
    "# !pip install pytorch-tabular==1.1.1 --no-deps\n",
    "# !pip install pytorch-lightning==2.0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip3 install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install pytorch-widedeep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install opencv-python-headless"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sqlite3\n",
    "import shutil\n",
    "import ipynbname\n",
    "import datetime\n",
    "import os\n",
    "import warnings\n",
    "import random\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "\n",
    "\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.preprocessing import LabelEncoder, FunctionTransformer, QuantileTransformer, MultiLabelBinarizer\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "\n",
    "\n",
    "import torch\n",
    "\n",
    "# pytorch-widedeep 라이브러리 import\n",
    "from pytorch_widedeep import Trainer\n",
    "from pytorch_widedeep.preprocessing import WidePreprocessor, TabPreprocessor\n",
    "from pytorch_widedeep.models import Wide, FTTransformer, WideDeep\n",
    "from pytorch_widedeep.bayesian_models import BayesianWide, BayesianTabMlp\n",
    "from pytorch_widedeep.training.bayesian_trainer import BayesianTrainer\n",
    "from pytorch_widedeep.metrics import Accuracy,F1Score\n",
    "from pytorch_widedeep.callbacks import EarlyStopping, ModelCheckpoint\n",
    "\n",
    "\n",
    "##################\n",
    "data_seed=1\n",
    "\n",
    "train_path = f'./data/custom_train_{data_seed}.csv'\n",
    "test_path = f'./data/custom_test_{data_seed}.csv'\n",
    "\n",
    "\n",
    "train_path = f'C:/Users/User/Desktop/LG Aimers/LG_Aimers_6th/data/custom_train_{data_seed}.csv'\n",
    "test_path = f'C:/Users/User/Desktop/LG Aimers/LG_Aimers_6th/data/custom_test_{data_seed}.csv'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(205080, 68) (51271, 67)\n"
     ]
    }
   ],
   "source": [
    "train = pd.read_csv(train_path).drop(columns=[\"ID\"])\n",
    "test = pd.read_csv(test_path).drop(columns=[\"ID\"])\n",
    "print(train.shape, test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def drop_columns(df):\n",
    "    cols = [\n",
    "        '불임 원인 - 여성 요인',  # 고유값 1\n",
    "        '불임 원인 - 정자 면역학적 요인',  # train, test 모두 '1'인 데이터 1개 >> 신뢰할 수 없음\n",
    "        '난자 해동 경과일',\n",
    "    ]\n",
    "    df = df.drop(cols, axis=1)\n",
    "    return df\n",
    "\n",
    "def 특정시술유형(train, test):\n",
    "    def categorize_procedure(proc):\n",
    "        tokens = [token.strip() for token in proc.split(\",\") if token.strip() and not token.strip().isdigit()]\n",
    "        # 우선순위에 따른 범주화\n",
    "        if tokens.count(\"Unknown\") >= 1:\n",
    "            return \"Unknown\"\n",
    "        if tokens.count(\"AH\") >= 1:\n",
    "            return \"AH\"\n",
    "        if tokens.count(\"BLASTOCYST\") >= 1:\n",
    "            return \"BLASTOCYST\"\n",
    "        if tokens.count(\"ICSI\") >= 2 or tokens.count(\"IVF\") >= 2:\n",
    "            return \"2ICSI_2IVF\"\n",
    "        if tokens.count(\"IVF\") >= 1 and tokens.count(\"ICSI\") >= 1:\n",
    "            return \"IVF_ICSI\"\n",
    "        if tokens == \"ICSI\":\n",
    "            return \"ICSI\"\n",
    "        if tokens == \"IVF\":\n",
    "            return \"IVF\"\n",
    "        return \",\".join(tokens) if tokens else None\n",
    "\n",
    "    for df in [train, test]:\n",
    "        df['특정 시술 유형'] = df['특정 시술 유형'].str.replace(\" / \", \",\")\n",
    "        df['특정 시술 유형'] = df['특정 시술 유형'].str.replace(\":\", \",\")\n",
    "        df['특정 시술 유형'] = df['특정 시술 유형'].str.replace(\" \", \"\")\n",
    "\n",
    "    counts = train['특정 시술 유형'].value_counts()\n",
    "    allowed_categories = counts[counts >= 100].index.tolist()\n",
    "\n",
    "    # allowed_categories에 속하지 않는 값은 \"Unknown\"으로 대체\n",
    "    train.loc[~train['특정 시술 유형'].isin(allowed_categories), '특정 시술 유형'] = \"Unknown\"\n",
    "    test.loc[~test['특정 시술 유형'].isin(allowed_categories), '특정 시술 유형'] = \"Unknown\"\n",
    "\n",
    "    train['특정 시술 유형'] = train['특정 시술 유형'].apply(categorize_procedure)\n",
    "    test['특정 시술 유형'] = test['특정 시술 유형'].apply(categorize_procedure)\n",
    "\n",
    "    train['시술유형_통합'] = train['시술 유형'].astype(str) + '_' + train['특정 시술 유형'].astype(str)\n",
    "    test['시술유형_통합'] = test['시술 유형'].astype(str) + '_' + test['특정 시술 유형'].astype(str)\n",
    "\n",
    "    drop_cols = ['시술 유형', '특정 시술 유형']\n",
    "    train = train.drop(drop_cols, axis=1)\n",
    "    test = test.drop(drop_cols, axis=1)\n",
    "\n",
    "    return train, test\n",
    "\n",
    "def 시술횟수(df_train):\n",
    "    for col in [col for col in df_train.columns if '횟수' in col]:\n",
    "        df_train[col] = df_train[col].replace({'6회 이상':'6회'})\n",
    "        df_train[col] = df_train[col].str[0].astype(int)\n",
    "    df_train['시술_임신'] = df_train['총 임신 횟수'] - df_train['총 시술 횟수']\n",
    "    df_train = df_train.drop('총 시술 횟수', axis=1)\n",
    "    return df_train\n",
    "\n",
    "def 배란유도유형(df_train, df_test):\n",
    "    mapping = {\n",
    "        '기록되지 않은 시행': 1,\n",
    "        '알 수 없음': 0,\n",
    "        '세트로타이드 (억제제)': 0,\n",
    "        '생식선 자극 호르몬': 0,\n",
    "    }\n",
    "    df_train['배란 유도 유형'] = df_train['배란 유도 유형'].replace(mapping)\n",
    "    df_test['배란 유도 유형'] = df_test['배란 유도 유형'].replace(mapping)\n",
    "    return df_train, df_test\n",
    "\n",
    "def 난자기증자나이(df_train, df_test):\n",
    "    mapping = {\n",
    "        '만20세 이하': 20,\n",
    "        '만21-25세': 25,\n",
    "        '만26-30세': 30,\n",
    "        '만31-35세': 35,\n",
    "        '알 수 없음': 20,  # 만20세 이하와 동일하게 처리\n",
    "    }\n",
    "    df_train['난자 기증자 나이'] = df_train['난자 기증자 나이'].replace(mapping)\n",
    "    df_test['난자 기증자 나이'] = df_test['난자 기증자 나이'].replace(mapping)\n",
    "    return df_train, df_test\n",
    "\n",
    "def 배아생성주요이유(df_train, df_test):\n",
    "    df_train['배아 생성 주요 이유'] = df_train['배아 생성 주요 이유'].fillna('DI')\n",
    "    df_test['배아 생성 주요 이유'] = df_test['배아 생성 주요 이유'].fillna('DI')\n",
    "\n",
    "    df_train['배아 생성 이유 리스트'] = df_train['배아 생성 주요 이유'].apply(lambda x: [reason.strip() for reason in x.split(',')])\n",
    "    df_test['배아 생성 이유 리스트'] = df_test['배아 생성 주요 이유'].apply(lambda x: [reason.strip() for reason in x.split(',')])\n",
    "\n",
    "    mlb = MultiLabelBinarizer()\n",
    "    train_one_hot = pd.DataFrame(\n",
    "        mlb.fit_transform(df_train['배아 생성 이유 리스트']),\n",
    "        columns=mlb.classes_,\n",
    "        index=df_train.index\n",
    "    )\n",
    "    train_one_hot.columns = ['배아생성이유_' + col for col in train_one_hot.columns]\n",
    "\n",
    "    test_one_hot = pd.DataFrame(\n",
    "        mlb.transform(df_test['배아 생성 이유 리스트']),\n",
    "        columns=mlb.classes_,\n",
    "        index=df_test.index\n",
    "    )\n",
    "    test_one_hot.columns = ['배아생성이유_' + col for col in test_one_hot.columns]\n",
    "\n",
    "    df_train = pd.concat([df_train, train_one_hot], axis=1)\n",
    "    df_test = pd.concat([df_test, test_one_hot], axis=1)\n",
    "\n",
    "    cols_to_drop = [\n",
    "        '배아 생성 주요 이유',\n",
    "        '배아 생성 이유 리스트',\n",
    "        '배아생성이유_연구용',\n",
    "        '배아생성이유_DI'\n",
    "    ]\n",
    "    df_train = df_train.drop(cols_to_drop, axis=1, errors='ignore')\n",
    "    df_test = df_test.drop(cols_to_drop, axis=1, errors='ignore')\n",
    "\n",
    "    cols = ['배아생성이유_기증용',\n",
    "            '배아생성이유_난자 저장용',\n",
    "            '배아생성이유_배아 저장용',\n",
    "            '배아생성이유_현재 시술용']\n",
    "\n",
    "    df_train[cols] = df_train[cols].div(df_train[cols].sum(axis=1).replace(0, np.nan), axis=0).fillna(0)\n",
    "    df_test[cols] = df_test[cols].div(df_test[cols].sum(axis=1).replace(0, np.nan), axis=0).fillna(0)\n",
    "\n",
    "    return df_train, df_test\n",
    "\n",
    "def 단일배아이식여부(df_train, df_val):\n",
    "    df_train['단일 배아 이식 여부'] = df_train['단일 배아 이식 여부'].fillna(0)\n",
    "    df_val['단일 배아 이식 여부'] = df_val['단일 배아 이식 여부'].fillna(0)\n",
    "    return df_train, df_val\n",
    "\n",
    "\n",
    "def 기증자정자와혼합된난자수(df_train, df_test):\n",
    "    df_train[\"기증자 정자와 혼합된 난자 수\"] = df_train[\"기증자 정자와 혼합된 난자 수\"].fillna(2)\n",
    "    df_test[\"기증자 정자와 혼합된 난자 수\"] = df_test[\"기증자 정자와 혼합된 난자 수\"].fillna(2)\n",
    "    return df_train, df_test\n",
    "\n",
    "def label_encoding(train, test, cols):\n",
    "    encoder = LabelEncoder()\n",
    "    for col in cols:\n",
    "        train[col] = encoder.fit_transform(train[col])\n",
    "        test[col] = encoder.transform(test[col])\n",
    "    return train, test\n",
    "\n",
    "def type_to_category(train, test, cols):\n",
    "    train[cols] = train[cols].astype('category')\n",
    "    test[cols] = test[cols].astype('category')\n",
    "    return train, test\n",
    "\n",
    "def impute_nan(train, test):\n",
    "    cols_to_impute = [\n",
    "        '임신 시도 또는 마지막 임신 경과 연수', # DI, IVF랑 관련 X\n",
    "    ]\n",
    "    imputer = SimpleImputer(strategy='mean')\n",
    "    train[cols_to_impute] = imputer.fit_transform(train[cols_to_impute])\n",
    "    test[cols_to_impute] = imputer.transform(test[cols_to_impute])\n",
    "\n",
    "    cols_to_impute = [\n",
    "        '난자 채취 경과일',\n",
    "        '난자 혼합 경과일',\n",
    "        '배아 이식 경과일',\n",
    "        '배아 해동 경과일',\n",
    "\n",
    "        '착상 전 유전 검사 사용 여부',\n",
    "        'PGD 시술 여부',\n",
    "        'PGS 시술 여부',\n",
    "\n",
    "        ### DI only\n",
    "        '착상 전 유전 진단 사용 여부',\n",
    "        '총 생성 배아 수',\n",
    "        '미세주입된 난자 수',\n",
    "        '미세주입에서 생성된 배아 수',\n",
    "        '이식된 배아 수',\n",
    "        '미세주입 배아 이식 수',\n",
    "        '저장된 배아 수',\n",
    "        '미세주입 후 저장된 배아 수',\n",
    "        '해동된 배아 수',\n",
    "        '해동 난자 수',\n",
    "        '수집된 신선 난자 수',\n",
    "        '저장된 신선 난자 수',\n",
    "        '혼합된 난자 수',\n",
    "        '파트너 정자와 혼합된 난자 수',\n",
    "        '기증자 정자와 혼합된 난자 수',\n",
    "        '동결 배아 사용 여부',\n",
    "        '신선 배아 사용 여부',\n",
    "        '기증 배아 사용 여부',\n",
    "        '대리모 여부',\n",
    "        ### DI\n",
    "    ]\n",
    "    train[cols_to_impute] = train[cols_to_impute].fillna(0)\n",
    "    test[cols_to_impute] = test[cols_to_impute].fillna(0)\n",
    "\n",
    "    return train, test\n",
    "\n",
    "def num_feature_scailing(train, test, seed=777):\n",
    "    numeric_cols = train.select_dtypes(include=[\"number\"]).columns.tolist()\n",
    "    cat_cols = [col for col in train.columns if pd.api.types.is_categorical_dtype(train[col])]\n",
    "    cols_to_scale = [\n",
    "        col for col in numeric_cols\n",
    "        if col not in cat_cols and col != '임신 성공 여부'\n",
    "    ]\n",
    "\n",
    "    arr_train = train[cols_to_scale].to_numpy()  # DataFrame -> NumPy\n",
    "    arr_train = arr_train.astype(np.float32)\n",
    "    arr_test = test[cols_to_scale].to_numpy()\n",
    "    arr_test = arr_test.astype(np.float32)\n",
    "\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "    noise = (\n",
    "        np.random.default_rng(0)\n",
    "        .normal(0.0, 1e-5, arr_train.shape)\n",
    "        .astype(arr_train.dtype)\n",
    "    )\n",
    "    preprocessing = QuantileTransformer(\n",
    "        n_quantiles=max(min(len(train[cols_to_scale]) // 30, 1000), 10),\n",
    "        output_distribution='normal',\n",
    "        subsample=10**9,\n",
    "    ).fit(arr_train + noise)\n",
    "\n",
    "    # train[cols_to_scale] = preprocessing.transform(arr_train + noise)\n",
    "    train[cols_to_scale] = preprocessing.transform(arr_train)\n",
    "    test[cols_to_scale] = preprocessing.transform(arr_test)\n",
    "    return train, test\n",
    "\n",
    "def drop_single_value_columns(df_train, df_test):\n",
    "    cols_to_drop = [col for col in df_train.columns if df_train[col].nunique() == 1]\n",
    "    return df_train.drop(columns=cols_to_drop), df_test.drop(columns=cols_to_drop)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\AppData\\Local\\Temp\\ipykernel_5824\\3576064510.py:69: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  df_train['배란 유도 유형'] = df_train['배란 유도 유형'].replace(mapping)\n",
      "C:\\Users\\User\\AppData\\Local\\Temp\\ipykernel_5824\\3576064510.py:70: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  df_test['배란 유도 유형'] = df_test['배란 유도 유형'].replace(mapping)\n",
      "C:\\Users\\User\\AppData\\Local\\Temp\\ipykernel_5824\\3576064510.py:199: DeprecationWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, pd.CategoricalDtype) instead\n",
      "  cat_cols = [col for col in train.columns if pd.api.types.is_categorical_dtype(train[col])]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "수치형 변수: 57개 \n",
      "['임신 시도 또는 마지막 임신 경과 연수', '배란 자극 여부', '단일 배아 이식 여부', '착상 전 유전 검사 사용 여부', '착상 전 유전 진단 사용 여부', '남성 주 불임 원인', '남성 부 불임 원인', '여성 주 불임 원인', '여성 부 불임 원인', '부부 주 불임 원인', '부부 부 불임 원인', '불명확 불임 원인', '불임 원인 - 난관 질환', '불임 원인 - 남성 요인', '불임 원인 - 배란 장애', '불임 원인 - 자궁경부 문제', '불임 원인 - 자궁내막증', '불임 원인 - 정자 농도', '불임 원인 - 정자 운동성', '불임 원인 - 정자 형태', '클리닉 내 총 시술 횟수', 'IVF 시술 횟수', 'DI 시술 횟수', '총 임신 횟수', 'IVF 임신 횟수', 'DI 임신 횟수', '총 출산 횟수', 'IVF 출산 횟수', 'DI 출산 횟수', '총 생성 배아 수', '미세주입된 난자 수', '미세주입에서 생성된 배아 수', '이식된 배아 수', '미세주입 배아 이식 수', '저장된 배아 수', '미세주입 후 저장된 배아 수', '해동된 배아 수', '해동 난자 수', '수집된 신선 난자 수', '저장된 신선 난자 수', '혼합된 난자 수', '파트너 정자와 혼합된 난자 수', '기증자 정자와 혼합된 난자 수', '동결 배아 사용 여부', '신선 배아 사용 여부', '기증 배아 사용 여부', '대리모 여부', 'PGD 시술 여부', 'PGS 시술 여부', '난자 혼합 경과일', '배아 이식 경과일', '배아 해동 경과일', '시술_임신', '배아생성이유_기증용', '배아생성이유_난자 저장용', '배아생성이유_배아 저장용', '배아생성이유_현재 시술용']\n",
      "범주형 변수: 8개 \n",
      "['시술 시기 코드', '시술 당시 나이', '배란 유도 유형', '난자 출처', '정자 출처', '난자 기증자 나이', '정자 기증자 나이', '시술유형_통합']\n",
      "(205080, 66) (51271, 65)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\AppData\\Local\\Temp\\ipykernel_5824\\2392652077.py:45: DeprecationWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, pd.CategoricalDtype) instead\n",
      "  cat_cols = [col for col in train.columns if pd.api.types.is_categorical_dtype(train[col])]\n"
     ]
    }
   ],
   "source": [
    "def all_process(train, val):\n",
    "    # 기본 전처리 단계\n",
    "    train, val = drop_columns(train), drop_columns(val)\n",
    "    train, val = 특정시술유형(train, val)\n",
    "    train, val = 시술횟수(train), 시술횟수(val)\n",
    "\n",
    "    train, val = 단일배아이식여부(train, val)\n",
    "    train, val = 배란유도유형(train, val)\n",
    "    train, val = 배아생성주요이유(train, val)\n",
    "\n",
    "    cols_to_encoding = [\n",
    "        \"시술 시기 코드\",\n",
    "        \"시술 당시 나이\",\n",
    "        \"배란 유도 유형\",\n",
    "        # \"클리닉 내 총 시술 횟수\",\n",
    "        # \"IVF 시술 횟수\",\n",
    "        # \"DI 시술 횟수\",\n",
    "        # \"총 임신 횟수\",\n",
    "        # \"IVF 임신 횟수\",\n",
    "        # \"DI 임신 횟수\",\n",
    "        # \"총 출산 횟수\",\n",
    "        # \"IVF 출산 횟수\",\n",
    "        # \"DI 출산 횟수\",\n",
    "        \"난자 출처\",\n",
    "        \"정자 출처\",\n",
    "        \"난자 기증자 나이\",\n",
    "        \"정자 기증자 나이\",\n",
    "        '시술유형_통합',\n",
    "    ]\n",
    "    train, val = label_encoding(train, val, cols=cols_to_encoding)\n",
    "    train, val = type_to_category(train, val, cols=cols_to_encoding)\n",
    "\n",
    "    train, val = impute_nan(train, val)\n",
    "    train, val = num_feature_scailing(train, val)\n",
    "\n",
    "    train, val = drop_single_value_columns(train, val)\n",
    "\n",
    "    return train, val\n",
    "\n",
    "train = pd.read_csv(train_path).drop(columns=[\"ID\"])\n",
    "test = pd.read_csv(test_path).drop(columns=[\"ID\"])\n",
    "\n",
    "train, test = all_process(train, test)\n",
    "\n",
    "cat_cols = [col for col in train.columns if pd.api.types.is_categorical_dtype(train[col])]\n",
    "numeric_cols = [col for col in train.columns if col not in cat_cols and col != '임신 성공 여부']\n",
    "\n",
    "print(f'수치형 변수: {len(numeric_cols)}개 \\n{numeric_cols}')\n",
    "print(f'범주형 변수: {len(cat_cols)}개 \\n{cat_cols}')\n",
    "print(train.shape, test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 실험 내용\n",
    "experiment_desc = '''\n",
    "WideDeep_BayesianWide 임신 성공 여부 \n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\AppData\\Local\\Temp\\ipykernel_5824\\3576064510.py:69: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  df_train['배란 유도 유형'] = df_train['배란 유도 유형'].replace(mapping)\n",
      "C:\\Users\\User\\AppData\\Local\\Temp\\ipykernel_5824\\3576064510.py:70: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  df_test['배란 유도 유형'] = df_test['배란 유도 유형'].replace(mapping)\n",
      "C:\\Users\\User\\AppData\\Local\\Temp\\ipykernel_5824\\3576064510.py:199: DeprecationWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, pd.CategoricalDtype) instead\n",
      "  cat_cols = [col for col in train.columns if pd.api.types.is_categorical_dtype(train[col])]\n",
      "C:\\Users\\User\\AppData\\Local\\Temp\\ipykernel_5824\\3576064510.py:69: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  df_train['배란 유도 유형'] = df_train['배란 유도 유형'].replace(mapping)\n",
      "C:\\Users\\User\\AppData\\Local\\Temp\\ipykernel_5824\\3576064510.py:70: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  df_test['배란 유도 유형'] = df_test['배란 유도 유형'].replace(mapping)\n",
      "C:\\Users\\User\\AppData\\Local\\Temp\\ipykernel_5824\\3576064510.py:199: DeprecationWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, pd.CategoricalDtype) instead\n",
      "  cat_cols = [col for col in train.columns if pd.api.types.is_categorical_dtype(train[col])]\n",
      "C:\\Users\\User\\AppData\\Local\\Temp\\ipykernel_5824\\4200938686.py:37: DeprecationWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, pd.CategoricalDtype) instead\n",
      "  categorical_cols = [col for col in fold_train.columns if pd.api.types.is_categorical_dtype(fold_train[col])]\n",
      "c:\\Users\\User\\anaconda3\\envs\\lg2025\\lib\\site-packages\\pytorch_widedeep\\preprocessing\\tab_preprocessor.py:364: UserWarning: Continuous columns will not be normalised\n",
      "  warnings.warn(\"Continuous columns will not be normalised\")\n",
      "epoch 1: 100%|██████████| 161/161 [00:25<00:00,  6.35it/s, loss=3.6e+3, metrics={'acc': 0.7346}] \n",
      "  0%|          | 0/41 [00:05<?, ?it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[11], line 147\u001b[0m\n\u001b[0;32m    133\u001b[0m trainer \u001b[38;5;241m=\u001b[39m BayesianTrainer(\n\u001b[0;32m    134\u001b[0m     model\u001b[38;5;241m=\u001b[39mmodel,\n\u001b[0;32m    135\u001b[0m     objective\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbinary\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    141\u001b[0m \n\u001b[0;32m    142\u001b[0m )\n\u001b[0;32m    146\u001b[0m \u001b[38;5;66;03m# 내부 validation split 없이 fold_train만 사용하여 학습\u001b[39;00m\n\u001b[1;32m--> 147\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    148\u001b[0m \u001b[43m\u001b[49m\u001b[43mX_tab\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mX_tab_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m       \u001b[49m\u001b[38;5;66;43;03m# 필수: 학습용 tabular 데이터 (ndarray)\u001b[39;49;00m\n\u001b[0;32m    149\u001b[0m \u001b[43m\u001b[49m\u001b[43mtarget\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m          \u001b[49m\u001b[38;5;66;43;03m# 필수: 학습 타겟 값 (ndarray)\u001b[39;49;00m\n\u001b[0;32m    150\u001b[0m \u001b[43m\u001b[49m\u001b[43mX_tab_val\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mX_tab_valid\u001b[49m\u001b[43m,\u001b[49m\u001b[43m   \u001b[49m\u001b[38;5;66;43;03m# 선택: 검증 데이터 (ndarray). 직접 전달하지 않을 경우, val_split 사용 가능\u001b[39;49;00m\n\u001b[0;32m    151\u001b[0m \u001b[43m\u001b[49m\u001b[43mtarget_val\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43my_valid\u001b[49m\u001b[43m,\u001b[49m\u001b[43m      \u001b[49m\u001b[38;5;66;43;03m# 선택: 검증 타겟 값 (ndarray)\u001b[39;49;00m\n\u001b[0;32m    152\u001b[0m \u001b[43m\u001b[49m\u001b[38;5;66;43;03m# val_split=0,         # X_tab_val과 target_val 대신 학습/검증 분할 비율 지정 가능 (예: 20% 검증 데이터)\u001b[39;49;00m\n\u001b[0;32m    153\u001b[0m \u001b[43m\u001b[49m\u001b[43mn_epochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m100\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m            \u001b[49m\u001b[38;5;66;43;03m# 에포크 수 (기본값: 1, 여기서는 100 에포크)\u001b[39;49;00m\n\u001b[0;32m    154\u001b[0m \u001b[43m\u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1024\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m          \u001b[49m\u001b[38;5;66;43;03m# 배치 크기 (기본값: 32, 여기서는 128)\u001b[39;49;00m\n\u001b[0;32m    155\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    159\u001b[0m \u001b[38;5;66;03m# 모델 학습 후 Validation 예측 코드:\u001b[39;00m\n\u001b[0;32m    160\u001b[0m \u001b[38;5;66;03m# 이미 확률이 계산되어 있는 컬럼을 사용합니다.\u001b[39;00m\n\u001b[0;32m    161\u001b[0m valid_probs \u001b[38;5;241m=\u001b[39m trainer\u001b[38;5;241m.\u001b[39mpredict_proba(X_tab\u001b[38;5;241m=\u001b[39mX_tab_valid)[:,\u001b[38;5;241m1\u001b[39m] \u001b[38;5;66;03m####\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\User\\anaconda3\\envs\\lg2025\\lib\\site-packages\\pytorch_widedeep\\training\\bayesian_trainer.py:232\u001b[0m, in \u001b[0;36mBayesianTrainer.fit\u001b[1;34m(self, X_tab, target, X_tab_val, target_val, val_split, n_epochs, validation_freq, batch_size, n_train_samples, n_val_samples)\u001b[0m\n\u001b[0;32m    230\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvalid_running_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.0\u001b[39m\n\u001b[0;32m    231\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m trange(eval_steps, disable\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mverbose \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m1\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m v:\n\u001b[1;32m--> 232\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i, (X, y) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(v, eval_loader):\n\u001b[0;32m    233\u001b[0m         v\u001b[38;5;241m.\u001b[39mset_description(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvalid\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    234\u001b[0m         val_score, val_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_eval_step(\n\u001b[0;32m    235\u001b[0m             X, y, n_val_samples, train_steps, i\n\u001b[0;32m    236\u001b[0m         )\n",
      "File \u001b[1;32mc:\\Users\\User\\anaconda3\\envs\\lg2025\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:708\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    705\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    706\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[0;32m    707\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[1;32m--> 708\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    709\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m    710\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[0;32m    711\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable\n\u001b[0;32m    712\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    713\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called\n\u001b[0;32m    714\u001b[0m ):\n",
      "File \u001b[1;32mc:\\Users\\User\\anaconda3\\envs\\lg2025\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:1458\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1455\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_process_data(data)\n\u001b[0;32m   1457\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_shutdown \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tasks_outstanding \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m-> 1458\u001b[0m idx, data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1459\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tasks_outstanding \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m   1460\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable:\n\u001b[0;32m   1461\u001b[0m     \u001b[38;5;66;03m# Check for _IterableDatasetStopIteration\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\User\\anaconda3\\envs\\lg2025\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:1420\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._get_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1416\u001b[0m     \u001b[38;5;66;03m# In this case, `self._data_queue` is a `queue.Queue`,. But we don't\u001b[39;00m\n\u001b[0;32m   1417\u001b[0m     \u001b[38;5;66;03m# need to call `.task_done()` because we don't use `.join()`.\u001b[39;00m\n\u001b[0;32m   1418\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1419\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m-> 1420\u001b[0m         success, data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_try_get_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1421\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m success:\n\u001b[0;32m   1422\u001b[0m             \u001b[38;5;28;01mreturn\u001b[39;00m data\n",
      "File \u001b[1;32mc:\\Users\\User\\anaconda3\\envs\\lg2025\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:1251\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._try_get_data\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m   1238\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_try_get_data\u001b[39m(\u001b[38;5;28mself\u001b[39m, timeout\u001b[38;5;241m=\u001b[39m_utils\u001b[38;5;241m.\u001b[39mMP_STATUS_CHECK_INTERVAL):\n\u001b[0;32m   1239\u001b[0m     \u001b[38;5;66;03m# Tries to fetch data from `self._data_queue` once for a given timeout.\u001b[39;00m\n\u001b[0;32m   1240\u001b[0m     \u001b[38;5;66;03m# This can also be used as inner loop of fetching without timeout, with\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1248\u001b[0m     \u001b[38;5;66;03m# Returns a 2-tuple:\u001b[39;00m\n\u001b[0;32m   1249\u001b[0m     \u001b[38;5;66;03m#   (bool: whether successfully get data, any: data if successful else None)\u001b[39;00m\n\u001b[0;32m   1250\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 1251\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_data_queue\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1252\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m (\u001b[38;5;28;01mTrue\u001b[39;00m, data)\n\u001b[0;32m   1253\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m   1254\u001b[0m         \u001b[38;5;66;03m# At timeout and error, we manually check whether any worker has\u001b[39;00m\n\u001b[0;32m   1255\u001b[0m         \u001b[38;5;66;03m# failed. Note that this is the only mechanism for Windows to detect\u001b[39;00m\n\u001b[0;32m   1256\u001b[0m         \u001b[38;5;66;03m# worker failures.\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\User\\anaconda3\\envs\\lg2025\\lib\\multiprocessing\\queues.py:113\u001b[0m, in \u001b[0;36mQueue.get\u001b[1;34m(self, block, timeout)\u001b[0m\n\u001b[0;32m    111\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m block:\n\u001b[0;32m    112\u001b[0m     timeout \u001b[38;5;241m=\u001b[39m deadline \u001b[38;5;241m-\u001b[39m time\u001b[38;5;241m.\u001b[39mmonotonic()\n\u001b[1;32m--> 113\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_poll\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[0;32m    114\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m Empty\n\u001b[0;32m    115\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_poll():\n",
      "File \u001b[1;32mc:\\Users\\User\\anaconda3\\envs\\lg2025\\lib\\multiprocessing\\connection.py:257\u001b[0m, in \u001b[0;36m_ConnectionBase.poll\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m    255\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_closed()\n\u001b[0;32m    256\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_readable()\n\u001b[1;32m--> 257\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_poll\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\User\\anaconda3\\envs\\lg2025\\lib\\multiprocessing\\connection.py:330\u001b[0m, in \u001b[0;36mPipeConnection._poll\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m    327\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_got_empty_message \u001b[38;5;129;01mor\u001b[39;00m\n\u001b[0;32m    328\u001b[0m             _winapi\u001b[38;5;241m.\u001b[39mPeekNamedPipe(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_handle)[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m):\n\u001b[0;32m    329\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m--> 330\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mbool\u001b[39m(\u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[1;32mc:\\Users\\User\\anaconda3\\envs\\lg2025\\lib\\multiprocessing\\connection.py:879\u001b[0m, in \u001b[0;36mwait\u001b[1;34m(object_list, timeout)\u001b[0m\n\u001b[0;32m    876\u001b[0m                 ready_objects\u001b[38;5;241m.\u001b[39madd(o)\n\u001b[0;32m    877\u001b[0m                 timeout \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m--> 879\u001b[0m     ready_handles \u001b[38;5;241m=\u001b[39m \u001b[43m_exhaustive_wait\u001b[49m\u001b[43m(\u001b[49m\u001b[43mwaithandle_to_obj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkeys\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    880\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    881\u001b[0m     \u001b[38;5;66;03m# request that overlapped reads stop\u001b[39;00m\n\u001b[0;32m    882\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m ov \u001b[38;5;129;01min\u001b[39;00m ov_list:\n",
      "File \u001b[1;32mc:\\Users\\User\\anaconda3\\envs\\lg2025\\lib\\multiprocessing\\connection.py:811\u001b[0m, in \u001b[0;36m_exhaustive_wait\u001b[1;34m(handles, timeout)\u001b[0m\n\u001b[0;32m    809\u001b[0m ready \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m    810\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m L:\n\u001b[1;32m--> 811\u001b[0m     res \u001b[38;5;241m=\u001b[39m \u001b[43m_winapi\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mWaitForMultipleObjects\u001b[49m\u001b[43m(\u001b[49m\u001b[43mL\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    812\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m res \u001b[38;5;241m==\u001b[39m WAIT_TIMEOUT:\n\u001b[0;32m    813\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import time\n",
    "start_time = time.time()\n",
    "\n",
    "\n",
    "# import scipy.special   # 시그모이드 대신 softmax에 사용할 수 있음\n",
    "# 교차 검증 설정: seed_list를 [333] 하나만 사용, n_splits=3\n",
    "seed_list = [333]\n",
    "n_splits = 5\n",
    "\n",
    "total_auc, total_acc, total_f1 = [], [], []\n",
    "test_preds = []\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# 교차 검증 시작\n",
    "for seed in seed_list:\n",
    "    # train, test 불러오기\n",
    "    train = pd.read_csv(train_path).drop(columns=[\"ID\"])\n",
    "    test = pd.read_csv(test_path).drop(columns=[\"ID\"])\n",
    "\n",
    "    skf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=seed)\n",
    "    auc_scores, acc_scores, f1_scores = [], [], []\n",
    "    \n",
    "    for fold, (train_idx, valid_idx) in enumerate(skf.split(train.drop(columns=['임신 성공 여부']), train[\"임신 성공 여부\"])):\n",
    "        # Fold 데이터 생성\n",
    "        fold_train, fold_valid = train.iloc[train_idx], train.iloc[valid_idx]\n",
    "        fold_train2 = fold_train.copy()\n",
    "        fold_test = test.copy()  # test 데이터는 별도 사용\n",
    "        \n",
    "        # 전처리 \n",
    "        fold_train, fold_valid = all_process(fold_train, fold_valid)\n",
    "        _, fold_test = all_process(fold_train2, fold_test)\n",
    "\n",
    "        # 범주형, 연속형 열 구분분\n",
    "        categorical_cols = [col for col in fold_train.columns if pd.api.types.is_categorical_dtype(fold_train[col])]\n",
    "        continuous_cols = [col for col in fold_train.columns if col not in cat_cols and col != '임신 성공 여부']\n",
    "\n",
    "\n",
    "\n",
    "        # Deep 부분: 임베딩 + 연속형 변수 처리\n",
    "        tab_preprocessor = TabPreprocessor(embed_cols=categorical_cols, continuous_cols=continuous_cols)\n",
    "\n",
    "        # 전처리: 각 Fold 별로 Wide & Deep 데이터 생성\n",
    "        # X_wide_train = wide_preprocessor.fit_transform(fold_train)\n",
    "        # X_wide_valid = wide_preprocessor.transform(fold_valid)\n",
    "        # X_wide_test = wide_preprocessor.transform(fold_test)\n",
    "        X_tab_train = tab_preprocessor.fit_transform(fold_train)\n",
    "        X_tab_valid = tab_preprocessor.transform(fold_valid)\n",
    "        X_tab_test = tab_preprocessor.transform(fold_test)\n",
    "\n",
    "        # Target 값: 정수형 (0,1)\n",
    "        y_train = fold_train['임신 성공 여부'].astype(int).values\n",
    "        y_valid = fold_valid['임신 성공 여부'].astype(int).values\n",
    "\n",
    "        \n",
    "        # BayesianTabMlp 모델 구성\n",
    "        model = BayesianTabMlp(\n",
    "            column_idx=tab_preprocessor.column_idx,  \n",
    "            # column_idx (Dict[str, int]): TabMlp 모델에 입력될 피처들의 인덱스를 담은 딕셔너리.\n",
    "            # 예시: {'education': 0, 'relationship': 1, 'workclass': 2, ...}\n",
    "            \n",
    "            cat_embed_input=tab_preprocessor.cat_embed_input,  \n",
    "            # cat_embed_input (Optional[List[Tuple[str, int, int]]]): \n",
    "            # 범주형 피처에 대해 (컬럼명, 고유값 개수, 임베딩 차원) 튜플로 구성된 리스트.\n",
    "            # 기본값: None (제공하지 않을 경우 임베딩이 적용되지 않음)\n",
    "            \n",
    "            continuous_cols=continuous_cols,  \n",
    "            # continuous_cols (Optional[List[str]]): 연속형(수치형) 피처의 이름 리스트.\n",
    "            # 기본값: None (연속형 피처가 없을 경우)\n",
    "            \n",
    "            mlp_hidden_dims=[200, 100],\n",
    "            # mlp_hidden_dims (List[int]): MLP의 각 Dense layer에 사용될 뉴런 수를 정의하는 리스트.\n",
    "            # 기본값: [200, 100]이지만, 여기서는 [128, 64]를 사용.\n",
    "            \n",
    "            prior_sigma_1=1.0,\n",
    "            # prior_sigma_1 (float): 첫 번째 Gaussian 분포의 sigma (표준편차)에 대한 prior 값.\n",
    "            # 기본값: 1.0\n",
    "            # 후보 옵션: 데이터 특성에 따라 1.0, 0.5 등으로 조정 가능.\n",
    "            \n",
    "            prior_sigma_2=0.002,\n",
    "            # prior_sigma_2 (float): 두 번째 Gaussian 분포의 sigma에 대한 prior 값.\n",
    "            # 기본값: 0.002\n",
    "            # 후보 옵션: 0.002, 0.001 등으로 변경 가능.\n",
    "            \n",
    "            prior_pi=0.8,\n",
    "            # prior_pi (float): 두 Gaussian 분포를 혼합할 때 사용하는 scaling factor.\n",
    "            # 기본값: 0.8\n",
    "            # 후보 옵션: 0.8, 0.5 등 문제에 따라 조정 가능.\n",
    "            \n",
    "            posterior_mu_init=0,\n",
    "            # posterior_mu_init (float): 가중치의 posterior 분포의 평균 초기값.\n",
    "            # 기본값: 0.0\n",
    "            \n",
    "            posterior_rho_init=-7.0,\n",
    "            # posterior_rho_init (float): 가중치의 posterior 분포의 rho 초기값.\n",
    "            # 기본값: -7.0\n",
    "            \n",
    "            pred_dim=1  \n",
    "            # pred_dim (int): 출력 차원의 크기.\n",
    "            # 이진 분류 문제에서는 1로 설정합니다.\n",
    "        )\n",
    "\n",
    "\n",
    "        \n",
    "        \n",
    "        # 옵티마이저 및 학습률 스케줄러 설정\n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr=0.0001)  # Set your desired learning rate here\n",
    "        \n",
    "        \n",
    "        # EarlyStopping 콜백 설정 (patience=5, min_delta=0.001)\n",
    "        early_stopping = EarlyStopping(\n",
    "            monitor=\"val_loss\", \n",
    "            patience=5, \n",
    "            min_delta=0.001, \n",
    "            verbose=1\n",
    "        )\n",
    "        \n",
    "        # 현재 날짜/시간을 포함한 파일 이름 생성\n",
    "        timestamp = datetime.datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "        model_path = f\"saved_models/best_model_{timestamp}.pt\"\n",
    "        \n",
    "        # ✅ 2. ModelCheckpoint: 최상의 모델을 자동 저장\n",
    "        model_checkpoint = ModelCheckpoint(\n",
    "            filepath=model_path,  # 모델 저장 경로\n",
    "            monitor=\"val_loss\",        # 감시할 지표 ('val_loss' 또는 'val_acc')\n",
    "            save_best_only=True        # 가장 좋은 성능의 모델만 저장\n",
    "        )\n",
    "        \n",
    "\n",
    "        # BayesianTrainer 생성 (metrics는 Accuracy 클래스를 그대로 전달)\n",
    "        trainer = BayesianTrainer(\n",
    "            model=model,\n",
    "            objective=\"binary\",\n",
    "            optimizer=optimizer,\n",
    "            callbacks=[early_stopping, model_checkpoint],  # 조기 종료 (3 에포크 동안 개선 없으면 종료)\n",
    "            metrics=[Accuracy],\n",
    "            verbose=1,                  # 학습 로그 출력 (기본값: 1)\n",
    "            seed=333                    # 랜덤 시드 설정 (기본값: 1)\n",
    "\n",
    "        )\n",
    "\n",
    "        \n",
    "\n",
    "        # 내부 validation split 없이 fold_train만 사용하여 학습\n",
    "        trainer.fit(\n",
    "        X_tab=X_tab_train,       # 필수: 학습용 tabular 데이터 (ndarray)\n",
    "        target=y_train,          # 필수: 학습 타겟 값 (ndarray)\n",
    "        X_tab_val=X_tab_valid,   # 선택: 검증 데이터 (ndarray). 직접 전달하지 않을 경우, val_split 사용 가능\n",
    "        target_val=y_valid,      # 선택: 검증 타겟 값 (ndarray)\n",
    "        # val_split=0,         # X_tab_val과 target_val 대신 학습/검증 분할 비율 지정 가능 (예: 20% 검증 데이터)\n",
    "        n_epochs=100,            # 에포크 수 (기본값: 1, 여기서는 100 에포크)\n",
    "        batch_size=1024,          # 배치 크기 (기본값: 32, 여기서는 128)\n",
    "        )\n",
    "        \n",
    "        \n",
    "        \n",
    "        # 모델 학습 후 Validation 예측 코드:\n",
    "        # 이미 확률이 계산되어 있는 컬럼을 사용합니다.\n",
    "        valid_probs = trainer.predict_proba(X_tab=X_tab_valid)[:,1] ####\n",
    "        valid_pred = (valid_probs > 0.5).astype(int)\n",
    "\n",
    "        \n",
    "\n",
    "        # 실제 정답 \n",
    "        y_valid = fold_valid['임신 성공 여부'].values.astype(int)\n",
    "\n",
    "\n",
    "        \n",
    "        # 평가 지표 계산: 클래스 1의 확률 사용\n",
    "        fold_auc = roc_auc_score(y_valid, valid_probs)\n",
    "        print(f\"Seed[{seed:<3}] Fold {fold + 1} | AUC: {fold_auc:.6f}\")\n",
    "        \n",
    "        auc_scores.append(fold_auc)\n",
    "        \n",
    "        total_auc.append(fold_auc)\n",
    "        # Test 데이터 예측 (각 fold의 모델로 예측한 결과 저장)\n",
    "        test_pred = trainer.predict_proba(X_tab=X_tab_test)[:,1]\n",
    "        test_preds.append(test_pred)\n",
    "    \n",
    "    # Fold 별 평균 성능 출력\n",
    "    avg_auc = np.mean(auc_scores)\n",
    "    \n",
    "    print(\"-\" * 80)\n",
    "    print(f\"Seed[{seed:<3}] Average Metrics | AUC: {avg_auc:.7f}\")\n",
    "    print(\"-\" * 80)\n",
    "\n",
    "# 전체 Validation 평균 성능 출력\n",
    "val_auc = np.mean(total_auc)\n",
    "print(\"-\" * 80)\n",
    "print(f\"Validation Average Metrics | AUC: {val_auc:.7f}\")\n",
    "\n",
    "finish_time = time.time()\n",
    "total_time = finish_time - start_time \n",
    "\n",
    "print(total_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# stratify_label = train[\"임신 성공 여부\"].astype(str)\n",
    "old_auc = 0.7289820 * 100\n",
    "\n",
    "new_auc = val_auc * 100\n",
    "\n",
    "\n",
    "def calculate_change(old_value, new_value):\n",
    "    change = new_value - old_value\n",
    "    percentage_change = (change / old_value) * 100 if old_value != 0 else float('inf')\n",
    "    return change, percentage_change\n",
    "\n",
    "def format_change(change):\n",
    "    return f\"{change:+.6f}\"\n",
    "\n",
    "# 각 지표의 변화량 계산\n",
    "auc_change, auc_pct = calculate_change(old_auc, new_auc)\n",
    "\n",
    "\n",
    "# 결과 출력\n",
    "print(\"\\n====== 모델 성능 변화 ======\")\n",
    "print(f\"{'Metric':<8}  {'AUC':>12}\")\n",
    "print(\"-\" * 30)\n",
    "print(f\"{'Old':<8}  {old_auc:>12.6f}\")\n",
    "print(f\"{'New':<8}  {new_auc:>12.6f}\")\n",
    "print(f\"{'Change':<8}  {format_change(auc_change):>12}\")\n",
    "print(f\"{'% Change':<8}  {auc_pct:>11.4f}%\")\n",
    "print(\"=\" * 30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp_submission = pd.DataFrame({f'tabm_{data_seed}': np.mean(test_preds, axis=0)})\n",
    "tmp_submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from LG_Aimers_6th.cal_auc import calculate_auc\n",
    "score = calculate_auc(tmp_submission, seed=data_seed)\n",
    "print(f'[seed {data_seed}]: {score}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 기본 test 데이터 AUC : 0.732737"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_submission = pd.read_csv(sample_path)\n",
    "# test_preds\n",
    "# sample_submission['임신 성공 확률'] = np.mean(test_preds, axis=0)\n",
    "\n",
    "# ratio = train['임신 성공 여부'].value_counts(normalize=True)[1]\n",
    "# real_true_count = int(ratio * len(sample_submission))\n",
    "# print(f'test의 True 갯수: {real_true_count:<5} (추정)')\n",
    "\n",
    "# count = (sample_submission['임신 성공 확률'] >= 0.5).sum()\n",
    "# print(f'test의 True 갯수: {count:<5} (예측 결과)')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "now = datetime.datetime.now().strftime(\"%m%d_%H%M\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission_path = 'Submission'\n",
    "if not os.path.exists(submission_path):\n",
    "    os.makedirs(submission_path)\n",
    "\n",
    "code_dir = 'Code'\n",
    "if not os.path.exists(code_dir):\n",
    "    os.makedirs(code_dir)\n",
    "\n",
    "\n",
    "submission_name = f\"submission_{now}.csv\"\n",
    "new_notebook_name = f\"code_{now}.ipynb\"\n",
    "\n",
    "sample_submission.to_csv(os.path.join(submission_path, submission_name), index=False)\n",
    "\n",
    "\n",
    "# 현재 노트북 파일 경로 직접 지정 (실제 노트북 파일명으로 수정)\n",
    "current_notebook = os.path.join(os.getcwd(), \"WideDeep_TabMlp_임신 여부.ipynb\")\n",
    "\n",
    "new_notebook_path = os.path.join(code_dir, new_notebook_name)\n",
    "\n",
    "# 노트북 파일 복사\n",
    "shutil.copy(current_notebook, new_notebook_path)\n",
    "\n",
    "print(f\"Notebook saved in '{code_dir}' as '{new_notebook_name}'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 📌 SQLite 데이터베이스 설정\n",
    "db_path = \"experiment_results.db\"\n",
    "conn = sqlite3.connect(db_path)\n",
    "cursor = conn.cursor()\n",
    "\n",
    "# 📌 테이블 생성 (처음 실행 시)\n",
    "cursor.execute('''\n",
    "CREATE TABLE IF NOT EXISTS experiments (\n",
    "    id INTEGER PRIMARY KEY AUTOINCREMENT,\n",
    "    code_name TEXT,\n",
    "    experiment_desc TEXT,\n",
    "    auc REAL,\n",
    "    acc REAL,\n",
    "    f1 REAL\n",
    ")\n",
    "''')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터 삽입\n",
    "cursor.execute('''\n",
    "INSERT INTO experiments (code_name, experiment_desc, auc, acc, f1)\n",
    "VALUES (?, ?, ?, ?, ?)\n",
    "''', (new_notebook_name, experiment_desc.strip(), new_auc, new_acc, new_f1))\n",
    "\n",
    "# 변경사항 저장 & 연결 종료\n",
    "conn.commit()\n",
    "conn.close()\n",
    "\n",
    "print(f\"Experiment '{new_notebook_name}' successfully saved in database\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sqlite3\n",
    "import pandas as pd\n",
    "\n",
    "# SQLite 데이터 조회 함수\n",
    "def get_experiment_results(db_path=\"experiment_results.db\", num_results=10):\n",
    "    \"\"\"\n",
    "    SQLite 데이터베이스에서 중복된 실험 데이터를 제거하고, 최근 num_results개의 실험 데이터를 불러오는 함수.\n",
    "    Returns:\n",
    "        - Pandas DataFrame: 중복 제거된 실험 데이터\n",
    "    \"\"\"\n",
    "    conn = sqlite3.connect(db_path)\n",
    "\n",
    "    # 중복 제거 & 최신 데이터 선택하는 SQL 쿼리\n",
    "    query = f\"\"\"\n",
    "    SELECT * FROM experiments\n",
    "    WHERE id IN (\n",
    "        SELECT MAX(id)  -- 가장 최신 데이터 선택\n",
    "        FROM experiments\n",
    "        GROUP BY code_name -- id 제외하고 중복 판단\n",
    "    )\n",
    "    ORDER BY id DESC  -- 최신 데이터부터 정렬\n",
    "    LIMIT {num_results};\n",
    "    \"\"\"\n",
    "\n",
    "    df = pd.read_sql_query(query, conn)\n",
    "    conn.close()\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_results = get_experiment_results(num_results=100)\n",
    "df_results.to_csv('experiment_results.csv', index=False, encoding='utf-8-sig', float_format='%.4f')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lg2025",
   "language": "python",
   "name": "lg2025"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
