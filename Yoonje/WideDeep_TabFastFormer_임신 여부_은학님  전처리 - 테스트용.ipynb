{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# WideDeep_TabFastFormer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 라이브러리 설치"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install numpy==1.26.0\n",
    "# !pip install pandas==2.2.2\n",
    "# !pip install scikit-learn==1.5.1\n",
    "# !pip install scipy==1.14.1\n",
    "# !pip install statsmodels==0.14.2\n",
    "# !pip install joblib==1.4.2\n",
    "# !pip install threadpoolctl==3.5.0\n",
    "# !pip install ipynbname"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 1. 기존의 패키지 정리\n",
    "# !pip uninstall -y torch torchvision torchaudio pytorch-lightning pytorch-tabular\n",
    "\n",
    "# # 2. 호환 가능한 버전으로 재설치\n",
    "# !pip install torch==2.0.1+cu118 --index-url https://download.pytorch.org/whl/cu118\n",
    "# !pip install pytorch-tabular==1.1.1 --no-deps\n",
    "# !pip install pytorch-lightning==2.0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip3 install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install pytorch-widedeep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install opencv-python-headless"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sqlite3\n",
    "import shutil\n",
    "import ipynbname\n",
    "import datetime\n",
    "import os\n",
    "import warnings\n",
    "import random\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "\n",
    "\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.preprocessing import LabelEncoder, FunctionTransformer, QuantileTransformer, MultiLabelBinarizer\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "\n",
    "\n",
    "import torch\n",
    "\n",
    "# pytorch-widedeep 라이브러리 import\n",
    "from pytorch_widedeep import Trainer\n",
    "from pytorch_widedeep.preprocessing import WidePreprocessor, TabPreprocessor\n",
    "from pytorch_widedeep.models import Wide, TabFastFormer, WideDeep\n",
    "from pytorch_widedeep.metrics import Accuracy,F1Score\n",
    "from pytorch_widedeep.callbacks import EarlyStopping, ModelCheckpoint\n",
    "\n",
    "\n",
    "##################\n",
    "data_seed=1\n",
    "\n",
    "train_path = f'./data/custom_train_{data_seed}.csv'\n",
    "test_path = f'./data/custom_test_{data_seed}.csv'\n",
    "\n",
    "\n",
    "train_path = f'C:/Users/User/Desktop/LG Aimers/LG_Aimers_6th/data/custom_train_{data_seed}.csv'\n",
    "test_path = f'C:/Users/User/Desktop/LG Aimers/LG_Aimers_6th/data/custom_test_{data_seed}.csv'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(205080, 68) (51271, 67)\n"
     ]
    }
   ],
   "source": [
    "train = pd.read_csv(train_path).drop(columns=[\"ID\"])\n",
    "test = pd.read_csv(test_path).drop(columns=[\"ID\"])\n",
    "print(train.shape, test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def drop_columns(df):\n",
    "    cols = [\n",
    "        '불임 원인 - 여성 요인',  # 고유값 1\n",
    "        '불임 원인 - 정자 면역학적 요인',  # train, test 모두 '1'인 데이터 1개 >> 신뢰할 수 없음\n",
    "        '난자 해동 경과일',\n",
    "    ]\n",
    "    df = df.drop(cols, axis=1)\n",
    "    return df\n",
    "\n",
    "def 특정시술유형(train, test):\n",
    "    def categorize_procedure(proc):\n",
    "        tokens = [token.strip() for token in proc.split(\",\") if token.strip() and not token.strip().isdigit()]\n",
    "        # 우선순위에 따른 범주화\n",
    "        if tokens.count(\"Unknown\") >= 1:\n",
    "            return \"Unknown\"\n",
    "        if tokens.count(\"AH\") >= 1:\n",
    "            return \"AH\"\n",
    "        if tokens.count(\"BLASTOCYST\") >= 1:\n",
    "            return \"BLASTOCYST\"\n",
    "        if tokens.count(\"ICSI\") >= 2 or tokens.count(\"IVF\") >= 2:\n",
    "            return \"2ICSI_2IVF\"\n",
    "        if tokens.count(\"IVF\") >= 1 and tokens.count(\"ICSI\") >= 1:\n",
    "            return \"IVF_ICSI\"\n",
    "        if tokens == \"ICSI\":\n",
    "            return \"ICSI\"\n",
    "        if tokens == \"IVF\":\n",
    "            return \"IVF\"\n",
    "        return \",\".join(tokens) if tokens else None\n",
    "\n",
    "    for df in [train, test]:\n",
    "        df['특정 시술 유형'] = df['특정 시술 유형'].str.replace(\" / \", \",\")\n",
    "        df['특정 시술 유형'] = df['특정 시술 유형'].str.replace(\":\", \",\")\n",
    "        df['특정 시술 유형'] = df['특정 시술 유형'].str.replace(\" \", \"\")\n",
    "\n",
    "    counts = train['특정 시술 유형'].value_counts()\n",
    "    allowed_categories = counts[counts >= 100].index.tolist()\n",
    "\n",
    "    # allowed_categories에 속하지 않는 값은 \"Unknown\"으로 대체\n",
    "    train.loc[~train['특정 시술 유형'].isin(allowed_categories), '특정 시술 유형'] = \"Unknown\"\n",
    "    test.loc[~test['특정 시술 유형'].isin(allowed_categories), '특정 시술 유형'] = \"Unknown\"\n",
    "\n",
    "    train['특정 시술 유형'] = train['특정 시술 유형'].apply(categorize_procedure)\n",
    "    test['특정 시술 유형'] = test['특정 시술 유형'].apply(categorize_procedure)\n",
    "\n",
    "    train['시술유형_통합'] = train['시술 유형'].astype(str) + '_' + train['특정 시술 유형'].astype(str)\n",
    "    test['시술유형_통합'] = test['시술 유형'].astype(str) + '_' + test['특정 시술 유형'].astype(str)\n",
    "\n",
    "    drop_cols = ['시술 유형', '특정 시술 유형']\n",
    "    train = train.drop(drop_cols, axis=1)\n",
    "    test = test.drop(drop_cols, axis=1)\n",
    "\n",
    "    return train, test\n",
    "\n",
    "def 시술횟수(df_train):\n",
    "    for col in [col for col in df_train.columns if '횟수' in col]:\n",
    "        df_train[col] = df_train[col].replace({'6회 이상':'6회'})\n",
    "        df_train[col] = df_train[col].str[0].astype(int)\n",
    "    df_train['시술_임신'] = df_train['총 임신 횟수'] - df_train['총 시술 횟수']\n",
    "    df_train = df_train.drop('총 시술 횟수', axis=1)\n",
    "    return df_train\n",
    "\n",
    "def 배란유도유형(df_train, df_test):\n",
    "    mapping = {\n",
    "        '기록되지 않은 시행': 1,\n",
    "        '알 수 없음': 0,\n",
    "        '세트로타이드 (억제제)': 0,\n",
    "        '생식선 자극 호르몬': 0,\n",
    "    }\n",
    "    df_train['배란 유도 유형'] = df_train['배란 유도 유형'].replace(mapping)\n",
    "    df_test['배란 유도 유형'] = df_test['배란 유도 유형'].replace(mapping)\n",
    "    return df_train, df_test\n",
    "\n",
    "def 난자기증자나이(df_train, df_test):\n",
    "    mapping = {\n",
    "        '만20세 이하': 20,\n",
    "        '만21-25세': 25,\n",
    "        '만26-30세': 30,\n",
    "        '만31-35세': 35,\n",
    "        '알 수 없음': 20,  # 만20세 이하와 동일하게 처리\n",
    "    }\n",
    "    df_train['난자 기증자 나이'] = df_train['난자 기증자 나이'].replace(mapping)\n",
    "    df_test['난자 기증자 나이'] = df_test['난자 기증자 나이'].replace(mapping)\n",
    "    return df_train, df_test\n",
    "\n",
    "def 배아생성주요이유(df_train, df_test):\n",
    "    df_train['배아 생성 주요 이유'] = df_train['배아 생성 주요 이유'].fillna('DI')\n",
    "    df_test['배아 생성 주요 이유'] = df_test['배아 생성 주요 이유'].fillna('DI')\n",
    "\n",
    "    df_train['배아 생성 이유 리스트'] = df_train['배아 생성 주요 이유'].apply(lambda x: [reason.strip() for reason in x.split(',')])\n",
    "    df_test['배아 생성 이유 리스트'] = df_test['배아 생성 주요 이유'].apply(lambda x: [reason.strip() for reason in x.split(',')])\n",
    "\n",
    "    mlb = MultiLabelBinarizer()\n",
    "    train_one_hot = pd.DataFrame(\n",
    "        mlb.fit_transform(df_train['배아 생성 이유 리스트']),\n",
    "        columns=mlb.classes_,\n",
    "        index=df_train.index\n",
    "    )\n",
    "    train_one_hot.columns = ['배아생성이유_' + col for col in train_one_hot.columns]\n",
    "\n",
    "    test_one_hot = pd.DataFrame(\n",
    "        mlb.transform(df_test['배아 생성 이유 리스트']),\n",
    "        columns=mlb.classes_,\n",
    "        index=df_test.index\n",
    "    )\n",
    "    test_one_hot.columns = ['배아생성이유_' + col for col in test_one_hot.columns]\n",
    "\n",
    "    df_train = pd.concat([df_train, train_one_hot], axis=1)\n",
    "    df_test = pd.concat([df_test, test_one_hot], axis=1)\n",
    "\n",
    "    cols_to_drop = [\n",
    "        '배아 생성 주요 이유',\n",
    "        '배아 생성 이유 리스트',\n",
    "        '배아생성이유_연구용',\n",
    "        '배아생성이유_DI'\n",
    "    ]\n",
    "    df_train = df_train.drop(cols_to_drop, axis=1, errors='ignore')\n",
    "    df_test = df_test.drop(cols_to_drop, axis=1, errors='ignore')\n",
    "\n",
    "    cols = ['배아생성이유_기증용',\n",
    "            '배아생성이유_난자 저장용',\n",
    "            '배아생성이유_배아 저장용',\n",
    "            '배아생성이유_현재 시술용']\n",
    "\n",
    "    df_train[cols] = df_train[cols].div(df_train[cols].sum(axis=1).replace(0, np.nan), axis=0).fillna(0)\n",
    "    df_test[cols] = df_test[cols].div(df_test[cols].sum(axis=1).replace(0, np.nan), axis=0).fillna(0)\n",
    "\n",
    "    return df_train, df_test\n",
    "\n",
    "def 단일배아이식여부(df_train, df_val):\n",
    "    df_train['단일 배아 이식 여부'] = df_train['단일 배아 이식 여부'].fillna(0)\n",
    "    df_val['단일 배아 이식 여부'] = df_val['단일 배아 이식 여부'].fillna(0)\n",
    "    return df_train, df_val\n",
    "\n",
    "\n",
    "def 기증자정자와혼합된난자수(df_train, df_test):\n",
    "    df_train[\"기증자 정자와 혼합된 난자 수\"] = df_train[\"기증자 정자와 혼합된 난자 수\"].fillna(2)\n",
    "    df_test[\"기증자 정자와 혼합된 난자 수\"] = df_test[\"기증자 정자와 혼합된 난자 수\"].fillna(2)\n",
    "    return df_train, df_test\n",
    "\n",
    "def label_encoding(train, test, cols):\n",
    "    encoder = LabelEncoder()\n",
    "    for col in cols:\n",
    "        train[col] = encoder.fit_transform(train[col])\n",
    "        test[col] = encoder.transform(test[col])\n",
    "    return train, test\n",
    "\n",
    "def type_to_category(train, test, cols):\n",
    "    train[cols] = train[cols].astype('category')\n",
    "    test[cols] = test[cols].astype('category')\n",
    "    return train, test\n",
    "\n",
    "def impute_nan(train, test):\n",
    "    cols_to_impute = [\n",
    "        '임신 시도 또는 마지막 임신 경과 연수', # DI, IVF랑 관련 X\n",
    "    ]\n",
    "    imputer = SimpleImputer(strategy='mean')\n",
    "    train[cols_to_impute] = imputer.fit_transform(train[cols_to_impute])\n",
    "    test[cols_to_impute] = imputer.transform(test[cols_to_impute])\n",
    "\n",
    "    cols_to_impute = [\n",
    "        '난자 채취 경과일',\n",
    "        '난자 혼합 경과일',\n",
    "        '배아 이식 경과일',\n",
    "        '배아 해동 경과일',\n",
    "\n",
    "        '착상 전 유전 검사 사용 여부',\n",
    "        'PGD 시술 여부',\n",
    "        'PGS 시술 여부',\n",
    "\n",
    "        ### DI only\n",
    "        '착상 전 유전 진단 사용 여부',\n",
    "        '총 생성 배아 수',\n",
    "        '미세주입된 난자 수',\n",
    "        '미세주입에서 생성된 배아 수',\n",
    "        '이식된 배아 수',\n",
    "        '미세주입 배아 이식 수',\n",
    "        '저장된 배아 수',\n",
    "        '미세주입 후 저장된 배아 수',\n",
    "        '해동된 배아 수',\n",
    "        '해동 난자 수',\n",
    "        '수집된 신선 난자 수',\n",
    "        '저장된 신선 난자 수',\n",
    "        '혼합된 난자 수',\n",
    "        '파트너 정자와 혼합된 난자 수',\n",
    "        '기증자 정자와 혼합된 난자 수',\n",
    "        '동결 배아 사용 여부',\n",
    "        '신선 배아 사용 여부',\n",
    "        '기증 배아 사용 여부',\n",
    "        '대리모 여부',\n",
    "        ### DI\n",
    "    ]\n",
    "    train[cols_to_impute] = train[cols_to_impute].fillna(0)\n",
    "    test[cols_to_impute] = test[cols_to_impute].fillna(0)\n",
    "\n",
    "    return train, test\n",
    "\n",
    "def num_feature_scailing(train, test, seed=777):\n",
    "    numeric_cols = train.select_dtypes(include=[\"number\"]).columns.tolist()\n",
    "    cat_cols = [col for col in train.columns if pd.api.types.is_categorical_dtype(train[col])]\n",
    "    cols_to_scale = [\n",
    "        col for col in numeric_cols\n",
    "        if col not in cat_cols and col != '임신 성공 여부'\n",
    "    ]\n",
    "\n",
    "    arr_train = train[cols_to_scale].to_numpy()  # DataFrame -> NumPy\n",
    "    arr_train = arr_train.astype(np.float32)\n",
    "    arr_test = test[cols_to_scale].to_numpy()\n",
    "    arr_test = arr_test.astype(np.float32)\n",
    "\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "    noise = (\n",
    "        np.random.default_rng(0)\n",
    "        .normal(0.0, 1e-5, arr_train.shape)\n",
    "        .astype(arr_train.dtype)\n",
    "    )\n",
    "    preprocessing = QuantileTransformer(\n",
    "        n_quantiles=max(min(len(train[cols_to_scale]) // 30, 1000), 10),\n",
    "        output_distribution='normal',\n",
    "        subsample=10**9,\n",
    "    ).fit(arr_train + noise)\n",
    "\n",
    "    # train[cols_to_scale] = preprocessing.transform(arr_train + noise)\n",
    "    train[cols_to_scale] = preprocessing.transform(arr_train)\n",
    "    test[cols_to_scale] = preprocessing.transform(arr_test)\n",
    "    return train, test\n",
    "\n",
    "def drop_single_value_columns(df_train, df_test):\n",
    "    cols_to_drop = [col for col in df_train.columns if df_train[col].nunique() == 1]\n",
    "    return df_train.drop(columns=cols_to_drop), df_test.drop(columns=cols_to_drop)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\AppData\\Local\\Temp\\ipykernel_9744\\3576064510.py:69: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  df_train['배란 유도 유형'] = df_train['배란 유도 유형'].replace(mapping)\n",
      "C:\\Users\\User\\AppData\\Local\\Temp\\ipykernel_9744\\3576064510.py:70: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  df_test['배란 유도 유형'] = df_test['배란 유도 유형'].replace(mapping)\n",
      "C:\\Users\\User\\AppData\\Local\\Temp\\ipykernel_9744\\3576064510.py:199: DeprecationWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, pd.CategoricalDtype) instead\n",
      "  cat_cols = [col for col in train.columns if pd.api.types.is_categorical_dtype(train[col])]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "수치형 변수: 57개 \n",
      "['임신 시도 또는 마지막 임신 경과 연수', '배란 자극 여부', '단일 배아 이식 여부', '착상 전 유전 검사 사용 여부', '착상 전 유전 진단 사용 여부', '남성 주 불임 원인', '남성 부 불임 원인', '여성 주 불임 원인', '여성 부 불임 원인', '부부 주 불임 원인', '부부 부 불임 원인', '불명확 불임 원인', '불임 원인 - 난관 질환', '불임 원인 - 남성 요인', '불임 원인 - 배란 장애', '불임 원인 - 자궁경부 문제', '불임 원인 - 자궁내막증', '불임 원인 - 정자 농도', '불임 원인 - 정자 운동성', '불임 원인 - 정자 형태', '클리닉 내 총 시술 횟수', 'IVF 시술 횟수', 'DI 시술 횟수', '총 임신 횟수', 'IVF 임신 횟수', 'DI 임신 횟수', '총 출산 횟수', 'IVF 출산 횟수', 'DI 출산 횟수', '총 생성 배아 수', '미세주입된 난자 수', '미세주입에서 생성된 배아 수', '이식된 배아 수', '미세주입 배아 이식 수', '저장된 배아 수', '미세주입 후 저장된 배아 수', '해동된 배아 수', '해동 난자 수', '수집된 신선 난자 수', '저장된 신선 난자 수', '혼합된 난자 수', '파트너 정자와 혼합된 난자 수', '기증자 정자와 혼합된 난자 수', '동결 배아 사용 여부', '신선 배아 사용 여부', '기증 배아 사용 여부', '대리모 여부', 'PGD 시술 여부', 'PGS 시술 여부', '난자 혼합 경과일', '배아 이식 경과일', '배아 해동 경과일', '시술_임신', '배아생성이유_기증용', '배아생성이유_난자 저장용', '배아생성이유_배아 저장용', '배아생성이유_현재 시술용']\n",
      "범주형 변수: 8개 \n",
      "['시술 시기 코드', '시술 당시 나이', '배란 유도 유형', '난자 출처', '정자 출처', '난자 기증자 나이', '정자 기증자 나이', '시술유형_통합']\n",
      "(205080, 66) (51271, 65)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\AppData\\Local\\Temp\\ipykernel_9744\\2392652077.py:45: DeprecationWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, pd.CategoricalDtype) instead\n",
      "  cat_cols = [col for col in train.columns if pd.api.types.is_categorical_dtype(train[col])]\n"
     ]
    }
   ],
   "source": [
    "def all_process(train, val):\n",
    "    # 기본 전처리 단계\n",
    "    train, val = drop_columns(train), drop_columns(val)\n",
    "    train, val = 특정시술유형(train, val)\n",
    "    train, val = 시술횟수(train), 시술횟수(val)\n",
    "\n",
    "    train, val = 단일배아이식여부(train, val)\n",
    "    train, val = 배란유도유형(train, val)\n",
    "    train, val = 배아생성주요이유(train, val)\n",
    "\n",
    "    cols_to_encoding = [\n",
    "        \"시술 시기 코드\",\n",
    "        \"시술 당시 나이\",\n",
    "        \"배란 유도 유형\",\n",
    "        # \"클리닉 내 총 시술 횟수\",\n",
    "        # \"IVF 시술 횟수\",\n",
    "        # \"DI 시술 횟수\",\n",
    "        # \"총 임신 횟수\",\n",
    "        # \"IVF 임신 횟수\",\n",
    "        # \"DI 임신 횟수\",\n",
    "        # \"총 출산 횟수\",\n",
    "        # \"IVF 출산 횟수\",\n",
    "        # \"DI 출산 횟수\",\n",
    "        \"난자 출처\",\n",
    "        \"정자 출처\",\n",
    "        \"난자 기증자 나이\",\n",
    "        \"정자 기증자 나이\",\n",
    "        '시술유형_통합',\n",
    "    ]\n",
    "    train, val = label_encoding(train, val, cols=cols_to_encoding)\n",
    "    train, val = type_to_category(train, val, cols=cols_to_encoding)\n",
    "\n",
    "    train, val = impute_nan(train, val)\n",
    "    train, val = num_feature_scailing(train, val)\n",
    "\n",
    "    train, val = drop_single_value_columns(train, val)\n",
    "\n",
    "    return train, val\n",
    "\n",
    "train = pd.read_csv(train_path).drop(columns=[\"ID\"])\n",
    "test = pd.read_csv(test_path).drop(columns=[\"ID\"])\n",
    "\n",
    "train, test = all_process(train, test)\n",
    "\n",
    "cat_cols = [col for col in train.columns if pd.api.types.is_categorical_dtype(train[col])]\n",
    "numeric_cols = [col for col in train.columns if col not in cat_cols and col != '임신 성공 여부']\n",
    "\n",
    "print(f'수치형 변수: {len(numeric_cols)}개 \\n{numeric_cols}')\n",
    "print(f'범주형 변수: {len(cat_cols)}개 \\n{cat_cols}')\n",
    "print(train.shape, test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 실험 내용\n",
    "experiment_desc = '''\n",
    "WideDeep_TabFastFormer 임신 성공 여부 cat_embed_dropout=0.1\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\AppData\\Local\\Temp\\ipykernel_9744\\3576064510.py:69: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  df_train['배란 유도 유형'] = df_train['배란 유도 유형'].replace(mapping)\n",
      "C:\\Users\\User\\AppData\\Local\\Temp\\ipykernel_9744\\3576064510.py:70: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  df_test['배란 유도 유형'] = df_test['배란 유도 유형'].replace(mapping)\n",
      "C:\\Users\\User\\AppData\\Local\\Temp\\ipykernel_9744\\3576064510.py:199: DeprecationWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, pd.CategoricalDtype) instead\n",
      "  cat_cols = [col for col in train.columns if pd.api.types.is_categorical_dtype(train[col])]\n",
      "C:\\Users\\User\\AppData\\Local\\Temp\\ipykernel_9744\\3576064510.py:69: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  df_train['배란 유도 유형'] = df_train['배란 유도 유형'].replace(mapping)\n",
      "C:\\Users\\User\\AppData\\Local\\Temp\\ipykernel_9744\\3576064510.py:70: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  df_test['배란 유도 유형'] = df_test['배란 유도 유형'].replace(mapping)\n",
      "C:\\Users\\User\\AppData\\Local\\Temp\\ipykernel_9744\\3576064510.py:199: DeprecationWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, pd.CategoricalDtype) instead\n",
      "  cat_cols = [col for col in train.columns if pd.api.types.is_categorical_dtype(train[col])]\n",
      "C:\\Users\\User\\AppData\\Local\\Temp\\ipykernel_9744\\1243771325.py:37: DeprecationWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, pd.CategoricalDtype) instead\n",
      "  categorical_cols = [col for col in fold_train.columns if pd.api.types.is_categorical_dtype(fold_train[col])]\n",
      "c:\\Users\\User\\anaconda3\\envs\\lg2025\\lib\\site-packages\\pytorch_widedeep\\preprocessing\\tab_preprocessor.py:364: UserWarning: Continuous columns will not be normalised\n",
      "  warnings.warn(\"Continuous columns will not be normalised\")\n",
      "epoch 1:   2%|▏         | 28/1282 [04:39<3:28:20,  9.97s/it, loss=0.665, metrics={'acc': 0.68}]   \n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[12], line 200\u001b[0m\n\u001b[0;32m    187\u001b[0m trainer \u001b[38;5;241m=\u001b[39m Trainer(\n\u001b[0;32m    188\u001b[0m     model\u001b[38;5;241m=\u001b[39mmodel, \n\u001b[0;32m    189\u001b[0m     objective\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbinary\u001b[39m\u001b[38;5;124m\"\u001b[39m, \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    196\u001b[0m     seed\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m42\u001b[39m                     \u001b[38;5;66;03m# 랜덤 시드 설정 (기본값: 1)\u001b[39;00m\n\u001b[0;32m    197\u001b[0m )\n\u001b[0;32m    199\u001b[0m \u001b[38;5;66;03m# 학습: validation 데이터는 별도의 인자로 전달\u001b[39;00m\n\u001b[1;32m--> 200\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    201\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX_train\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# ✅ Training 데이터를 딕셔너리로 \u001b[39;49;00m\n\u001b[0;32m    202\u001b[0m \u001b[43m    \u001b[49m\u001b[43mn_epochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m100\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m    203\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m128\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m    204\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX_val\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mX_val\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# ✅ Validation 데이터를 딕셔너리로 전달\u001b[39;49;00m\n\u001b[0;32m    205\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    209\u001b[0m \u001b[38;5;66;03m# 모델 학습 후 Validation 예측 코드:\u001b[39;00m\n\u001b[0;32m    210\u001b[0m \u001b[38;5;66;03m# 이미 확률이 계산되어 있는 컬럼을 사용합니다.\u001b[39;00m\n\u001b[0;32m    211\u001b[0m valid_probs \u001b[38;5;241m=\u001b[39m trainer\u001b[38;5;241m.\u001b[39mpredict_proba(X_wide\u001b[38;5;241m=\u001b[39mX_wide_valid, X_tab\u001b[38;5;241m=\u001b[39mX_tab_valid)[:,\u001b[38;5;241m1\u001b[39m] \u001b[38;5;66;03m####\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\User\\anaconda3\\envs\\lg2025\\lib\\site-packages\\pytorch_widedeep\\utils\\general_utils.py:62\u001b[0m, in \u001b[0;36malias.<locals>.decorator.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     60\u001b[0m         kwargs[original_name] \u001b[38;5;241m=\u001b[39m kwargs\u001b[38;5;241m.\u001b[39mpop(alt_name)\n\u001b[0;32m     61\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[1;32m---> 62\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\User\\anaconda3\\envs\\lg2025\\lib\\site-packages\\pytorch_widedeep\\utils\\general_utils.py:62\u001b[0m, in \u001b[0;36malias.<locals>.decorator.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     60\u001b[0m         kwargs[original_name] \u001b[38;5;241m=\u001b[39m kwargs\u001b[38;5;241m.\u001b[39mpop(alt_name)\n\u001b[0;32m     61\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[1;32m---> 62\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\User\\anaconda3\\envs\\lg2025\\lib\\site-packages\\pytorch_widedeep\\utils\\general_utils.py:62\u001b[0m, in \u001b[0;36malias.<locals>.decorator.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     60\u001b[0m         kwargs[original_name] \u001b[38;5;241m=\u001b[39m kwargs\u001b[38;5;241m.\u001b[39mpop(alt_name)\n\u001b[0;32m     61\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[1;32m---> 62\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\User\\anaconda3\\envs\\lg2025\\lib\\site-packages\\pytorch_widedeep\\training\\trainer.py:470\u001b[0m, in \u001b[0;36mTrainer.fit\u001b[1;34m(self, X_wide, X_tab, X_text, X_img, X_train, X_val, val_split, target, n_epochs, validation_freq, batch_size, train_dataloader, eval_dataloader, feature_importance_sample_size, finetune, **kwargs)\u001b[0m\n\u001b[0;32m    462\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcallback_container\u001b[38;5;241m.\u001b[39mon_train_begin(\n\u001b[0;32m    463\u001b[0m     {\n\u001b[0;32m    464\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbatch_size\u001b[39m\u001b[38;5;124m\"\u001b[39m: batch_size,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    467\u001b[0m     }\n\u001b[0;32m    468\u001b[0m )\n\u001b[0;32m    469\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(n_epochs):\n\u001b[1;32m--> 470\u001b[0m     epoch_logs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_train_epoch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepoch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    471\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m eval_loader \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m epoch \u001b[38;5;241m%\u001b[39m validation_freq \u001b[38;5;241m==\u001b[39m (\n\u001b[0;32m    472\u001b[0m         validation_freq \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m    473\u001b[0m     ):\n\u001b[0;32m    474\u001b[0m         epoch_logs, on_epoch_end_metric \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_eval_epoch(\n\u001b[0;32m    475\u001b[0m             eval_loader, epoch_logs\n\u001b[0;32m    476\u001b[0m         )\n",
      "File \u001b[1;32mc:\\Users\\User\\anaconda3\\envs\\lg2025\\lib\\site-packages\\pytorch_widedeep\\training\\trainer.py:905\u001b[0m, in \u001b[0;36mTrainer._train_epoch\u001b[1;34m(self, train_loader, epoch)\u001b[0m\n\u001b[0;32m    903\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m batch_idx, (data, targett) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(t, train_loader):\n\u001b[0;32m    904\u001b[0m     t\u001b[38;5;241m.\u001b[39mset_description(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mepoch \u001b[39m\u001b[38;5;132;01m%i\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m (epoch \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m))\n\u001b[1;32m--> 905\u001b[0m     train_score, train_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_train_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtargett\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_idx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    906\u001b[0m     print_loss_and_metric(t, train_loss, train_score)\n\u001b[0;32m    907\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcallback_container\u001b[38;5;241m.\u001b[39mon_batch_end(batch\u001b[38;5;241m=\u001b[39mbatch_idx)\n",
      "File \u001b[1;32mc:\\Users\\User\\anaconda3\\envs\\lg2025\\lib\\site-packages\\pytorch_widedeep\\training\\trainer.py:946\u001b[0m, in \u001b[0;36mTrainer._train_step\u001b[1;34m(self, data, target, batch_idx)\u001b[0m\n\u001b[0;32m    943\u001b[0m     loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mloss_fn(y_pred, y)\n\u001b[0;32m    944\u001b[0m     score \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_score(y_pred, y, is_train\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m--> 946\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    947\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[0;32m    949\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrain_running_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mitem()\n",
      "File \u001b[1;32mc:\\Users\\User\\anaconda3\\envs\\lg2025\\lib\\site-packages\\torch\\_tensor.py:626\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    616\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    617\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    618\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[0;32m    619\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    624\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[0;32m    625\u001b[0m     )\n\u001b[1;32m--> 626\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    627\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[0;32m    628\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\User\\anaconda3\\envs\\lg2025\\lib\\site-packages\\torch\\autograd\\__init__.py:347\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    342\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[0;32m    344\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[0;32m    345\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    346\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 347\u001b[0m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    348\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    349\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    350\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    351\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    352\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    353\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    354\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    355\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\User\\anaconda3\\envs\\lg2025\\lib\\site-packages\\torch\\autograd\\graph.py:823\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[1;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[0;32m    821\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[0;32m    822\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 823\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m Variable\u001b[38;5;241m.\u001b[39m_execution_engine\u001b[38;5;241m.\u001b[39mrun_backward(  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[0;32m    824\u001b[0m         t_outputs, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[0;32m    825\u001b[0m     )  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[0;32m    826\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    827\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import time\n",
    "start_time = time.time()\n",
    "\n",
    "\n",
    "# import scipy.special   # 시그모이드 대신 softmax에 사용할 수 있음\n",
    "# 교차 검증 설정: seed_list를 [333] 하나만 사용, n_splits=3\n",
    "seed_list = [333]\n",
    "n_splits = 5\n",
    "\n",
    "total_auc, total_acc, total_f1 = [], [], []\n",
    "test_preds = []\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# 교차 검증 시작\n",
    "for seed in seed_list:\n",
    "    # train, test 불러오기\n",
    "    train = pd.read_csv(train_path).drop(columns=[\"ID\"])\n",
    "    test = pd.read_csv(test_path).drop(columns=[\"ID\"])\n",
    "\n",
    "    skf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=seed)\n",
    "    auc_scores, acc_scores, f1_scores = [], [], []\n",
    "    \n",
    "    for fold, (train_idx, valid_idx) in enumerate(skf.split(train.drop(columns=['임신 성공 여부']), train[\"임신 성공 여부\"])):\n",
    "        # Fold 데이터 생성\n",
    "        fold_train, fold_valid = train.iloc[train_idx], train.iloc[valid_idx]\n",
    "        fold_train2 = fold_train.copy()\n",
    "        fold_test = test.copy()  # test 데이터는 별도 사용\n",
    "        \n",
    "        # 전처리 \n",
    "        fold_train, fold_valid = all_process(fold_train, fold_valid)\n",
    "        _, fold_test = all_process(fold_train2, fold_test)\n",
    "\n",
    "        # 범주형, 연속형 열 구분분\n",
    "        categorical_cols = [col for col in fold_train.columns if pd.api.types.is_categorical_dtype(fold_train[col])]\n",
    "        continuous_cols = [col for col in fold_train.columns if col not in cat_cols and col != '임신 성공 여부']\n",
    "\n",
    "\n",
    "        # Wide 부분: 원-핫 인코딩 (crossed_cols 옵션도 사용할 수 있음)\n",
    "        wide_cols = categorical_cols  # Wide 모델에는 범주형 변수를 원-핫 인코딩으로 처리\n",
    "        crossed_cols = []  # 필요시 두 개 이상의 컬럼을 교차시켜 상호작용 feature 생성\n",
    "\n",
    "        # WidePreprocessor (원-핫 인코딩, crossed_cols 사용 가능)\n",
    "        wide_preprocessor = WidePreprocessor(wide_cols=wide_cols, crossed_cols=crossed_cols)\n",
    "\n",
    "        # Deep 부분: 임베딩 + 연속형 변수 처리\n",
    "        tab_preprocessor = TabPreprocessor(embed_cols=categorical_cols, continuous_cols=continuous_cols)\n",
    "\n",
    "        # 전처리: 각 Fold 별로 Wide & Deep 데이터 생성\n",
    "        X_wide_train = wide_preprocessor.fit_transform(fold_train)\n",
    "        X_wide_valid = wide_preprocessor.transform(fold_valid)\n",
    "        X_wide_test = wide_preprocessor.transform(fold_test)\n",
    "        X_tab_train = tab_preprocessor.fit_transform(fold_train)\n",
    "        X_tab_valid = tab_preprocessor.transform(fold_valid)\n",
    "        X_tab_test = tab_preprocessor.transform(fold_test)\n",
    "\n",
    "        # Target 값: 정수형 (0,1)\n",
    "        y_train = fold_train['임신 성공 여부'].astype(int).values\n",
    "        y_valid = fold_valid['임신 성공 여부'].astype(int).values\n",
    "\n",
    "        # Wide 모델: 입력 차원은 원-핫 인코딩된 피처 수####\n",
    "        wide_model = Wide(input_dim=int(X_wide_train.max().item()), pred_dim=1)\n",
    "\n",
    "\n",
    "        \n",
    "        # Deep 모델: TabFastFormer 사용\n",
    "        tab_model = TabFastFormer(\n",
    "            column_idx=tab_preprocessor.column_idx,  # 각 컬럼의 인덱스 정보\n",
    "            cat_embed_input=tab_preprocessor.cat_embed_input,  # (컬럼명, 고유값 수, 임베딩 차원)\n",
    "\n",
    "            continuous_cols=continuous_cols,  # 연속형 변수 리스트\n",
    "\n",
    "            # ▶ 범주형 임베딩 설정\n",
    "            cat_embed_dropout=0.1,  # 범주형 임베딩 드롭아웃\n",
    "            # 후보 값: 0.0 (기본), 0.1, 0.2, 0.3\n",
    "\n",
    "            use_cat_bias=False,  # 범주형 임베딩에 bias 사용 여부\n",
    "            # 후보 값: False (기본), True\n",
    "\n",
    "            cat_embed_activation=None,  # 범주형 임베딩 활성화 함수\n",
    "            # 후보 값: None (기본), 'relu', 'tanh', 'leaky_relu', 'gelu'\n",
    "\n",
    "\n",
    "            # ▶ 연속형 변수 설정\n",
    "            cont_norm_layer='batchnorm',  # 연속형 변수 정규화 방식\n",
    "            # 후보 값: None, 'batchnorm', 'layernorm'\n",
    "\n",
    "            embed_continuous_method='standard',  # 연속형 변수 임베딩 방식\n",
    "            # 후보 값: 'standard' (기본), 'piecewise', 'periodic'\n",
    "\n",
    "            cont_embed_dropout=0.0,  # 연속형 임베딩 드롭아웃\n",
    "            # 후보 값: 0.0 (기본), 0.1, 0.3\n",
    "\n",
    "            cont_embed_activation='relu',  # 연속형 임베딩 활성화 함수\n",
    "            # 후보 값: None (기본), 'relu', 'leaky_relu', 'gelu'\n",
    "\n",
    "            quantization_setup=None,  # piecewise 임베딩 시 구간 설정\n",
    "            # 후보 값: None (기본), {'age': [20, 30, 40], 'income': [10000, 30000]}\n",
    "\n",
    "\n",
    "            full_embed_dropout=False,  # 전체 임베딩 dropout 여부\n",
    "            # 후보 값: False (기본), True\n",
    "\n",
    "            # ▶ FastFormer 구조 설정\n",
    "            input_dim=32,  # 임베딩 차원\n",
    "            # 후보 값: 32 (기본), 64, 128\n",
    "\n",
    "            n_heads=8,  # 어텐션 헤드 개수\n",
    "            # 후보 값: 4, 8 (기본), 16\n",
    "\n",
    "            use_bias=False,  # QKV에 bias 적용 여부\n",
    "            # 후보 값: False (기본), True\n",
    "\n",
    "            n_blocks=4,  # FastFormer 블록 수\n",
    "            # 후보 값: 2, 4 (기본), 6\n",
    "\n",
    "            attn_dropout=0.1,  # 어텐션 드롭아웃\n",
    "            # 후보 값: 0.0, 0.1 (기본), 0.2\n",
    "\n",
    "\n",
    "            transformer_activation=\"relu\",  # Transformer 내부 활성화 함수\n",
    "            # 후보 값: 'relu' (기본), 'gelu', 'geglu', 'reglu', 'tanh', 'leaky_relu'\n",
    "\n",
    "            # ▶ MLP 추가 설정 (선택적)\n",
    "            mlp_hidden_dims=[64, 32],  # MLP 은닉층 크기\n",
    "            # 후보 값: [64, 32] (기본), [128, 64], None (사용 안 함)\n",
    "\n",
    "            mlp_activation=\"relu\",  # MLP 활성화 함수\n",
    "            # 후보 값: 'relu' (기본), 'gelu', 'leaky_relu', 'tanh'\n",
    "\n",
    "            mlp_dropout=0.1,  # MLP dropout\n",
    "            # 후보 값: 0.0, 0.1 (기본), 0.3\n",
    "\n",
    "            mlp_batchnorm=False,  # MLP에 배치 정규화 사용 여부\n",
    "            # 후보 값: False (기본), True\n",
    "\n",
    "            mlp_batchnorm_last=False,  # 마지막 층에도 배치 정규화\n",
    "            # 후보 값: False (기본), True\n",
    "\n",
    "            mlp_linear_first=True  # MLP 연산 순서\n",
    "            # 후보 값: True (기본), False\n",
    "        )\n",
    "        \n",
    "        # Wide & Deep 모델 결합\n",
    "        model = WideDeep(wide=wide_model, deeptabular=tab_model)\n",
    "        \n",
    "        # 옵티마이저 및 학습률 스케줄러 설정\n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr=0.0001)  # Set your desired learning rate here\n",
    "        \n",
    "\n",
    "        # ✅ Training 데이터를 딕셔너리 형태로 생성\n",
    "        X_train = {\n",
    "            \"X_wide\": X_wide_train,\n",
    "            \"X_tab\": X_tab_train,\n",
    "            \"target\": y_train\n",
    "        }\n",
    "\n",
    "        # ✅ Validation 데이터를 딕셔너리로 전달 (X_val 방식 사용)\n",
    "        X_val = {\n",
    "            \"X_wide\": X_wide_valid,\n",
    "            \"X_tab\": X_tab_valid,\n",
    "            \"target\": y_valid\n",
    "        }\n",
    "        \n",
    "        # EarlyStopping 콜백 설정 (patience=5, min_delta=0.001)\n",
    "        early_stopping = EarlyStopping(\n",
    "            monitor=\"val_loss\", \n",
    "            patience=5, \n",
    "            min_delta=0.001, \n",
    "            verbose=1\n",
    "        )\n",
    "        \n",
    "        # 현재 날짜/시간을 포함한 파일 이름 생성\n",
    "        timestamp = datetime.datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "        model_path = f\"saved_models/best_model_{timestamp}.pt\"\n",
    "        \n",
    "        # ✅ 2. ModelCheckpoint: 최상의 모델을 자동 저장\n",
    "        model_checkpoint = ModelCheckpoint(\n",
    "            filepath=model_path,  # 모델 저장 경로\n",
    "            monitor=\"val_loss\",        # 감시할 지표 ('val_loss' 또는 'val_acc')\n",
    "            save_best_only=True        # 가장 좋은 성능의 모델만 저장\n",
    "        )\n",
    "        \n",
    "        # Trainer 생성: objective \"binary\"로 설정, 평가 지표로 Accuracy, AUROC, F1Score 사용\n",
    "        trainer = Trainer(\n",
    "            model=model, \n",
    "            objective=\"binary\", \n",
    "            custom_loss_function=None,  # 기본 손실 함수 사용 (필요 시 사용자 정의 함수 가능)\n",
    "            optimizers=optimizer,       # 옵티마이저 (기본값: Adam)\n",
    "            initializers=None,          # 가중치 초기화 없음 (기본값)\n",
    "            callbacks=[early_stopping, model_checkpoint],  # 조기 종료 (3 에포크 동안 개선 없으면 종료)\n",
    "            metrics=[Accuracy()],\n",
    "            verbose=1,                  # 학습 로그 출력 (기본값: 1)\n",
    "            seed=42                     # 랜덤 시드 설정 (기본값: 1)\n",
    "        )\n",
    "        \n",
    "        # 학습: validation 데이터는 별도의 인자로 전달\n",
    "        trainer.fit(\n",
    "            X_train=X_train,  # ✅ Training 데이터를 딕셔너리로 \n",
    "            n_epochs=100, \n",
    "            batch_size=1024, \n",
    "            X_val=X_val  # ✅ Validation 데이터를 딕셔너리로 전달\n",
    "        )\n",
    "        \n",
    "        \n",
    "        \n",
    "        # 모델 학습 후 Validation 예측 코드:\n",
    "        # 이미 확률이 계산되어 있는 컬럼을 사용합니다.\n",
    "        valid_probs = trainer.predict_proba(X_wide=X_wide_valid, X_tab=X_tab_valid)[:,1] ####\n",
    "        valid_pred = (valid_probs > 0.5).astype(int)\n",
    "\n",
    "        \n",
    "\n",
    "        # 실제 정답 \n",
    "        y_valid = fold_valid['임신 성공 여부'].values.astype(int)\n",
    "\n",
    "\n",
    "        \n",
    "        # 평가 지표 계산: 클래스 1의 확률 사용\n",
    "        fold_auc = roc_auc_score(y_valid, valid_probs)\n",
    "        print(f\"Seed[{seed:<3}] Fold {fold + 1} | AUC: {fold_auc:.6f}\")\n",
    "        \n",
    "        auc_scores.append(fold_auc)\n",
    "        \n",
    "        total_auc.append(fold_auc)\n",
    "        # Test 데이터 예측 (각 fold의 모델로 예측한 결과 저장)\n",
    "        test_pred = trainer.predict_proba(X_wide=X_wide_test, X_tab=X_tab_test)[:,1]\n",
    "        test_preds.append(test_pred)\n",
    "    \n",
    "    # Fold 별 평균 성능 출력\n",
    "    avg_auc = np.mean(auc_scores)\n",
    "    \n",
    "    print(\"-\" * 80)\n",
    "    print(f\"Seed[{seed:<3}] Average Metrics | AUC: {avg_auc:.7f}\")\n",
    "    print(\"-\" * 80)\n",
    "\n",
    "# 전체 Validation 평균 성능 출력\n",
    "val_auc = np.mean(total_auc)\n",
    "print(\"-\" * 80)\n",
    "print(f\"Validation Average Metrics | AUC: {val_auc:.7f}\")\n",
    "\n",
    "finish_time = time.time()\n",
    "total_time = finish_time - start_time \n",
    "\n",
    "print(total_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# stratify_label = train[\"임신 성공 여부\"].astype(str)\n",
    "old_auc = 0.7289820 * 100\n",
    "\n",
    "new_auc = val_auc * 100\n",
    "\n",
    "\n",
    "def calculate_change(old_value, new_value):\n",
    "    change = new_value - old_value\n",
    "    percentage_change = (change / old_value) * 100 if old_value != 0 else float('inf')\n",
    "    return change, percentage_change\n",
    "\n",
    "def format_change(change):\n",
    "    return f\"{change:+.6f}\"\n",
    "\n",
    "# 각 지표의 변화량 계산\n",
    "auc_change, auc_pct = calculate_change(old_auc, new_auc)\n",
    "\n",
    "\n",
    "# 결과 출력\n",
    "print(\"\\n====== 모델 성능 변화 ======\")\n",
    "print(f\"{'Metric':<8}  {'AUC':>12}\")\n",
    "print(\"-\" * 30)\n",
    "print(f\"{'Old':<8}  {old_auc:>12.6f}\")\n",
    "print(f\"{'New':<8}  {new_auc:>12.6f}\")\n",
    "print(f\"{'Change':<8}  {format_change(auc_change):>12}\")\n",
    "print(f\"{'% Change':<8}  {auc_pct:>11.4f}%\")\n",
    "print(\"=\" * 30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp_submission = pd.DataFrame({f'tabm_{data_seed}': np.mean(test_preds, axis=0)})\n",
    "tmp_submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from LG_Aimers_6th.cal_auc import calculate_auc\n",
    "score = calculate_auc(tmp_submission, seed=data_seed)\n",
    "print(f'[seed {data_seed}]: {score}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 기본 test 데이터 AUC : 0.732737"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_submission = pd.read_csv(sample_path)\n",
    "# test_preds\n",
    "# sample_submission['임신 성공 확률'] = np.mean(test_preds, axis=0)\n",
    "\n",
    "# ratio = train['임신 성공 여부'].value_counts(normalize=True)[1]\n",
    "# real_true_count = int(ratio * len(sample_submission))\n",
    "# print(f'test의 True 갯수: {real_true_count:<5} (추정)')\n",
    "\n",
    "# count = (sample_submission['임신 성공 확률'] >= 0.5).sum()\n",
    "# print(f'test의 True 갯수: {count:<5} (예측 결과)')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "now = datetime.datetime.now().strftime(\"%m%d_%H%M\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission_path = 'Submission'\n",
    "if not os.path.exists(submission_path):\n",
    "    os.makedirs(submission_path)\n",
    "\n",
    "code_dir = 'Code'\n",
    "if not os.path.exists(code_dir):\n",
    "    os.makedirs(code_dir)\n",
    "\n",
    "\n",
    "submission_name = f\"submission_{now}.csv\"\n",
    "new_notebook_name = f\"code_{now}.ipynb\"\n",
    "\n",
    "sample_submission.to_csv(os.path.join(submission_path, submission_name), index=False)\n",
    "\n",
    "\n",
    "# 현재 노트북 파일 경로 직접 지정 (실제 노트북 파일명으로 수정)\n",
    "current_notebook = os.path.join(os.getcwd(), \"WideDeep_TabMlp_임신 여부.ipynb\")\n",
    "\n",
    "new_notebook_path = os.path.join(code_dir, new_notebook_name)\n",
    "\n",
    "# 노트북 파일 복사\n",
    "shutil.copy(current_notebook, new_notebook_path)\n",
    "\n",
    "print(f\"Notebook saved in '{code_dir}' as '{new_notebook_name}'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 📌 SQLite 데이터베이스 설정\n",
    "db_path = \"experiment_results.db\"\n",
    "conn = sqlite3.connect(db_path)\n",
    "cursor = conn.cursor()\n",
    "\n",
    "# 📌 테이블 생성 (처음 실행 시)\n",
    "cursor.execute('''\n",
    "CREATE TABLE IF NOT EXISTS experiments (\n",
    "    id INTEGER PRIMARY KEY AUTOINCREMENT,\n",
    "    code_name TEXT,\n",
    "    experiment_desc TEXT,\n",
    "    auc REAL,\n",
    "    acc REAL,\n",
    "    f1 REAL\n",
    ")\n",
    "''')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터 삽입\n",
    "cursor.execute('''\n",
    "INSERT INTO experiments (code_name, experiment_desc, auc, acc, f1)\n",
    "VALUES (?, ?, ?, ?, ?)\n",
    "''', (new_notebook_name, experiment_desc.strip(), new_auc, new_acc, new_f1))\n",
    "\n",
    "# 변경사항 저장 & 연결 종료\n",
    "conn.commit()\n",
    "conn.close()\n",
    "\n",
    "print(f\"Experiment '{new_notebook_name}' successfully saved in database\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sqlite3\n",
    "import pandas as pd\n",
    "\n",
    "# SQLite 데이터 조회 함수\n",
    "def get_experiment_results(db_path=\"experiment_results.db\", num_results=10):\n",
    "    \"\"\"\n",
    "    SQLite 데이터베이스에서 중복된 실험 데이터를 제거하고, 최근 num_results개의 실험 데이터를 불러오는 함수.\n",
    "    Returns:\n",
    "        - Pandas DataFrame: 중복 제거된 실험 데이터\n",
    "    \"\"\"\n",
    "    conn = sqlite3.connect(db_path)\n",
    "\n",
    "    # 중복 제거 & 최신 데이터 선택하는 SQL 쿼리\n",
    "    query = f\"\"\"\n",
    "    SELECT * FROM experiments\n",
    "    WHERE id IN (\n",
    "        SELECT MAX(id)  -- 가장 최신 데이터 선택\n",
    "        FROM experiments\n",
    "        GROUP BY code_name -- id 제외하고 중복 판단\n",
    "    )\n",
    "    ORDER BY id DESC  -- 최신 데이터부터 정렬\n",
    "    LIMIT {num_results};\n",
    "    \"\"\"\n",
    "\n",
    "    df = pd.read_sql_query(query, conn)\n",
    "    conn.close()\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_results = get_experiment_results(num_results=100)\n",
    "df_results.to_csv('experiment_results.csv', index=False, encoding='utf-8-sig', float_format='%.4f')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lg2025",
   "language": "python",
   "name": "lg2025"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
