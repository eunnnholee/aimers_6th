{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# WideDeep_TabMlp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„¤ì¹˜"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install numpy==1.26.0\n",
    "# !pip install pandas==2.2.2\n",
    "# !pip install scikit-learn==1.5.1\n",
    "# !pip install scipy==1.14.1\n",
    "# !pip install statsmodels==0.14.2\n",
    "# !pip install joblib==1.4.2\n",
    "# !pip install threadpoolctl==3.5.0\n",
    "# !pip install ipynbname"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 1. ê¸°ì¡´ì˜ íŒ¨í‚¤ì§€ ì •ë¦¬\n",
    "# !pip uninstall -y torch torchvision torchaudio pytorch-lightning pytorch-tabular\n",
    "\n",
    "# # 2. í˜¸í™˜ ê°€ëŠ¥í•œ ë²„ì „ìœ¼ë¡œ ì¬ì„¤ì¹˜\n",
    "# !pip install torch==2.0.1+cu118 --index-url https://download.pytorch.org/whl/cu118\n",
    "# !pip install pytorch-tabular==1.1.1 --no-deps\n",
    "# !pip install pytorch-lightning==2.0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip3 install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install pytorch-widedeep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install opencv-python-headless"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sqlite3\n",
    "import shutil\n",
    "import ipynbname\n",
    "import datetime\n",
    "import os\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "\n",
    "\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from sklearn.metrics import roc_auc_score, accuracy_score, f1_score, log_loss\n",
    "\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "\n",
    "\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "\n",
    "\n",
    "import torch\n",
    "\n",
    "# pytorch-widedeep ë¼ì´ë¸ŒëŸ¬ë¦¬ import\n",
    "from pytorch_widedeep import Trainer\n",
    "from pytorch_widedeep.preprocessing import WidePreprocessor, TabPreprocessor\n",
    "from pytorch_widedeep.models import Wide, TabMlp, WideDeep\n",
    "from pytorch_widedeep.metrics import Accuracy,F1Score\n",
    "from pytorch_widedeep.callbacks import EarlyStopping, ModelCheckpoint\n",
    "\n",
    "\n",
    "\n",
    "train_path = 'C:/Users/User/Desktop/LG Aimers/LG Aimers 2025/LG Aimers 2025 ë°ì´í„°/train.csv'\n",
    "test_path = 'C:/Users/User/Desktop/LG Aimers/LG Aimers 2025/LG Aimers 2025 ë°ì´í„°/test.csv'\n",
    "sample_path = 'C:/Users/User/Desktop/LG Aimers/LG Aimers 2025/LG Aimers 2025 ë°ì´í„°//sample_submission.csv'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv(train_path).drop(columns=[\"ID\"])\n",
    "test = pd.read_csv(test_path).drop(columns=[\"ID\"])\n",
    "print(train.shape, test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler, FunctionTransformer\n",
    "\n",
    "def drop_columns(df):\n",
    "    cols = [\n",
    "        'ë¶ˆì„ ì›ì¸ - ì—¬ì„± ìš”ì¸', # ê³ ìœ ê°’ 1\n",
    "        'ë¶ˆì„ ì›ì¸ - ì •ì ë©´ì—­í•™ì  ìš”ì¸' # '1'ì¸ ë°ì´í„°ê°€ train, testì—ì„œ ëª¨ë‘ 1ê°œ >> ì‹ ë¢°í•  ìˆ˜ ì—†ìŒ\n",
    "    ]\n",
    "    df = df.drop(cols, axis=1)\n",
    "    return df\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "def íŠ¹ì •ì‹œìˆ ìœ í˜•(train, test):\n",
    "    def categorize_procedure(proc):\n",
    "        tokens = [token.strip() for token in proc.split(\",\") if token.strip() and not token.strip().isdigit()]\n",
    "        # ìš°ì„ ìˆœìœ„ì— ë”°ë¥¸ ë²”ì£¼í™”\n",
    "        if tokens.count(\"Unknown\") >= 1:\n",
    "            return \"Unknown\"\n",
    "        if tokens.count(\"AH\") >= 1:\n",
    "            return \"AH\"\n",
    "        if tokens.count(\"BLASTOCYST\") >= 1:\n",
    "            return \"BLASTOCYST\"\n",
    "        if tokens.count(\"ICSI\") >= 2 or tokens.count(\"IVF\") >= 2:\n",
    "            return \"2ICSI_2IVF\"\n",
    "        if tokens.count(\"IVF\") >= 1 and tokens.count(\"ICSI\") >= 1:\n",
    "            return \"IVF_ICSI\"\n",
    "        if tokens == \"ICSI\":\n",
    "            return \"ICSI\"\n",
    "        if tokens == \"IVF\":\n",
    "            return \"IVF\"\n",
    "        return \",\".join(tokens) if tokens else None\n",
    "\n",
    "    for df in [train, test]:\n",
    "        df['íŠ¹ì • ì‹œìˆ  ìœ í˜•'] = df['íŠ¹ì • ì‹œìˆ  ìœ í˜•'].str.replace(\" / \", \",\")\n",
    "        df['íŠ¹ì • ì‹œìˆ  ìœ í˜•'] = df['íŠ¹ì • ì‹œìˆ  ìœ í˜•'].str.replace(\":\", \",\")\n",
    "        df['íŠ¹ì • ì‹œìˆ  ìœ í˜•'] = df['íŠ¹ì • ì‹œìˆ  ìœ í˜•'].str.replace(\" \", \"\")\n",
    "\n",
    "    counts = train['íŠ¹ì • ì‹œìˆ  ìœ í˜•'].value_counts()\n",
    "    allowed_categories = counts[counts >= 100].index.tolist()\n",
    "\n",
    "    # allowed_categoriesì— ì†í•˜ì§€ ì•ŠëŠ” ê°’ì€ \"Unknown\"ìœ¼ë¡œ ëŒ€ì²´\n",
    "    train.loc[~train['íŠ¹ì • ì‹œìˆ  ìœ í˜•'].isin(allowed_categories), 'íŠ¹ì • ì‹œìˆ  ìœ í˜•'] = \"Unknown\"\n",
    "    test.loc[~test['íŠ¹ì • ì‹œìˆ  ìœ í˜•'].isin(allowed_categories), 'íŠ¹ì • ì‹œìˆ  ìœ í˜•'] = \"Unknown\"\n",
    "\n",
    "    train['íŠ¹ì • ì‹œìˆ  ìœ í˜•'] = train['íŠ¹ì • ì‹œìˆ  ìœ í˜•'].apply(categorize_procedure)\n",
    "    test['íŠ¹ì • ì‹œìˆ  ìœ í˜•'] = test['íŠ¹ì • ì‹œìˆ  ìœ í˜•'].apply(categorize_procedure)\n",
    "\n",
    "    train['ì‹œìˆ ìœ í˜•_í†µí•©'] = train['ì‹œìˆ  ìœ í˜•'].astype(str) + '_' + train['íŠ¹ì • ì‹œìˆ  ìœ í˜•'].astype(str)\n",
    "    test['ì‹œìˆ ìœ í˜•_í†µí•©'] = test['ì‹œìˆ  ìœ í˜•'].astype(str) + '_' + test['íŠ¹ì • ì‹œìˆ  ìœ í˜•'].astype(str)\n",
    "\n",
    "    drop_cols = ['ì‹œìˆ  ìœ í˜•', 'íŠ¹ì • ì‹œìˆ  ìœ í˜•']\n",
    "    train = train.drop(drop_cols, axis=1)\n",
    "    test = test.drop(drop_cols, axis=1)\n",
    "\n",
    "    return train, test\n",
    "\n",
    "def ì‹œìˆ íšŸìˆ˜(df_train):\n",
    "    for col in [col for col in df_train.columns if 'íšŸìˆ˜' in col]:\n",
    "        df_train[col] = df_train[col].replace({'6íšŒ ì´ìƒ':'6íšŒ'})\n",
    "        df_train[col] = df_train[col].str[0].astype(int)\n",
    "    df_train['ì‹œìˆ _ì„ì‹ '] = df_train['ì´ ì„ì‹  íšŸìˆ˜'] - df_train['ì´ ì‹œìˆ  íšŸìˆ˜']\n",
    "    df_train = df_train.drop('ì´ ì‹œìˆ  íšŸìˆ˜', axis=1)\n",
    "    return df_train\n",
    "\n",
    "def numeric_process(train, test, cols_to_numeric=None, value='zero'):\n",
    "    if cols_to_numeric is None:\n",
    "        cols_to_numeric = [\n",
    "            'ì„ì‹  ì‹œë„ ë˜ëŠ” ë§ˆì§€ë§‰ ì„ì‹  ê²½ê³¼ ì—°ìˆ˜',\n",
    "            'ë°°ë€ ìê·¹ ì—¬ë¶€',\n",
    "            'ë‹¨ì¼ ë°°ì•„ ì´ì‹ ì—¬ë¶€',\n",
    "            'ì°©ìƒ ì „ ìœ ì „ ê²€ì‚¬ ì‚¬ìš© ì—¬ë¶€',\n",
    "            'ì°©ìƒ ì „ ìœ ì „ ì§„ë‹¨ ì‚¬ìš© ì—¬ë¶€',\n",
    "            'ì´ ìƒì„± ë°°ì•„ ìˆ˜',\n",
    "            'ë¯¸ì„¸ì£¼ì…ëœ ë‚œì ìˆ˜',\n",
    "            'ë¯¸ì„¸ì£¼ì…ì—ì„œ ìƒì„±ëœ ë°°ì•„ ìˆ˜',\n",
    "            'ì´ì‹ëœ ë°°ì•„ ìˆ˜',\n",
    "            'ë¯¸ì„¸ì£¼ì… ë°°ì•„ ì´ì‹ ìˆ˜',\n",
    "            'ì €ì¥ëœ ë°°ì•„ ìˆ˜',\n",
    "            'ë¯¸ì„¸ì£¼ì… í›„ ì €ì¥ëœ ë°°ì•„ ìˆ˜',\n",
    "            'í•´ë™ëœ ë°°ì•„ ìˆ˜',\n",
    "            'í•´ë™ ë‚œì ìˆ˜',\n",
    "            'ìˆ˜ì§‘ëœ ì‹ ì„  ë‚œì ìˆ˜',\n",
    "            'ì €ì¥ëœ ì‹ ì„  ë‚œì ìˆ˜',\n",
    "            'í˜¼í•©ëœ ë‚œì ìˆ˜',\n",
    "            'íŒŒíŠ¸ë„ˆ ì •ìì™€ í˜¼í•©ëœ ë‚œì ìˆ˜',\n",
    "            'ê¸°ì¦ì ì •ìì™€ í˜¼í•©ëœ ë‚œì ìˆ˜',\n",
    "            'ë™ê²° ë°°ì•„ ì‚¬ìš© ì—¬ë¶€',\n",
    "            'ì‹ ì„  ë°°ì•„ ì‚¬ìš© ì—¬ë¶€',\n",
    "            'ê¸°ì¦ ë°°ì•„ ì‚¬ìš© ì—¬ë¶€',\n",
    "            'ëŒ€ë¦¬ëª¨ ì—¬ë¶€',\n",
    "            'PGD ì‹œìˆ  ì—¬ë¶€',\n",
    "            'PGS ì‹œìˆ  ì—¬ë¶€',\n",
    "            'ë‚œì ì±„ì·¨ ê²½ê³¼ì¼',\n",
    "            'ë‚œì í˜¼í•© ê²½ê³¼ì¼',\n",
    "            'ë°°ì•„ ì´ì‹ ê²½ê³¼ì¼',\n",
    "            'ë°°ì•„ í•´ë™ ê²½ê³¼ì¼',\n",
    "        ]\n",
    "    if value == 'mean':\n",
    "        imputer_mean = SimpleImputer(strategy='mean')\n",
    "        train[cols_to_numeric] = imputer_mean.fit_transform(train[cols_to_numeric])\n",
    "        test[cols_to_numeric] = imputer_mean.transform(test[cols_to_numeric])\n",
    "    elif value == 'median':\n",
    "        imputer_median = SimpleImputer(strategy='median')\n",
    "        train[cols_to_numeric] = imputer_median.fit_transform(train[cols_to_numeric])\n",
    "        test[cols_to_numeric] = imputer_median.transform(test[cols_to_numeric])\n",
    "    elif value == 'zero':\n",
    "        train[cols_to_numeric] = train[cols_to_numeric].fillna(0)\n",
    "        test[cols_to_numeric] = test[cols_to_numeric].fillna(0)\n",
    "    else:\n",
    "        train[cols_to_numeric] = train[cols_to_numeric].fillna(value).astype(float)\n",
    "        test[cols_to_numeric] = test[cols_to_numeric].fillna(value).astype(float)\n",
    "    return train, test\n",
    "\n",
    "\n",
    "def encoding(train, test, seed=42):\n",
    "    categorical_columns = [\n",
    "        \"ì‹œìˆ  ì‹œê¸° ì½”ë“œ\",\n",
    "        \"ì‹œìˆ  ë‹¹ì‹œ ë‚˜ì´\",\n",
    "        \"ë°°ë€ ìœ ë„ ìœ í˜•\",\n",
    "        \"ë°°ì•„ ìƒì„± ì£¼ìš” ì´ìœ \",\n",
    "\n",
    "        ## ì‹œìˆ  íšŸìˆ˜\n",
    "        \"í´ë¦¬ë‹‰ ë‚´ ì´ ì‹œìˆ  íšŸìˆ˜\",\n",
    "        \"IVF ì‹œìˆ  íšŸìˆ˜\",\n",
    "        \"DI ì‹œìˆ  íšŸìˆ˜\",\n",
    "\n",
    "        ## ì„ì‹  íšŸìˆ˜\n",
    "        \"ì´ ì„ì‹  íšŸìˆ˜\",\n",
    "        \"IVF ì„ì‹  íšŸìˆ˜\",\n",
    "        \"DI ì„ì‹  íšŸìˆ˜\",\n",
    "\n",
    "        ## ì¶œì‚° íšŸìˆ˜\n",
    "        \"ì´ ì¶œì‚° íšŸìˆ˜\",\n",
    "        \"IVF ì¶œì‚° íšŸìˆ˜\",\n",
    "        \"DI ì¶œì‚° íšŸìˆ˜\",\n",
    "\n",
    "        \"ë‚œì ì¶œì²˜\",\n",
    "        \"ì •ì ì¶œì²˜\",\n",
    "        \"ë‚œì ê¸°ì¦ì ë‚˜ì´\",\n",
    "        \"ì •ì ê¸°ì¦ì ë‚˜ì´\",\n",
    "\n",
    "        'ì‹œìˆ ìœ í˜•_í†µí•©', # íŠ¹ì •ì‹œìˆ ìœ í˜•()\n",
    "    ]\n",
    "    train[categorical_columns] = train[categorical_columns].astype(str)\n",
    "    test[categorical_columns] = test[categorical_columns].astype(str)\n",
    "\n",
    "    train[categorical_columns] = train[categorical_columns].astype('category')\n",
    "    test[categorical_columns] = test[categorical_columns].astype('category')\n",
    "\n",
    "    return train, test\n",
    "\n",
    "\n",
    "def num_feature_scailing(train, test):\n",
    "    cols_to_divide = [\n",
    "            'ì„ì‹  ì‹œë„ ë˜ëŠ” ë§ˆì§€ë§‰ ì„ì‹  ê²½ê³¼ ì—°ìˆ˜',\n",
    "            'ë°°ë€ ìê·¹ ì—¬ë¶€',\n",
    "            'ë‹¨ì¼ ë°°ì•„ ì´ì‹ ì—¬ë¶€',\n",
    "            'ì°©ìƒ ì „ ìœ ì „ ê²€ì‚¬ ì‚¬ìš© ì—¬ë¶€',\n",
    "            'ì°©ìƒ ì „ ìœ ì „ ì§„ë‹¨ ì‚¬ìš© ì—¬ë¶€',\n",
    "            'ì´ ìƒì„± ë°°ì•„ ìˆ˜',\n",
    "            'ë¯¸ì„¸ì£¼ì…ëœ ë‚œì ìˆ˜',\n",
    "            'ë¯¸ì„¸ì£¼ì…ì—ì„œ ìƒì„±ëœ ë°°ì•„ ìˆ˜',\n",
    "            'ì €ì¥ëœ ë°°ì•„ ìˆ˜',\n",
    "            'ë¯¸ì„¸ì£¼ì… í›„ ì €ì¥ëœ ë°°ì•„ ìˆ˜',\n",
    "            'í•´ë™ëœ ë°°ì•„ ìˆ˜',\n",
    "            'í•´ë™ ë‚œì ìˆ˜',\n",
    "            'ìˆ˜ì§‘ëœ ì‹ ì„  ë‚œì ìˆ˜',\n",
    "            'ì €ì¥ëœ ì‹ ì„  ë‚œì ìˆ˜',\n",
    "            'í˜¼í•©ëœ ë‚œì ìˆ˜',\n",
    "            'íŒŒíŠ¸ë„ˆ ì •ìì™€ í˜¼í•©ëœ ë‚œì ìˆ˜',\n",
    "            'ê¸°ì¦ì ì •ìì™€ í˜¼í•©ëœ ë‚œì ìˆ˜',\n",
    "            'ë™ê²° ë°°ì•„ ì‚¬ìš© ì—¬ë¶€',\n",
    "            'ì‹ ì„  ë°°ì•„ ì‚¬ìš© ì—¬ë¶€',\n",
    "            'ê¸°ì¦ ë°°ì•„ ì‚¬ìš© ì—¬ë¶€',\n",
    "            'ëŒ€ë¦¬ëª¨ ì—¬ë¶€',\n",
    "            'PGD ì‹œìˆ  ì—¬ë¶€',\n",
    "            'PGS ì‹œìˆ  ì—¬ë¶€',\n",
    "        ]\n",
    "    # # í¬ê¸°ì˜ ì°¨ì´ë¥¼ ì¤„ì´ê¸° ìœ„í•´ì„œì„œ\n",
    "    # train[cols_to_divide] = train[cols_to_divide] / 10\n",
    "    # test[cols_to_divide] = test[cols_to_divide] / 10\n",
    "\n",
    "    cols_to_log = [\n",
    "            'ì„ì‹  ì‹œë„ ë˜ëŠ” ë§ˆì§€ë§‰ ì„ì‹  ê²½ê³¼ ì—°ìˆ˜',\n",
    "            'ë°°ë€ ìê·¹ ì—¬ë¶€',\n",
    "            'ë‹¨ì¼ ë°°ì•„ ì´ì‹ ì—¬ë¶€',\n",
    "            'ì°©ìƒ ì „ ìœ ì „ ê²€ì‚¬ ì‚¬ìš© ì—¬ë¶€',\n",
    "            'ì°©ìƒ ì „ ìœ ì „ ì§„ë‹¨ ì‚¬ìš© ì—¬ë¶€',\n",
    "            'ì´ ìƒì„± ë°°ì•„ ìˆ˜',\n",
    "            'ë¯¸ì„¸ì£¼ì…ëœ ë‚œì ìˆ˜',\n",
    "            'ë¯¸ì„¸ì£¼ì…ì—ì„œ ìƒì„±ëœ ë°°ì•„ ìˆ˜',\n",
    "            'ì´ì‹ëœ ë°°ì•„ ìˆ˜',\n",
    "            'ë¯¸ì„¸ì£¼ì… ë°°ì•„ ì´ì‹ ìˆ˜',\n",
    "            'ì €ì¥ëœ ë°°ì•„ ìˆ˜',\n",
    "            'ë¯¸ì„¸ì£¼ì… í›„ ì €ì¥ëœ ë°°ì•„ ìˆ˜',\n",
    "            'í•´ë™ëœ ë°°ì•„ ìˆ˜',\n",
    "            'í•´ë™ ë‚œì ìˆ˜',\n",
    "            'ìˆ˜ì§‘ëœ ì‹ ì„  ë‚œì ìˆ˜',\n",
    "            'ì €ì¥ëœ ì‹ ì„  ë‚œì ìˆ˜',\n",
    "            'í˜¼í•©ëœ ë‚œì ìˆ˜',\n",
    "            'íŒŒíŠ¸ë„ˆ ì •ìì™€ í˜¼í•©ëœ ë‚œì ìˆ˜',\n",
    "            'ê¸°ì¦ì ì •ìì™€ í˜¼í•©ëœ ë‚œì ìˆ˜',\n",
    "            'ë™ê²° ë°°ì•„ ì‚¬ìš© ì—¬ë¶€',\n",
    "            'ì‹ ì„  ë°°ì•„ ì‚¬ìš© ì—¬ë¶€',\n",
    "            'ê¸°ì¦ ë°°ì•„ ì‚¬ìš© ì—¬ë¶€',\n",
    "            'ëŒ€ë¦¬ëª¨ ì—¬ë¶€',\n",
    "            'PGD ì‹œìˆ  ì—¬ë¶€',\n",
    "            'PGS ì‹œìˆ  ì—¬ë¶€',\n",
    "            'ë‚œì ì±„ì·¨ ê²½ê³¼ì¼',\n",
    "            'ë‚œì í˜¼í•© ê²½ê³¼ì¼',\n",
    "            'ë°°ì•„ ì´ì‹ ê²½ê³¼ì¼',\n",
    "            'ë°°ì•„ í•´ë™ ê²½ê³¼ì¼',\n",
    "        ]\n",
    "    log_transformer = FunctionTransformer(np.log1p, validate=True)\n",
    "    train[cols_to_log] = log_transformer.fit_transform(train[cols_to_log])\n",
    "    test[cols_to_log] = log_transformer.transform(test[cols_to_log])\n",
    "\n",
    "    numeric_cols = train.select_dtypes(include=[\"number\"]).columns.tolist()\n",
    "    cat_cols = [col for col in train.columns if pd.api.types.is_categorical_dtype(train[col])]\n",
    "    cols_to_scale = [\n",
    "        col for col in numeric_cols\n",
    "        if col not in cat_cols and col != 'ì„ì‹  ì„±ê³µ ì—¬ë¶€'\n",
    "    ]\n",
    "    scaler = StandardScaler()\n",
    "    train[cols_to_scale] = scaler.fit_transform(train[cols_to_scale])\n",
    "    test[cols_to_scale] = scaler.transform(test[cols_to_scale])\n",
    "\n",
    "    return train, test\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def all_process(train, test):\n",
    "    \n",
    "    train, test = drop_columns(train), drop_columns(test)\n",
    "\n",
    "    train, test = íŠ¹ì •ì‹œìˆ ìœ í˜•(train, test)\n",
    "\n",
    "    train, test = ì‹œìˆ íšŸìˆ˜(train), ì‹œìˆ íšŸìˆ˜(test)\n",
    "    \n",
    "    # ì¹´í…Œê³ ë¦¬ ì»¬ëŸ¼ì„ ì¹´í…Œê³ ë¦¬ íƒ€ì…ìœ¼ë¡œ ë°”ê¾¸ê¸°\n",
    "    train, test = encoding(train, test)\n",
    "\n",
    "    # ê²°ì¸¡ê°’ 0ìœ¼ë¡œ ì±„ìš°ê¸°ê¸°\n",
    "    train, test = numeric_process(train, test, value='zero')\n",
    "\n",
    "    # ë¡œê·¸ë³€í™˜ í›„ ì •ê·œí™”í™”\n",
    "    train, test = num_feature_scailing(train, test)\n",
    "\n",
    "    return train, test\n",
    "\n",
    "train = pd.read_csv(train_path).drop(columns=['ID'])\n",
    "# test = pd.read_csv(test_path).drop(columns=['ID'])\n",
    "\n",
    "####### í…ŒìŠ¤íŠ¸ë¡œ ì ì€ ìˆ˜ì˜ ë°ì´í„°ë¡œ##################\n",
    "train = train.sample(n=1000, random_state=42)\n",
    "###################################################\n",
    "\n",
    "# train ë°ì´í„°ì˜ label ì»¬ëŸ¼ì„ ê¸°ì¤€ìœ¼ë¡œ stratify ì„¤ì •\n",
    "train, test = train_test_split(train, test_size=0.2, stratify=train['ì„ì‹  ì„±ê³µ ì—¬ë¶€'], random_state=42)\n",
    "\n",
    "\n",
    "# train, test = all_process(train, test)\n",
    "\n",
    "print(train.shape, test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ì‹¤í—˜ ë‚´ìš©\n",
    "experiment_desc = '''\n",
    "WideDeep_TabMlp ì„ì‹  ì„±ê³µ ì—¬ë¶€ ì—°ì†í˜• ë³€ìˆ˜ ì„ë² ë”©\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import time\n",
    "start_time = time.time()\n",
    "\n",
    "\n",
    "# import scipy.special   # ì‹œê·¸ëª¨ì´ë“œ ëŒ€ì‹  softmaxì— ì‚¬ìš©í•  ìˆ˜ ìˆìŒ\n",
    "# êµì°¨ ê²€ì¦ ì„¤ì •: seed_listë¥¼ [333] í•˜ë‚˜ë§Œ ì‚¬ìš©, n_splits=3\n",
    "seed_list = [333]\n",
    "n_splits = 5\n",
    "\n",
    "total_auc, total_acc, total_f1 = [], [], []\n",
    "test_preds = []\n",
    "\n",
    "# --- DataConfig ì„¤ì • ---\n",
    "# ë²”ì£¼í˜• ë³€ìˆ˜ ì§€ì • (ì‹¤ì œ ë°ì´í„°ì— ë§ê²Œ ìˆ˜ì •)\n",
    "categorical_cols = [\n",
    "        \"ì‹œìˆ  ì‹œê¸° ì½”ë“œ\",\n",
    "        \"ì‹œìˆ  ë‹¹ì‹œ ë‚˜ì´\",\n",
    "        \"ë°°ë€ ìœ ë„ ìœ í˜•\",\n",
    "        \"ë°°ì•„ ìƒì„± ì£¼ìš” ì´ìœ \",\n",
    "\n",
    "        ## ì‹œìˆ  íšŸìˆ˜\n",
    "        \"í´ë¦¬ë‹‰ ë‚´ ì´ ì‹œìˆ  íšŸìˆ˜\",\n",
    "        \"IVF ì‹œìˆ  íšŸìˆ˜\",\n",
    "        \"DI ì‹œìˆ  íšŸìˆ˜\",\n",
    "\n",
    "        ## ì„ì‹  íšŸìˆ˜\n",
    "        \"ì´ ì„ì‹  íšŸìˆ˜\",\n",
    "        \"IVF ì„ì‹  íšŸìˆ˜\",\n",
    "        \"DI ì„ì‹  íšŸìˆ˜\",\n",
    "\n",
    "        ## ì¶œì‚° íšŸìˆ˜\n",
    "        \"ì´ ì¶œì‚° íšŸìˆ˜\",\n",
    "        \"IVF ì¶œì‚° íšŸìˆ˜\",\n",
    "        \"DI ì¶œì‚° íšŸìˆ˜\",\n",
    "\n",
    "        \"ë‚œì ì¶œì²˜\",\n",
    "        \"ì •ì ì¶œì²˜\",\n",
    "        \"ë‚œì ê¸°ì¦ì ë‚˜ì´\",\n",
    "        \"ì •ì ê¸°ì¦ì ë‚˜ì´\",\n",
    "\n",
    "        'ì‹œìˆ ìœ í˜•_í†µí•©', # íŠ¹ì •ì‹œìˆ ìœ í˜•()\n",
    "    ]  \n",
    "\n",
    "# íƒ€ê²Ÿê³¼ ë²”ì£¼í˜• ë³€ìˆ˜ ì™¸ ë‚˜ë¨¸ì§€ë¥¼ ì—°ì†í˜• ë³€ìˆ˜ë¡œ ì²˜ë¦¬ (ì‹¤ì œ ì»¬ëŸ¼ëª…ì— ë§ê²Œ ìˆ˜ì •)\n",
    "continuous_cols = [\n",
    "            'ì„ì‹  ì‹œë„ ë˜ëŠ” ë§ˆì§€ë§‰ ì„ì‹  ê²½ê³¼ ì—°ìˆ˜',\n",
    "            'ë°°ë€ ìê·¹ ì—¬ë¶€',\n",
    "            'ë‹¨ì¼ ë°°ì•„ ì´ì‹ ì—¬ë¶€',\n",
    "            'ì°©ìƒ ì „ ìœ ì „ ê²€ì‚¬ ì‚¬ìš© ì—¬ë¶€',\n",
    "            'ì°©ìƒ ì „ ìœ ì „ ì§„ë‹¨ ì‚¬ìš© ì—¬ë¶€',\n",
    "            'ì´ ìƒì„± ë°°ì•„ ìˆ˜',\n",
    "            'ë¯¸ì„¸ì£¼ì…ëœ ë‚œì ìˆ˜',\n",
    "            'ë¯¸ì„¸ì£¼ì…ì—ì„œ ìƒì„±ëœ ë°°ì•„ ìˆ˜',\n",
    "            'ì´ì‹ëœ ë°°ì•„ ìˆ˜',\n",
    "            'ë¯¸ì„¸ì£¼ì… ë°°ì•„ ì´ì‹ ìˆ˜',\n",
    "            'ì €ì¥ëœ ë°°ì•„ ìˆ˜',\n",
    "            'ë¯¸ì„¸ì£¼ì… í›„ ì €ì¥ëœ ë°°ì•„ ìˆ˜',\n",
    "            'í•´ë™ëœ ë°°ì•„ ìˆ˜',\n",
    "            'í•´ë™ ë‚œì ìˆ˜',\n",
    "            'ìˆ˜ì§‘ëœ ì‹ ì„  ë‚œì ìˆ˜',\n",
    "            'ì €ì¥ëœ ì‹ ì„  ë‚œì ìˆ˜',\n",
    "            'í˜¼í•©ëœ ë‚œì ìˆ˜',\n",
    "            'íŒŒíŠ¸ë„ˆ ì •ìì™€ í˜¼í•©ëœ ë‚œì ìˆ˜',\n",
    "            'ê¸°ì¦ì ì •ìì™€ í˜¼í•©ëœ ë‚œì ìˆ˜',\n",
    "            'ë™ê²° ë°°ì•„ ì‚¬ìš© ì—¬ë¶€',\n",
    "            'ì‹ ì„  ë°°ì•„ ì‚¬ìš© ì—¬ë¶€',\n",
    "            'ê¸°ì¦ ë°°ì•„ ì‚¬ìš© ì—¬ë¶€',\n",
    "            'ëŒ€ë¦¬ëª¨ ì—¬ë¶€',\n",
    "            'PGD ì‹œìˆ  ì—¬ë¶€',\n",
    "            'PGS ì‹œìˆ  ì—¬ë¶€',\n",
    "            'ë‚œì ì±„ì·¨ ê²½ê³¼ì¼',\n",
    "            'ë‚œì í˜¼í•© ê²½ê³¼ì¼',\n",
    "            'ë°°ì•„ ì´ì‹ ê²½ê³¼ì¼',\n",
    "            'ë°°ì•„ í•´ë™ ê²½ê³¼ì¼',\n",
    "        ]\n",
    "\n",
    "# Wide ë¶€ë¶„: ì›-í•« ì¸ì½”ë”© (crossed_cols ì˜µì…˜ë„ ì‚¬ìš©í•  ìˆ˜ ìˆìŒ)\n",
    "wide_cols = categorical_cols  # Wide ëª¨ë¸ì—ëŠ” ë²”ì£¼í˜• ë³€ìˆ˜ë¥¼ ì›-í•« ì¸ì½”ë”©ìœ¼ë¡œ ì²˜ë¦¬\n",
    "crossed_cols = []  # í•„ìš”ì‹œ ë‘ ê°œ ì´ìƒì˜ ì»¬ëŸ¼ì„ êµì°¨ì‹œì¼œ ìƒí˜¸ì‘ìš© feature ìƒì„±\n",
    "\n",
    "# WidePreprocessor (ì›-í•« ì¸ì½”ë”©, crossed_cols ì‚¬ìš© ê°€ëŠ¥)\n",
    "wide_preprocessor = WidePreprocessor(wide_cols=wide_cols, crossed_cols=crossed_cols)\n",
    "\n",
    "# Deep ë¶€ë¶„: ì„ë² ë”© + ì—°ì†í˜• ë³€ìˆ˜ ì²˜ë¦¬\n",
    "tab_preprocessor = TabPreprocessor(embed_cols=categorical_cols, continuous_cols=continuous_cols)\n",
    "\n",
    "\n",
    "\n",
    "# êµì°¨ ê²€ì¦ ì‹œì‘\n",
    "for seed in seed_list:\n",
    "    skf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=seed)\n",
    "    auc_scores, acc_scores, f1_scores = [], [], []\n",
    "    \n",
    "    for fold, (train_idx, valid_idx) in enumerate(skf.split(train.drop(columns=['ì„ì‹  ì„±ê³µ ì—¬ë¶€']), train[\"ì„ì‹  ì„±ê³µ ì—¬ë¶€\"])):\n",
    "        # Fold ë°ì´í„° ìƒì„±\n",
    "        fold_train, fold_valid = train.iloc[train_idx], train.iloc[valid_idx]\n",
    "        fold_train2 = fold_train.copy()\n",
    "        fold_test = test.copy()  # test ë°ì´í„°ëŠ” ë³„ë„ ì‚¬ìš©\n",
    "        \n",
    "        # ì „ì²˜ë¦¬ \n",
    "        fold_train, fold_valid = all_process(fold_train, fold_valid)\n",
    "        _, fold_test = all_process(fold_train2, fold_test)\n",
    "\n",
    "\n",
    "        # ì „ì²˜ë¦¬: ê° Fold ë³„ë¡œ Wide & Deep ë°ì´í„° ìƒì„±\n",
    "        X_wide_train = wide_preprocessor.fit_transform(fold_train)\n",
    "        X_wide_valid = wide_preprocessor.transform(fold_valid)\n",
    "        X_wide_test = wide_preprocessor.transform(fold_test)\n",
    "        X_tab_train = tab_preprocessor.fit_transform(fold_train)\n",
    "        X_tab_valid = tab_preprocessor.transform(fold_valid)\n",
    "        X_tab_test = tab_preprocessor.transform(fold_test)\n",
    "\n",
    "        # Target ê°’: ì •ìˆ˜í˜• (0,1)\n",
    "        y_train = fold_train['ì„ì‹  ì„±ê³µ ì—¬ë¶€'].astype(int).values\n",
    "        y_valid = fold_valid['ì„ì‹  ì„±ê³µ ì—¬ë¶€'].astype(int).values\n",
    "\n",
    "        # Wide ëª¨ë¸: ì…ë ¥ ì°¨ì›ì€ ì›-í•« ì¸ì½”ë”©ëœ í”¼ì²˜ ìˆ˜####\n",
    "        wide_model = Wide(input_dim=int(X_wide_train.max().item()), pred_dim=1)\n",
    "\n",
    "\n",
    "        # Deep ëª¨ë¸: TabMlp ì‚¬ìš©\n",
    "        tab_model = TabMlp(\n",
    "            column_idx=tab_preprocessor.column_idx,  # ë°ì´í„° ì»¬ëŸ¼ì˜ ì¸ë±ìŠ¤ ì •ë³´ (í•„ìˆ˜)\n",
    "            cat_embed_input=tab_preprocessor.cat_embed_input,  # (ì»¬ëŸ¼ëª…, ê³ ìœ ê°’ ê°œìˆ˜, ì„ë² ë”© ì°¨ì›) ë¦¬ìŠ¤íŠ¸ (ë²”ì£¼í˜• ë³€ìˆ˜)\n",
    "            continuous_cols=continuous_cols,  # ì—°ì†í˜• ë³€ìˆ˜ ë¦¬ìŠ¤íŠ¸\n",
    "            \n",
    "            # â–¶ ë²”ì£¼í˜• ì„ë² ë”© ê´€ë ¨ íŒŒë¼ë¯¸í„° (Categorical Embeddings)\n",
    "            cat_embed_dropout=0.0,  # ë²”ì£¼í˜• ë³€ìˆ˜ì˜ ì„ë² ë”© ë“œë¡­ì•„ì›ƒ í™•ë¥  (ê¸°ë³¸ê°’: 0.1)\n",
    "            use_cat_bias=False,  # ë²”ì£¼í˜• ì„ë² ë”©ì— bias ì¶”ê°€ ì—¬ë¶€ (ê¸°ë³¸ê°’: False)\n",
    "            cat_embed_activation=None,  # ë²”ì£¼í˜• ì„ë² ë”©ì˜ í™œì„±í™” í•¨ìˆ˜ ('relu', 'tanh', 'leaky_relu', 'gelu' ê°€ëŠ¥, ê¸°ë³¸ê°’ ì—†ìŒ)\n",
    "\n",
    "            # â–¶ ì—°ì†í˜• ë³€ìˆ˜ ê´€ë ¨ íŒŒë¼ë¯¸í„° (Continuous Features)\n",
    "            cont_norm_layer='batchnorm',  # ì—°ì†í˜• ë³€ìˆ˜ ì •ê·œí™” ('batchnorm' ë˜ëŠ” 'layernorm', ê¸°ë³¸ê°’ ì—†ìŒ)\n",
    "            embed_continuous_method='standard', # ì—°ì†í˜• ë³€ìˆ˜ ì„ë² ë”© ë°©ì‹\n",
    "            # í›„ë³´ ê°’: 'standard' (ê¸°ë³¸), 'piecewise' (êµ¬ê°„ë³„ ë³€í™˜), 'periodic' (ì£¼ê¸°ì  ë³€í™˜)\n",
    "\n",
    "            cont_embed_dim=8,  # ì—°ì†í˜• ë³€ìˆ˜ ì„ë² ë”© ì°¨ì› (Noneì´ë©´ ì‚¬ìš© ì•ˆ í•¨)\n",
    "            # ì¶”ì²œ: 8~16 ì •ë„ë¡œ ì‹œì‘í•˜ê³ , ë°ì´í„°ê°€ ë³µì¡í• ìˆ˜ë¡ 32 ì´ìƒìœ¼ë¡œ ì¦ê°€ ê°€ëŠ¥.\n",
    "\n",
    "            cont_embed_dropout=0.0,  # ì—°ì†í˜• ë³€ìˆ˜ ì„ë² ë”© ë“œë¡­ì•„ì›ƒ í™•ë¥  (ê¸°ë³¸ê°’: 0.0)\n",
    "            # 0.0 â†’ ë“œë¡­ì•„ì›ƒ ì‚¬ìš© ì•ˆ í•¨ (ë°ì´í„°ê°€ ë§ê±°ë‚˜ ê³¼ì í•© ê±±ì •ì´ ì—†ì„ ë•Œ)\n",
    "            # 0.1~0.3 â†’ ì ì ˆí•œ ì •ê·œí™” íš¨ê³¼ (ì¼ë°˜ì ì¸ ê²½ìš°)\n",
    "            # 0.4 ì´ìƒ â†’ ë°ì´í„°ê°€ ë§¤ìš° ì ê³  ë…¸ì´ì¦ˆê°€ ì‹¬í•  ë•Œ\n",
    "\n",
    "            cont_embed_activation='relu',  # ì—°ì†í˜• ë³€ìˆ˜ ì„ë² ë”© í™œì„±í™” í•¨ìˆ˜ (ê¸°ë³¸ê°’ ì—†ìŒ)\n",
    "            # ('relu', 'tanh', 'leaky_relu', 'gelu' ê°€ëŠ¥)\n",
    "\n",
    "            # â–¶ MLP ê´€ë ¨ íŒŒë¼ë¯¸í„° (Multi-Layer Perceptron)\n",
    "            mlp_hidden_dims=[200, 100],  # MLPì˜ ì€ë‹‰ì¸µ í¬ê¸° (ê¸°ë³¸ê°’: [200, 100])\n",
    "            mlp_activation=\"relu\",  # MLPì˜ í™œì„±í™” í•¨ìˆ˜ ('relu', 'tanh', 'leaky_relu', 'gelu' ê°€ëŠ¥, ê¸°ë³¸ê°’: 'relu')\n",
    "            mlp_dropout=0.1,  # MLP ì€ë‹‰ì¸µ ë“œë¡­ì•„ì›ƒ í™•ë¥  (ê¸°ë³¸ê°’: 0.1)\n",
    "            mlp_batchnorm=True,  # MLPì˜ ë°°ì¹˜ ì •ê·œí™” ì—¬ë¶€ (ê¸°ë³¸ê°’: True)\n",
    "            mlp_batchnorm_last=False,  # MLP ë§ˆì§€ë§‰ ì¸µì—ë„ ë°°ì¹˜ ì •ê·œí™” ì ìš© ì—¬ë¶€ (ê¸°ë³¸ê°’: False)\n",
    "            mlp_linear_first=True,  # ì„ í˜• ë³€í™˜ì„ ë¨¼ì € ì ìš©í• ì§€ ì—¬ë¶€ (ê¸°ë³¸ê°’: True)\n",
    "        )\n",
    "        \n",
    "        \n",
    "        # Wide & Deep ëª¨ë¸ ê²°í•©\n",
    "        model = WideDeep(wide=wide_model, deeptabular=tab_model)\n",
    "        \n",
    "        # ì˜µí‹°ë§ˆì´ì € ë° í•™ìŠµë¥  ìŠ¤ì¼€ì¤„ëŸ¬ ì„¤ì •\n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr=0.0001)  # Set your desired learning rate here\n",
    "        \n",
    "\n",
    "        # âœ… Training ë°ì´í„°ë¥¼ ë”•ì…”ë„ˆë¦¬ í˜•íƒœë¡œ ìƒì„±\n",
    "        X_train = {\n",
    "            \"X_wide\": X_wide_train,\n",
    "            \"X_tab\": X_tab_train,\n",
    "            \"target\": y_train\n",
    "        }\n",
    "\n",
    "        # âœ… Validation ë°ì´í„°ë¥¼ ë”•ì…”ë„ˆë¦¬ë¡œ ì „ë‹¬ (X_val ë°©ì‹ ì‚¬ìš©)\n",
    "        X_val = {\n",
    "            \"X_wide\": X_wide_valid,\n",
    "            \"X_tab\": X_tab_valid,\n",
    "            \"target\": y_valid\n",
    "        }\n",
    "        \n",
    "        # EarlyStopping ì½œë°± ì„¤ì • (patience=5, min_delta=0.001)\n",
    "        early_stopping = EarlyStopping(\n",
    "            monitor=\"val_loss\", \n",
    "            patience=5, \n",
    "            min_delta=0.001, \n",
    "            verbose=1\n",
    "        )\n",
    "        \n",
    "        # í˜„ì¬ ë‚ ì§œ/ì‹œê°„ì„ í¬í•¨í•œ íŒŒì¼ ì´ë¦„ ìƒì„±\n",
    "        timestamp = datetime.datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "        model_path = f\"saved_models/best_model_{timestamp}.pt\"\n",
    "        \n",
    "        # âœ… 2. ModelCheckpoint: ìµœìƒì˜ ëª¨ë¸ì„ ìë™ ì €ì¥\n",
    "        model_checkpoint = ModelCheckpoint(\n",
    "            filepath=model_path,  # ëª¨ë¸ ì €ì¥ ê²½ë¡œ\n",
    "            monitor=\"val_loss\",        # ê°ì‹œí•  ì§€í‘œ ('val_loss' ë˜ëŠ” 'val_acc')\n",
    "            save_best_only=True        # ê°€ì¥ ì¢‹ì€ ì„±ëŠ¥ì˜ ëª¨ë¸ë§Œ ì €ì¥\n",
    "        )\n",
    "        \n",
    "        # Trainer ìƒì„±: objective \"binary\"ë¡œ ì„¤ì •, í‰ê°€ ì§€í‘œë¡œ Accuracy, AUROC, F1Score ì‚¬ìš©\n",
    "        trainer = Trainer(\n",
    "            model=model, \n",
    "            objective=\"binary\", \n",
    "            custom_loss_function=None,  # ê¸°ë³¸ ì†ì‹¤ í•¨ìˆ˜ ì‚¬ìš© (í•„ìš” ì‹œ ì‚¬ìš©ì ì •ì˜ í•¨ìˆ˜ ê°€ëŠ¥)\n",
    "            optimizers=optimizer,       # ì˜µí‹°ë§ˆì´ì € (ê¸°ë³¸ê°’: Adam)\n",
    "            initializers=None,          # ê°€ì¤‘ì¹˜ ì´ˆê¸°í™” ì—†ìŒ (ê¸°ë³¸ê°’)\n",
    "            callbacks=[early_stopping],  # ì¡°ê¸° ì¢…ë£Œ (3 ì—í¬í¬ ë™ì•ˆ ê°œì„  ì—†ìœ¼ë©´ ì¢…ë£Œ)\n",
    "            metrics=[Accuracy()],\n",
    "            verbose=1,                  # í•™ìŠµ ë¡œê·¸ ì¶œë ¥ (ê¸°ë³¸ê°’: 1)\n",
    "            seed=42                     # ëœë¤ ì‹œë“œ ì„¤ì • (ê¸°ë³¸ê°’: 1)\n",
    "        )\n",
    "        \n",
    "        # í•™ìŠµ: validation ë°ì´í„°ëŠ” ë³„ë„ì˜ ì¸ìë¡œ ì „ë‹¬\n",
    "        trainer.fit(\n",
    "            X_train=X_train,  # âœ… Training ë°ì´í„°ë¥¼ ë”•ì…”ë„ˆë¦¬ë¡œ \n",
    "            n_epochs=100, \n",
    "            batch_size=128, \n",
    "            X_val=X_val  # âœ… Validation ë°ì´í„°ë¥¼ ë”•ì…”ë„ˆë¦¬ë¡œ ì „ë‹¬\n",
    "        )\n",
    "        \n",
    "        \n",
    "        \n",
    "        # ëª¨ë¸ í•™ìŠµ í›„ Validation ì˜ˆì¸¡ ì½”ë“œ:\n",
    "        # ì´ë¯¸ í™•ë¥ ì´ ê³„ì‚°ë˜ì–´ ìˆëŠ” ì»¬ëŸ¼ì„ ì‚¬ìš©í•©ë‹ˆë‹¤.\n",
    "        valid_probs = trainer.predict_proba(X_wide=X_wide_valid, X_tab=X_tab_valid)[:,1] ####\n",
    "        valid_pred = (valid_probs > 0.5).astype(int)\n",
    "\n",
    "        \n",
    "\n",
    "        # ì‹¤ì œ ì •ë‹µ \n",
    "        y_valid = fold_valid['ì„ì‹  ì„±ê³µ ì—¬ë¶€'].values.astype(int)\n",
    "\n",
    "\n",
    "        \n",
    "        # í‰ê°€ ì§€í‘œ ê³„ì‚°: í´ë˜ìŠ¤ 1ì˜ í™•ë¥  ì‚¬ìš©\n",
    "        fold_auc = roc_auc_score(y_valid, valid_probs)\n",
    "        fold_acc = accuracy_score(y_valid, valid_pred)\n",
    "        fold_f1 = f1_score(y_valid, valid_pred)\n",
    "        print(f\"Seed[{seed:<3}] Fold {fold + 1} | AUC: {fold_auc:.6f} | Acc: {fold_acc:.6f} | F1: {fold_f1:.6f}\")\n",
    "        \n",
    "        auc_scores.append(fold_auc)\n",
    "        acc_scores.append(fold_acc)\n",
    "        f1_scores.append(fold_f1)\n",
    "        \n",
    "        total_auc.append(fold_auc)\n",
    "        total_acc.append(fold_acc)\n",
    "        total_f1.append(fold_f1)\n",
    "        \n",
    "        # Test ë°ì´í„° ì˜ˆì¸¡ (ê° foldì˜ ëª¨ë¸ë¡œ ì˜ˆì¸¡í•œ ê²°ê³¼ ì €ì¥)\n",
    "        test_pred = trainer.predict_proba(X_wide=X_wide_test, X_tab=X_tab_test)[:,1]\n",
    "        test_preds.append(test_pred)\n",
    "    \n",
    "    # Fold ë³„ í‰ê·  ì„±ëŠ¥ ì¶œë ¥\n",
    "    avg_auc = np.mean(auc_scores)\n",
    "    avg_acc = np.mean(acc_scores)\n",
    "    avg_f1 = np.mean(f1_scores)\n",
    "    \n",
    "    print(\"-\" * 80)\n",
    "    print(f\"Seed[{seed:<3}] Average Metrics | AUC: {avg_auc:.7f} | Acc: {avg_acc:.7f} | F1: {avg_f1:.7f}\")\n",
    "    print(\"-\" * 80)\n",
    "\n",
    "# ì „ì²´ Validation í‰ê·  ì„±ëŠ¥ ì¶œë ¥\n",
    "val_auc = np.mean(total_auc)\n",
    "val_acc = np.mean(total_acc)\n",
    "val_f1 = np.mean(total_f1)\n",
    "print(\"-\" * 80)\n",
    "print(f\"Validation Average Metrics | AUC: {val_auc:.7f} | Acc: {val_acc:.7f} | F1: {val_f1:.7f}\")\n",
    "\n",
    "finish_time = time.time()\n",
    "total_time = finish_time - start_time \n",
    "\n",
    "print(total_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# stratify_label = train[\"ì„ì‹  ì„±ê³µ ì—¬ë¶€\"].astype(str)\n",
    "old_auc = 0.7289820 * 100\n",
    "old_acc = 0.7413351 * 100\n",
    "old_f1 = 0.2031272 * 100\n",
    "\n",
    "new_auc = val_auc * 100\n",
    "new_acc = val_acc * 100\n",
    "new_f1 = val_f1 * 100\n",
    "\n",
    "def calculate_change(old_value, new_value):\n",
    "    change = new_value - old_value\n",
    "    percentage_change = (change / old_value) * 100 if old_value != 0 else float('inf')\n",
    "    return change, percentage_change\n",
    "\n",
    "def format_change(change):\n",
    "    return f\"{change:+.6f}\"\n",
    "\n",
    "# ê° ì§€í‘œì˜ ë³€í™”ëŸ‰ ê³„ì‚°\n",
    "auc_change, auc_pct = calculate_change(old_auc, new_auc)\n",
    "acc_change, acc_pct = calculate_change(old_acc, new_acc)\n",
    "f1_change, f1_pct = calculate_change(old_f1, new_f1)\n",
    "\n",
    "# ê²°ê³¼ ì¶œë ¥\n",
    "print(\"\\n================= ëª¨ë¸ ì„±ëŠ¥ ë³€í™” =================\")\n",
    "print(f\"{'Metric':<8}  {'AUC':>12}  {'Acc':>12}  {'F1':>12}\")\n",
    "print(\"-\" * 50)\n",
    "print(f\"{'Old':<8}  {old_auc:>12.6f}  {old_acc:>12.6f}  {old_f1:>12.6f}\")\n",
    "print(f\"{'New':<8}  {new_auc:>12.6f}  {new_acc:>12.6f}  {new_f1:>12.6f}\")\n",
    "print(f\"{'Change':<8}  {format_change(auc_change):>12}  {format_change(acc_change):>12}  {format_change(f1_change):>12}\")\n",
    "print(f\"{'% Change':<8}  {auc_pct:>11.4f}%  {acc_pct:>11.4f}%  {f1_pct:>11.4f}%\")\n",
    "print(\"=\" * 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# í…ŒìŠ¤íŠ¸ ë°ì´í„° í™œìš© ì§€í‘œ ê³„ì‚°\n",
    "# í‰ê°€ ì§€í‘œ ê³„ì‚°: í´ë˜ìŠ¤ 1ì˜ í™•ë¥  ì‚¬ìš©\n",
    "\n",
    "test_auc = roc_auc_score(test['ì„ì‹  ì„±ê³µ ì—¬ë¶€'], np.mean(test_preds, axis=0))\n",
    "print(f\"test ë°ì´í„° AUC: {test_auc:.6f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- ê¸°ë³¸ test ë°ì´í„° AUC : 0.732737"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_submission = pd.read_csv(sample_path)\n",
    "# test_preds\n",
    "# sample_submission['ì„ì‹  ì„±ê³µ í™•ë¥ '] = np.mean(test_preds, axis=0)\n",
    "\n",
    "# ratio = train['ì„ì‹  ì„±ê³µ ì—¬ë¶€'].value_counts(normalize=True)[1]\n",
    "# real_true_count = int(ratio * len(sample_submission))\n",
    "# print(f'testì˜ True ê°¯ìˆ˜: {real_true_count:<5} (ì¶”ì •)')\n",
    "\n",
    "# count = (sample_submission['ì„ì‹  ì„±ê³µ í™•ë¥ '] >= 0.5).sum()\n",
    "# print(f'testì˜ True ê°¯ìˆ˜: {count:<5} (ì˜ˆì¸¡ ê²°ê³¼)')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "now = datetime.datetime.now().strftime(\"%m%d_%H%M\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission_path = 'Submission'\n",
    "if not os.path.exists(submission_path):\n",
    "    os.makedirs(submission_path)\n",
    "\n",
    "code_dir = 'Code'\n",
    "if not os.path.exists(code_dir):\n",
    "    os.makedirs(code_dir)\n",
    "\n",
    "\n",
    "submission_name = f\"submission_{now}.csv\"\n",
    "new_notebook_name = f\"code_{now}.ipynb\"\n",
    "\n",
    "sample_submission.to_csv(os.path.join(submission_path, submission_name), index=False)\n",
    "\n",
    "\n",
    "# í˜„ì¬ ë…¸íŠ¸ë¶ íŒŒì¼ ê²½ë¡œ ì§ì ‘ ì§€ì • (ì‹¤ì œ ë…¸íŠ¸ë¶ íŒŒì¼ëª…ìœ¼ë¡œ ìˆ˜ì •)\n",
    "current_notebook = os.path.join(os.getcwd(), \"WideDeep_TabMlp_ì„ì‹  ì—¬ë¶€.ipynb\")\n",
    "\n",
    "new_notebook_path = os.path.join(code_dir, new_notebook_name)\n",
    "\n",
    "# ë…¸íŠ¸ë¶ íŒŒì¼ ë³µì‚¬\n",
    "shutil.copy(current_notebook, new_notebook_path)\n",
    "\n",
    "print(f\"Notebook saved in '{code_dir}' as '{new_notebook_name}'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ğŸ“Œ SQLite ë°ì´í„°ë² ì´ìŠ¤ ì„¤ì •\n",
    "db_path = \"experiment_results.db\"\n",
    "conn = sqlite3.connect(db_path)\n",
    "cursor = conn.cursor()\n",
    "\n",
    "# ğŸ“Œ í…Œì´ë¸” ìƒì„± (ì²˜ìŒ ì‹¤í–‰ ì‹œ)\n",
    "cursor.execute('''\n",
    "CREATE TABLE IF NOT EXISTS experiments (\n",
    "    id INTEGER PRIMARY KEY AUTOINCREMENT,\n",
    "    code_name TEXT,\n",
    "    experiment_desc TEXT,\n",
    "    auc REAL,\n",
    "    acc REAL,\n",
    "    f1 REAL\n",
    ")\n",
    "''')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ë°ì´í„° ì‚½ì…\n",
    "cursor.execute('''\n",
    "INSERT INTO experiments (code_name, experiment_desc, auc, acc, f1)\n",
    "VALUES (?, ?, ?, ?, ?)\n",
    "''', (new_notebook_name, experiment_desc.strip(), new_auc, new_acc, new_f1))\n",
    "\n",
    "# ë³€ê²½ì‚¬í•­ ì €ì¥ & ì—°ê²° ì¢…ë£Œ\n",
    "conn.commit()\n",
    "conn.close()\n",
    "\n",
    "print(f\"Experiment '{new_notebook_name}' successfully saved in database\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sqlite3\n",
    "import pandas as pd\n",
    "\n",
    "# SQLite ë°ì´í„° ì¡°íšŒ í•¨ìˆ˜\n",
    "def get_experiment_results(db_path=\"experiment_results.db\", num_results=10):\n",
    "    \"\"\"\n",
    "    SQLite ë°ì´í„°ë² ì´ìŠ¤ì—ì„œ ì¤‘ë³µëœ ì‹¤í—˜ ë°ì´í„°ë¥¼ ì œê±°í•˜ê³ , ìµœê·¼ num_resultsê°œì˜ ì‹¤í—˜ ë°ì´í„°ë¥¼ ë¶ˆëŸ¬ì˜¤ëŠ” í•¨ìˆ˜.\n",
    "    Returns:\n",
    "        - Pandas DataFrame: ì¤‘ë³µ ì œê±°ëœ ì‹¤í—˜ ë°ì´í„°\n",
    "    \"\"\"\n",
    "    conn = sqlite3.connect(db_path)\n",
    "\n",
    "    # ì¤‘ë³µ ì œê±° & ìµœì‹  ë°ì´í„° ì„ íƒí•˜ëŠ” SQL ì¿¼ë¦¬\n",
    "    query = f\"\"\"\n",
    "    SELECT * FROM experiments\n",
    "    WHERE id IN (\n",
    "        SELECT MAX(id)  -- ê°€ì¥ ìµœì‹  ë°ì´í„° ì„ íƒ\n",
    "        FROM experiments\n",
    "        GROUP BY code_name -- id ì œì™¸í•˜ê³  ì¤‘ë³µ íŒë‹¨\n",
    "    )\n",
    "    ORDER BY id DESC  -- ìµœì‹  ë°ì´í„°ë¶€í„° ì •ë ¬\n",
    "    LIMIT {num_results};\n",
    "    \"\"\"\n",
    "\n",
    "    df = pd.read_sql_query(query, conn)\n",
    "    conn.close()\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_results = get_experiment_results(num_results=100)\n",
    "df_results.to_csv('experiment_results.csv', index=False, encoding='utf-8-sig', float_format='%.4f')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lg2025",
   "language": "python",
   "name": "lg2025"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
