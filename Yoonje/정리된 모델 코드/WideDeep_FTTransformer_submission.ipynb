{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# WideDeep_FTTransformer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 라이브러리 설치"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip -q install torch==2.5.1 torchvision==0.20.1 torchaudio==2.5.1 --index-url https://download.pytorch.org/whl/cu124"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install pytorch-widedeep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install opencv-python-headless -> 저는 이거 없으면 엘리스에서 에러 나서 깔았습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sqlite3\n",
    "import shutil\n",
    "import datetime\n",
    "import os\n",
    "import warnings\n",
    "import random\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "\n",
    "\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.preprocessing import LabelEncoder, FunctionTransformer, QuantileTransformer, MultiLabelBinarizer\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "\n",
    "\n",
    "import torch\n",
    "\n",
    "# pytorch-widedeep 라이브러리 import\n",
    "from pytorch_widedeep import Trainer\n",
    "from pytorch_widedeep.preprocessing import WidePreprocessor, TabPreprocessor\n",
    "from pytorch_widedeep.models import Wide, FTTransformer, WideDeep\n",
    "from pytorch_widedeep.metrics import Accuracy,F1Score\n",
    "from pytorch_widedeep.callbacks import EarlyStopping, ModelCheckpoint\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(205080, 68) (51271, 67)\n"
     ]
    }
   ],
   "source": [
    "train_path = f'../data/train.csv'\n",
    "test_path = f'../data/test.csv'\n",
    "sample_path = f'../data/sample_submission.csv'\n",
    "\n",
    "\n",
    "train = pd.read_csv(train_path).drop(columns=[\"ID\"])\n",
    "test = pd.read_csv(test_path).drop(columns=[\"ID\"])\n",
    "print(train.shape, test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def drop_columns(df):\n",
    "    cols = [\n",
    "        '불임 원인 - 여성 요인',  # 고유값 1\n",
    "        '불임 원인 - 정자 면역학적 요인',  # train, test 모두 '1'인 데이터 1개 >> 신뢰할 수 없음\n",
    "        '난자 해동 경과일',\n",
    "    ]\n",
    "    df = df.drop(cols, axis=1)\n",
    "    return df\n",
    "\n",
    "def 특정시술유형(train, test):\n",
    "    def categorize_procedure(proc):\n",
    "        tokens = [token.strip() for token in proc.split(\",\") if token.strip() and not token.strip().isdigit()]\n",
    "        # 우선순위에 따른 범주화\n",
    "        if tokens.count(\"Unknown\") >= 1:\n",
    "            return \"Unknown\"\n",
    "        if tokens.count(\"AH\") >= 1:\n",
    "            return \"AH\"\n",
    "        if tokens.count(\"BLASTOCYST\") >= 1:\n",
    "            return \"BLASTOCYST\"\n",
    "        if tokens.count(\"ICSI\") >= 2 or tokens.count(\"IVF\") >= 2:\n",
    "            return \"2ICSI_2IVF\"\n",
    "        if tokens.count(\"IVF\") >= 1 and tokens.count(\"ICSI\") >= 1:\n",
    "            return \"IVF_ICSI\"\n",
    "        if tokens == \"ICSI\":\n",
    "            return \"ICSI\"\n",
    "        if tokens == \"IVF\":\n",
    "            return \"IVF\"\n",
    "        return \",\".join(tokens) if tokens else None\n",
    "\n",
    "    for df in [train, test]:\n",
    "        df['특정 시술 유형'] = df['특정 시술 유형'].str.replace(\" / \", \",\")\n",
    "        df['특정 시술 유형'] = df['특정 시술 유형'].str.replace(\":\", \",\")\n",
    "        df['특정 시술 유형'] = df['특정 시술 유형'].str.replace(\" \", \"\")\n",
    "\n",
    "    counts = train['특정 시술 유형'].value_counts()\n",
    "    allowed_categories = counts[counts >= 100].index.tolist()\n",
    "\n",
    "    # allowed_categories에 속하지 않는 값은 \"Unknown\"으로 대체\n",
    "    train.loc[~train['특정 시술 유형'].isin(allowed_categories), '특정 시술 유형'] = \"Unknown\"\n",
    "    test.loc[~test['특정 시술 유형'].isin(allowed_categories), '특정 시술 유형'] = \"Unknown\"\n",
    "\n",
    "    train['특정 시술 유형'] = train['특정 시술 유형'].apply(categorize_procedure)\n",
    "    test['특정 시술 유형'] = test['특정 시술 유형'].apply(categorize_procedure)\n",
    "\n",
    "    train['시술유형_통합'] = train['시술 유형'].astype(str) + '_' + train['특정 시술 유형'].astype(str)\n",
    "    test['시술유형_통합'] = test['시술 유형'].astype(str) + '_' + test['특정 시술 유형'].astype(str)\n",
    "\n",
    "    drop_cols = ['시술 유형', '특정 시술 유형']\n",
    "    train = train.drop(drop_cols, axis=1)\n",
    "    test = test.drop(drop_cols, axis=1)\n",
    "\n",
    "    return train, test\n",
    "\n",
    "def 시술횟수(df_train):\n",
    "    for col in [col for col in df_train.columns if '횟수' in col]:\n",
    "        df_train[col] = df_train[col].replace({'6회 이상':'6회'})\n",
    "        df_train[col] = df_train[col].str[0].astype(int)\n",
    "    df_train['시술_임신'] = df_train['총 임신 횟수'] - df_train['총 시술 횟수']\n",
    "    df_train = df_train.drop('총 시술 횟수', axis=1)\n",
    "    return df_train\n",
    "\n",
    "def 배란유도유형(df_train, df_test):\n",
    "    mapping = {\n",
    "        '기록되지 않은 시행': 1,\n",
    "        '알 수 없음': 0,\n",
    "        '세트로타이드 (억제제)': 0,\n",
    "        '생식선 자극 호르몬': 0,\n",
    "    }\n",
    "    df_train['배란 유도 유형'] = df_train['배란 유도 유형'].replace(mapping)\n",
    "    df_test['배란 유도 유형'] = df_test['배란 유도 유형'].replace(mapping)\n",
    "    return df_train, df_test\n",
    "\n",
    "def 난자기증자나이(df_train, df_test):\n",
    "    mapping = {\n",
    "        '만20세 이하': 20,\n",
    "        '만21-25세': 25,\n",
    "        '만26-30세': 30,\n",
    "        '만31-35세': 35,\n",
    "        '알 수 없음': 20,  # 만20세 이하와 동일하게 처리\n",
    "    }\n",
    "    df_train['난자 기증자 나이'] = df_train['난자 기증자 나이'].replace(mapping)\n",
    "    df_test['난자 기증자 나이'] = df_test['난자 기증자 나이'].replace(mapping)\n",
    "    return df_train, df_test\n",
    "\n",
    "def 배아생성주요이유(df_train, df_test):\n",
    "    df_train['배아 생성 주요 이유'] = df_train['배아 생성 주요 이유'].fillna('DI')\n",
    "    df_test['배아 생성 주요 이유'] = df_test['배아 생성 주요 이유'].fillna('DI')\n",
    "\n",
    "    df_train['배아 생성 이유 리스트'] = df_train['배아 생성 주요 이유'].apply(lambda x: [reason.strip() for reason in x.split(',')])\n",
    "    df_test['배아 생성 이유 리스트'] = df_test['배아 생성 주요 이유'].apply(lambda x: [reason.strip() for reason in x.split(',')])\n",
    "\n",
    "    mlb = MultiLabelBinarizer()\n",
    "    train_one_hot = pd.DataFrame(\n",
    "        mlb.fit_transform(df_train['배아 생성 이유 리스트']),\n",
    "        columns=mlb.classes_,\n",
    "        index=df_train.index\n",
    "    )\n",
    "    train_one_hot.columns = ['배아생성이유_' + col for col in train_one_hot.columns]\n",
    "\n",
    "    test_one_hot = pd.DataFrame(\n",
    "        mlb.transform(df_test['배아 생성 이유 리스트']),\n",
    "        columns=mlb.classes_,\n",
    "        index=df_test.index\n",
    "    )\n",
    "    test_one_hot.columns = ['배아생성이유_' + col for col in test_one_hot.columns]\n",
    "\n",
    "    df_train = pd.concat([df_train, train_one_hot], axis=1)\n",
    "    df_test = pd.concat([df_test, test_one_hot], axis=1)\n",
    "\n",
    "    cols_to_drop = [\n",
    "        '배아 생성 주요 이유',\n",
    "        '배아 생성 이유 리스트',\n",
    "        '배아생성이유_연구용',\n",
    "        '배아생성이유_DI'\n",
    "    ]\n",
    "    df_train = df_train.drop(cols_to_drop, axis=1, errors='ignore')\n",
    "    df_test = df_test.drop(cols_to_drop, axis=1, errors='ignore')\n",
    "\n",
    "    cols = ['배아생성이유_기증용',\n",
    "            '배아생성이유_난자 저장용',\n",
    "            '배아생성이유_배아 저장용',\n",
    "            '배아생성이유_현재 시술용']\n",
    "\n",
    "    df_train[cols] = df_train[cols].div(df_train[cols].sum(axis=1).replace(0, np.nan), axis=0).fillna(0)\n",
    "    df_test[cols] = df_test[cols].div(df_test[cols].sum(axis=1).replace(0, np.nan), axis=0).fillna(0)\n",
    "\n",
    "    return df_train, df_test\n",
    "\n",
    "def 단일배아이식여부(df_train, df_val):\n",
    "    df_train['단일 배아 이식 여부'] = df_train['단일 배아 이식 여부'].fillna(0)\n",
    "    df_val['단일 배아 이식 여부'] = df_val['단일 배아 이식 여부'].fillna(0)\n",
    "    return df_train, df_val\n",
    "\n",
    "\n",
    "def 기증자정자와혼합된난자수(df_train, df_test):\n",
    "    df_train[\"기증자 정자와 혼합된 난자 수\"] = df_train[\"기증자 정자와 혼합된 난자 수\"].fillna(2)\n",
    "    df_test[\"기증자 정자와 혼합된 난자 수\"] = df_test[\"기증자 정자와 혼합된 난자 수\"].fillna(2)\n",
    "    return df_train, df_test\n",
    "\n",
    "def label_encoding(train, test, cols):\n",
    "    encoder = LabelEncoder()\n",
    "    for col in cols:\n",
    "        train[col] = encoder.fit_transform(train[col])\n",
    "        test[col] = encoder.transform(test[col])\n",
    "    return train, test\n",
    "\n",
    "def type_to_category(train, test, cols):\n",
    "    train[cols] = train[cols].astype('category')\n",
    "    test[cols] = test[cols].astype('category')\n",
    "    return train, test\n",
    "\n",
    "def impute_nan(train, test):\n",
    "    cols_to_impute = [\n",
    "        '임신 시도 또는 마지막 임신 경과 연수', # DI, IVF랑 관련 X\n",
    "    ]\n",
    "    imputer = SimpleImputer(strategy='mean')\n",
    "    train[cols_to_impute] = imputer.fit_transform(train[cols_to_impute])\n",
    "    test[cols_to_impute] = imputer.transform(test[cols_to_impute])\n",
    "\n",
    "    cols_to_impute = [\n",
    "        '난자 채취 경과일',\n",
    "        '난자 혼합 경과일',\n",
    "        '배아 이식 경과일',\n",
    "        '배아 해동 경과일',\n",
    "\n",
    "        '착상 전 유전 검사 사용 여부',\n",
    "        'PGD 시술 여부',\n",
    "        'PGS 시술 여부',\n",
    "\n",
    "        ### DI only\n",
    "        '착상 전 유전 진단 사용 여부',\n",
    "        '총 생성 배아 수',\n",
    "        '미세주입된 난자 수',\n",
    "        '미세주입에서 생성된 배아 수',\n",
    "        '이식된 배아 수',\n",
    "        '미세주입 배아 이식 수',\n",
    "        '저장된 배아 수',\n",
    "        '미세주입 후 저장된 배아 수',\n",
    "        '해동된 배아 수',\n",
    "        '해동 난자 수',\n",
    "        '수집된 신선 난자 수',\n",
    "        '저장된 신선 난자 수',\n",
    "        '혼합된 난자 수',\n",
    "        '파트너 정자와 혼합된 난자 수',\n",
    "        '기증자 정자와 혼합된 난자 수',\n",
    "        '동결 배아 사용 여부',\n",
    "        '신선 배아 사용 여부',\n",
    "        '기증 배아 사용 여부',\n",
    "        '대리모 여부',\n",
    "        ### DI\n",
    "    ]\n",
    "    train[cols_to_impute] = train[cols_to_impute].fillna(0)\n",
    "    test[cols_to_impute] = test[cols_to_impute].fillna(0)\n",
    "\n",
    "    return train, test\n",
    "\n",
    "def num_feature_scailing(train, test, seed=777):\n",
    "    numeric_cols = train.select_dtypes(include=[\"number\"]).columns.tolist()\n",
    "    cat_cols = [col for col in train.columns if pd.api.types.is_categorical_dtype(train[col])]\n",
    "    cols_to_scale = [\n",
    "        col for col in numeric_cols\n",
    "        if col not in cat_cols and col != '임신 성공 여부'\n",
    "    ]\n",
    "\n",
    "    arr_train = train[cols_to_scale].to_numpy()  # DataFrame -> NumPy\n",
    "    arr_train = arr_train.astype(np.float32)\n",
    "    arr_test = test[cols_to_scale].to_numpy()\n",
    "    arr_test = arr_test.astype(np.float32)\n",
    "\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "    noise = (\n",
    "        np.random.default_rng(0)\n",
    "        .normal(0.0, 1e-5, arr_train.shape)\n",
    "        .astype(arr_train.dtype)\n",
    "    )\n",
    "    preprocessing = QuantileTransformer(\n",
    "        n_quantiles=max(min(len(train[cols_to_scale]) // 30, 1000), 10),\n",
    "        output_distribution='normal',\n",
    "        subsample=10**9,\n",
    "    ).fit(arr_train + noise)\n",
    "\n",
    "    # train[cols_to_scale] = preprocessing.transform(arr_train + noise)\n",
    "    train[cols_to_scale] = preprocessing.transform(arr_train)\n",
    "    test[cols_to_scale] = preprocessing.transform(arr_test)\n",
    "    return train, test\n",
    "\n",
    "def drop_single_value_columns(df_train, df_test):\n",
    "    cols_to_drop = [col for col in df_train.columns if df_train[col].nunique() == 1]\n",
    "    return df_train.drop(columns=cols_to_drop), df_test.drop(columns=cols_to_drop)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_89/3576064510.py:69: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  df_train['배란 유도 유형'] = df_train['배란 유도 유형'].replace(mapping)\n",
      "/tmp/ipykernel_89/3576064510.py:70: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  df_test['배란 유도 유형'] = df_test['배란 유도 유형'].replace(mapping)\n",
      "/tmp/ipykernel_89/3576064510.py:199: DeprecationWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, pd.CategoricalDtype) instead\n",
      "  cat_cols = [col for col in train.columns if pd.api.types.is_categorical_dtype(train[col])]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "수치형 변수: 57개 \n",
      "['임신 시도 또는 마지막 임신 경과 연수', '배란 자극 여부', '단일 배아 이식 여부', '착상 전 유전 검사 사용 여부', '착상 전 유전 진단 사용 여부', '남성 주 불임 원인', '남성 부 불임 원인', '여성 주 불임 원인', '여성 부 불임 원인', '부부 주 불임 원인', '부부 부 불임 원인', '불명확 불임 원인', '불임 원인 - 난관 질환', '불임 원인 - 남성 요인', '불임 원인 - 배란 장애', '불임 원인 - 자궁경부 문제', '불임 원인 - 자궁내막증', '불임 원인 - 정자 농도', '불임 원인 - 정자 운동성', '불임 원인 - 정자 형태', '클리닉 내 총 시술 횟수', 'IVF 시술 횟수', 'DI 시술 횟수', '총 임신 횟수', 'IVF 임신 횟수', 'DI 임신 횟수', '총 출산 횟수', 'IVF 출산 횟수', 'DI 출산 횟수', '총 생성 배아 수', '미세주입된 난자 수', '미세주입에서 생성된 배아 수', '이식된 배아 수', '미세주입 배아 이식 수', '저장된 배아 수', '미세주입 후 저장된 배아 수', '해동된 배아 수', '해동 난자 수', '수집된 신선 난자 수', '저장된 신선 난자 수', '혼합된 난자 수', '파트너 정자와 혼합된 난자 수', '기증자 정자와 혼합된 난자 수', '동결 배아 사용 여부', '신선 배아 사용 여부', '기증 배아 사용 여부', '대리모 여부', 'PGD 시술 여부', 'PGS 시술 여부', '난자 혼합 경과일', '배아 이식 경과일', '배아 해동 경과일', '시술_임신', '배아생성이유_기증용', '배아생성이유_난자 저장용', '배아생성이유_배아 저장용', '배아생성이유_현재 시술용']\n",
      "범주형 변수: 8개 \n",
      "['시술 시기 코드', '시술 당시 나이', '배란 유도 유형', '난자 출처', '정자 출처', '난자 기증자 나이', '정자 기증자 나이', '시술유형_통합']\n",
      "(205080, 66) (51271, 65)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_89/2392652077.py:45: DeprecationWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, pd.CategoricalDtype) instead\n",
      "  cat_cols = [col for col in train.columns if pd.api.types.is_categorical_dtype(train[col])]\n"
     ]
    }
   ],
   "source": [
    "def all_process(train, val):\n",
    "    # 기본 전처리 단계\n",
    "    train, val = drop_columns(train), drop_columns(val)\n",
    "    train, val = 특정시술유형(train, val)\n",
    "    train, val = 시술횟수(train), 시술횟수(val)\n",
    "\n",
    "    train, val = 단일배아이식여부(train, val)\n",
    "    train, val = 배란유도유형(train, val)\n",
    "    train, val = 배아생성주요이유(train, val)\n",
    "\n",
    "    cols_to_encoding = [\n",
    "        \"시술 시기 코드\",\n",
    "        \"시술 당시 나이\",\n",
    "        \"배란 유도 유형\",\n",
    "        # \"클리닉 내 총 시술 횟수\",\n",
    "        # \"IVF 시술 횟수\",\n",
    "        # \"DI 시술 횟수\",\n",
    "        # \"총 임신 횟수\",\n",
    "        # \"IVF 임신 횟수\",\n",
    "        # \"DI 임신 횟수\",\n",
    "        # \"총 출산 횟수\",\n",
    "        # \"IVF 출산 횟수\",\n",
    "        # \"DI 출산 횟수\",\n",
    "        \"난자 출처\",\n",
    "        \"정자 출처\",\n",
    "        \"난자 기증자 나이\",\n",
    "        \"정자 기증자 나이\",\n",
    "        '시술유형_통합',\n",
    "    ]\n",
    "    train, val = label_encoding(train, val, cols=cols_to_encoding)\n",
    "    train, val = type_to_category(train, val, cols=cols_to_encoding)\n",
    "\n",
    "    train, val = impute_nan(train, val)\n",
    "    train, val = num_feature_scailing(train, val)\n",
    "\n",
    "    train, val = drop_single_value_columns(train, val)\n",
    "\n",
    "    return train, val\n",
    "\n",
    "train = pd.read_csv(train_path).drop(columns=[\"ID\"])\n",
    "test = pd.read_csv(test_path).drop(columns=[\"ID\"])\n",
    "\n",
    "train, test = all_process(train, test)\n",
    "\n",
    "cat_cols = [col for col in train.columns if pd.api.types.is_categorical_dtype(train[col])]\n",
    "numeric_cols = [col for col in train.columns if col not in cat_cols and col != '임신 성공 여부']\n",
    "\n",
    "print(f'수치형 변수: {len(numeric_cols)}개 \\n{numeric_cols}')\n",
    "print(f'범주형 변수: {len(cat_cols)}개 \\n{cat_cols}')\n",
    "print(train.shape, test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_seed(seed: int = 42):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)  # if using multi-GPU\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_89/3576064510.py:69: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  df_train['배란 유도 유형'] = df_train['배란 유도 유형'].replace(mapping)\n",
      "/tmp/ipykernel_89/3576064510.py:70: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  df_test['배란 유도 유형'] = df_test['배란 유도 유형'].replace(mapping)\n",
      "/tmp/ipykernel_89/3576064510.py:199: DeprecationWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, pd.CategoricalDtype) instead\n",
      "  cat_cols = [col for col in train.columns if pd.api.types.is_categorical_dtype(train[col])]\n",
      "/tmp/ipykernel_89/3576064510.py:69: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  df_train['배란 유도 유형'] = df_train['배란 유도 유형'].replace(mapping)\n",
      "/tmp/ipykernel_89/3576064510.py:70: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  df_test['배란 유도 유형'] = df_test['배란 유도 유형'].replace(mapping)\n",
      "/tmp/ipykernel_89/3576064510.py:199: DeprecationWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, pd.CategoricalDtype) instead\n",
      "  cat_cols = [col for col in train.columns if pd.api.types.is_categorical_dtype(train[col])]\n",
      "/tmp/ipykernel_89/3861805098.py:38: DeprecationWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, pd.CategoricalDtype) instead\n",
      "  categorical_cols = [col for col in fold_train.columns if pd.api.types.is_categorical_dtype(fold_train[col])]\n",
      "/home/elicer/.local/lib/python3.10/site-packages/pytorch_widedeep/preprocessing/tab_preprocessor.py:364: UserWarning: Continuous columns will not be normalised\n",
      "  warnings.warn(\"Continuous columns will not be normalised\")\n",
      "epoch 1: 100%|██████████| 161/161 [00:42<00:00,  3.76it/s, loss=0.58, metrics={'acc': 0.709}]  \n",
      "valid: 100%|██████████| 41/41 [00:03<00:00, 11.05it/s, loss=0.511, metrics={'acc': 0.7282}]\n",
      "epoch 2: 100%|██████████| 161/161 [00:38<00:00,  4.23it/s, loss=0.507, metrics={'acc': 0.738}] \n",
      "valid: 100%|██████████| 41/41 [00:03<00:00, 11.24it/s, loss=0.495, metrics={'acc': 0.7405}]\n",
      "epoch 3: 100%|██████████| 161/161 [00:37<00:00,  4.24it/s, loss=0.499, metrics={'acc': 0.7404}]\n",
      "valid: 100%|██████████| 41/41 [00:03<00:00, 10.97it/s, loss=0.493, metrics={'acc': 0.7415}]\n",
      "epoch 4: 100%|██████████| 161/161 [00:37<00:00,  4.25it/s, loss=0.496, metrics={'acc': 0.7419}]\n",
      "valid: 100%|██████████| 41/41 [00:03<00:00, 10.80it/s, loss=0.492, metrics={'acc': 0.741}] \n",
      "epoch 5: 100%|██████████| 161/161 [00:37<00:00,  4.24it/s, loss=0.495, metrics={'acc': 0.7423}]\n",
      "valid: 100%|██████████| 41/41 [00:03<00:00, 11.23it/s, loss=0.491, metrics={'acc': 0.7417}]\n",
      "epoch 6: 100%|██████████| 161/161 [00:38<00:00,  4.22it/s, loss=0.494, metrics={'acc': 0.7422}]\n",
      "valid: 100%|██████████| 41/41 [00:03<00:00, 11.02it/s, loss=0.491, metrics={'acc': 0.7424}]\n",
      "epoch 7: 100%|██████████| 161/161 [00:38<00:00,  4.23it/s, loss=0.493, metrics={'acc': 0.7437}]\n",
      "valid: 100%|██████████| 41/41 [00:03<00:00, 11.04it/s, loss=0.491, metrics={'acc': 0.7426}]\n",
      "epoch 8: 100%|██████████| 161/161 [00:38<00:00,  4.22it/s, loss=0.492, metrics={'acc': 0.7439}]\n",
      "valid: 100%|██████████| 41/41 [00:03<00:00, 11.46it/s, loss=0.491, metrics={'acc': 0.743}] \n",
      "epoch 9: 100%|██████████| 161/161 [00:38<00:00,  4.23it/s, loss=0.492, metrics={'acc': 0.7438}]\n",
      "valid: 100%|██████████| 41/41 [00:03<00:00, 11.52it/s, loss=0.49, metrics={'acc': 0.7425}] \n",
      "epoch 10: 100%|██████████| 161/161 [00:38<00:00,  4.21it/s, loss=0.491, metrics={'acc': 0.7437}]\n",
      "valid: 100%|██████████| 41/41 [00:03<00:00, 11.50it/s, loss=0.49, metrics={'acc': 0.742}]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Epoch: 5. Best val_loss: 0.49143\n",
      "Model weights restored to best epoch: 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "predict: 100%|██████████| 41/41 [00:03<00:00, 12.70it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Seed[333] Fold 1 | AUC: 0.734884\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "predict: 100%|██████████| 51/51 [00:04<00:00, 12.67it/s]\n",
      "/tmp/ipykernel_89/3576064510.py:69: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  df_train['배란 유도 유형'] = df_train['배란 유도 유형'].replace(mapping)\n",
      "/tmp/ipykernel_89/3576064510.py:70: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  df_test['배란 유도 유형'] = df_test['배란 유도 유형'].replace(mapping)\n",
      "/tmp/ipykernel_89/3576064510.py:199: DeprecationWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, pd.CategoricalDtype) instead\n",
      "  cat_cols = [col for col in train.columns if pd.api.types.is_categorical_dtype(train[col])]\n",
      "/tmp/ipykernel_89/3576064510.py:69: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  df_train['배란 유도 유형'] = df_train['배란 유도 유형'].replace(mapping)\n",
      "/tmp/ipykernel_89/3576064510.py:70: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  df_test['배란 유도 유형'] = df_test['배란 유도 유형'].replace(mapping)\n",
      "/tmp/ipykernel_89/3576064510.py:199: DeprecationWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, pd.CategoricalDtype) instead\n",
      "  cat_cols = [col for col in train.columns if pd.api.types.is_categorical_dtype(train[col])]\n",
      "/tmp/ipykernel_89/3861805098.py:38: DeprecationWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, pd.CategoricalDtype) instead\n",
      "  categorical_cols = [col for col in fold_train.columns if pd.api.types.is_categorical_dtype(fold_train[col])]\n",
      "/home/elicer/.local/lib/python3.10/site-packages/pytorch_widedeep/preprocessing/tab_preprocessor.py:364: UserWarning: Continuous columns will not be normalised\n",
      "  warnings.warn(\"Continuous columns will not be normalised\")\n",
      "epoch 1: 100%|██████████| 161/161 [00:38<00:00,  4.22it/s, loss=0.643, metrics={'acc': 0.689}] \n",
      "valid: 100%|██████████| 41/41 [00:03<00:00, 11.55it/s, loss=0.522, metrics={'acc': 0.7223}]\n",
      "epoch 2: 100%|██████████| 161/161 [00:38<00:00,  4.24it/s, loss=0.517, metrics={'acc': 0.7354}]\n",
      "valid: 100%|██████████| 41/41 [00:03<00:00, 12.00it/s, loss=0.503, metrics={'acc': 0.7281}]\n",
      "epoch 3: 100%|██████████| 161/161 [00:37<00:00,  4.24it/s, loss=0.504, metrics={'acc': 0.74}]  \n",
      "valid: 100%|██████████| 41/41 [00:03<00:00, 12.28it/s, loss=0.495, metrics={'acc': 0.7356}]\n",
      "epoch 4: 100%|██████████| 161/161 [00:38<00:00,  4.22it/s, loss=0.5, metrics={'acc': 0.7398}]  \n",
      "valid: 100%|██████████| 41/41 [00:03<00:00, 12.24it/s, loss=0.492, metrics={'acc': 0.7385}]\n",
      "epoch 5: 100%|██████████| 161/161 [00:37<00:00,  4.25it/s, loss=0.498, metrics={'acc': 0.7409}]\n",
      "valid: 100%|██████████| 41/41 [00:03<00:00, 11.75it/s, loss=0.489, metrics={'acc': 0.7413}]\n",
      "epoch 6: 100%|██████████| 161/161 [00:38<00:00,  4.23it/s, loss=0.496, metrics={'acc': 0.7408}]\n",
      "valid: 100%|██████████| 41/41 [00:03<00:00, 11.91it/s, loss=0.488, metrics={'acc': 0.7421}]\n",
      "epoch 7: 100%|██████████| 161/161 [00:37<00:00,  4.25it/s, loss=0.496, metrics={'acc': 0.7419}]\n",
      "valid: 100%|██████████| 41/41 [00:03<00:00, 12.36it/s, loss=0.487, metrics={'acc': 0.7441}]\n",
      "epoch 8: 100%|██████████| 161/161 [00:38<00:00,  4.22it/s, loss=0.495, metrics={'acc': 0.7418}]\n",
      "valid: 100%|██████████| 41/41 [00:03<00:00, 12.68it/s, loss=0.486, metrics={'acc': 0.7437}]\n",
      "epoch 9: 100%|██████████| 161/161 [00:38<00:00,  4.21it/s, loss=0.494, metrics={'acc': 0.7436}]\n",
      "valid: 100%|██████████| 41/41 [00:03<00:00, 12.47it/s, loss=0.486, metrics={'acc': 0.7434}]\n",
      "epoch 10: 100%|██████████| 161/161 [00:38<00:00,  4.21it/s, loss=0.494, metrics={'acc': 0.7427}]\n",
      "valid: 100%|██████████| 41/41 [00:03<00:00, 11.91it/s, loss=0.485, metrics={'acc': 0.7451}]\n",
      "epoch 11: 100%|██████████| 161/161 [00:37<00:00,  4.25it/s, loss=0.494, metrics={'acc': 0.7428}]\n",
      "valid: 100%|██████████| 41/41 [00:03<00:00, 11.80it/s, loss=0.485, metrics={'acc': 0.7442}]\n",
      "epoch 12: 100%|██████████| 161/161 [00:38<00:00,  4.23it/s, loss=0.493, metrics={'acc': 0.7429}]\n",
      "valid: 100%|██████████| 41/41 [00:03<00:00, 12.13it/s, loss=0.485, metrics={'acc': 0.7443}]\n",
      "epoch 13: 100%|██████████| 161/161 [00:38<00:00,  4.23it/s, loss=0.493, metrics={'acc': 0.7431}]\n",
      "valid: 100%|██████████| 41/41 [00:03<00:00, 12.68it/s, loss=0.485, metrics={'acc': 0.7445}]\n",
      "epoch 14: 100%|██████████| 161/161 [00:37<00:00,  4.24it/s, loss=0.492, metrics={'acc': 0.7432}]\n",
      "valid: 100%|██████████| 41/41 [00:03<00:00, 11.75it/s, loss=0.485, metrics={'acc': 0.7456}]\n",
      "epoch 15: 100%|██████████| 161/161 [00:37<00:00,  4.25it/s, loss=0.492, metrics={'acc': 0.7432}]\n",
      "valid: 100%|██████████| 41/41 [00:03<00:00, 11.47it/s, loss=0.485, metrics={'acc': 0.7453}]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Epoch: 10. Best val_loss: 0.48540\n",
      "Model weights restored to best epoch: 15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "predict: 100%|██████████| 41/41 [00:03<00:00, 12.14it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Seed[333] Fold 2 | AUC: 0.740915\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "predict: 100%|██████████| 51/51 [00:03<00:00, 13.03it/s]\n",
      "/tmp/ipykernel_89/3576064510.py:69: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  df_train['배란 유도 유형'] = df_train['배란 유도 유형'].replace(mapping)\n",
      "/tmp/ipykernel_89/3576064510.py:70: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  df_test['배란 유도 유형'] = df_test['배란 유도 유형'].replace(mapping)\n",
      "/tmp/ipykernel_89/3576064510.py:199: DeprecationWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, pd.CategoricalDtype) instead\n",
      "  cat_cols = [col for col in train.columns if pd.api.types.is_categorical_dtype(train[col])]\n",
      "/tmp/ipykernel_89/3576064510.py:69: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  df_train['배란 유도 유형'] = df_train['배란 유도 유형'].replace(mapping)\n",
      "/tmp/ipykernel_89/3576064510.py:70: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  df_test['배란 유도 유형'] = df_test['배란 유도 유형'].replace(mapping)\n",
      "/tmp/ipykernel_89/3576064510.py:199: DeprecationWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, pd.CategoricalDtype) instead\n",
      "  cat_cols = [col for col in train.columns if pd.api.types.is_categorical_dtype(train[col])]\n",
      "/tmp/ipykernel_89/3861805098.py:38: DeprecationWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, pd.CategoricalDtype) instead\n",
      "  categorical_cols = [col for col in fold_train.columns if pd.api.types.is_categorical_dtype(fold_train[col])]\n",
      "/home/elicer/.local/lib/python3.10/site-packages/pytorch_widedeep/preprocessing/tab_preprocessor.py:364: UserWarning: Continuous columns will not be normalised\n",
      "  warnings.warn(\"Continuous columns will not be normalised\")\n",
      "epoch 1: 100%|██████████| 161/161 [00:37<00:00,  4.25it/s, loss=0.561, metrics={'acc': 0.7177}]\n",
      "valid: 100%|██████████| 41/41 [00:03<00:00, 12.37it/s, loss=0.503, metrics={'acc': 0.739}] \n",
      "epoch 2: 100%|██████████| 161/161 [00:38<00:00,  4.19it/s, loss=0.503, metrics={'acc': 0.738}] \n",
      "valid: 100%|██████████| 41/41 [00:03<00:00, 11.29it/s, loss=0.496, metrics={'acc': 0.74}]  \n",
      "epoch 3: 100%|██████████| 161/161 [00:38<00:00,  4.24it/s, loss=0.498, metrics={'acc': 0.7408}]\n",
      "valid: 100%|██████████| 41/41 [00:03<00:00, 12.12it/s, loss=0.494, metrics={'acc': 0.7417}]\n",
      "epoch 4: 100%|██████████| 161/161 [00:38<00:00,  4.22it/s, loss=0.496, metrics={'acc': 0.7415}]\n",
      "valid: 100%|██████████| 41/41 [00:03<00:00, 12.36it/s, loss=0.494, metrics={'acc': 0.7425}]\n",
      "epoch 5: 100%|██████████| 161/161 [00:38<00:00,  4.23it/s, loss=0.494, metrics={'acc': 0.7424}]\n",
      "valid: 100%|██████████| 41/41 [00:03<00:00, 12.38it/s, loss=0.493, metrics={'acc': 0.7432}]\n",
      "epoch 6: 100%|██████████| 161/161 [00:38<00:00,  4.21it/s, loss=0.494, metrics={'acc': 0.7424}]\n",
      "valid: 100%|██████████| 41/41 [00:03<00:00, 11.55it/s, loss=0.494, metrics={'acc': 0.7436}]\n",
      "epoch 7: 100%|██████████| 161/161 [00:38<00:00,  4.21it/s, loss=0.493, metrics={'acc': 0.7427}]\n",
      "valid: 100%|██████████| 41/41 [00:03<00:00, 12.37it/s, loss=0.492, metrics={'acc': 0.7438}]\n",
      "epoch 8: 100%|██████████| 161/161 [00:38<00:00,  4.21it/s, loss=0.492, metrics={'acc': 0.7441}]\n",
      "valid: 100%|██████████| 41/41 [00:03<00:00, 12.03it/s, loss=0.493, metrics={'acc': 0.7427}]\n",
      "epoch 9: 100%|██████████| 161/161 [00:37<00:00,  4.24it/s, loss=0.492, metrics={'acc': 0.7444}]\n",
      "valid: 100%|██████████| 41/41 [00:03<00:00, 12.06it/s, loss=0.492, metrics={'acc': 0.7428}]\n",
      "epoch 10: 100%|██████████| 161/161 [00:38<00:00,  4.23it/s, loss=0.491, metrics={'acc': 0.7437}]\n",
      "valid: 100%|██████████| 41/41 [00:03<00:00, 12.43it/s, loss=0.492, metrics={'acc': 0.7426}]\n",
      "epoch 11: 100%|██████████| 161/161 [00:38<00:00,  4.22it/s, loss=0.491, metrics={'acc': 0.744}] \n",
      "valid: 100%|██████████| 41/41 [00:03<00:00, 11.80it/s, loss=0.492, metrics={'acc': 0.7432}]\n",
      "epoch 12: 100%|██████████| 161/161 [00:38<00:00,  4.22it/s, loss=0.491, metrics={'acc': 0.744}] \n",
      "valid: 100%|██████████| 41/41 [00:03<00:00, 11.65it/s, loss=0.491, metrics={'acc': 0.743}] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Epoch: 7. Best val_loss: 0.49232\n",
      "Model weights restored to best epoch: 12\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "predict: 100%|██████████| 41/41 [00:03<00:00, 12.85it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Seed[333] Fold 3 | AUC: 0.737331\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "predict: 100%|██████████| 51/51 [00:03<00:00, 12.80it/s]\n",
      "/tmp/ipykernel_89/3576064510.py:69: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  df_train['배란 유도 유형'] = df_train['배란 유도 유형'].replace(mapping)\n",
      "/tmp/ipykernel_89/3576064510.py:70: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  df_test['배란 유도 유형'] = df_test['배란 유도 유형'].replace(mapping)\n",
      "/tmp/ipykernel_89/3576064510.py:199: DeprecationWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, pd.CategoricalDtype) instead\n",
      "  cat_cols = [col for col in train.columns if pd.api.types.is_categorical_dtype(train[col])]\n",
      "/tmp/ipykernel_89/3576064510.py:69: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  df_train['배란 유도 유형'] = df_train['배란 유도 유형'].replace(mapping)\n",
      "/tmp/ipykernel_89/3576064510.py:70: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  df_test['배란 유도 유형'] = df_test['배란 유도 유형'].replace(mapping)\n",
      "/tmp/ipykernel_89/3576064510.py:199: DeprecationWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, pd.CategoricalDtype) instead\n",
      "  cat_cols = [col for col in train.columns if pd.api.types.is_categorical_dtype(train[col])]\n",
      "/tmp/ipykernel_89/3861805098.py:38: DeprecationWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, pd.CategoricalDtype) instead\n",
      "  categorical_cols = [col for col in fold_train.columns if pd.api.types.is_categorical_dtype(fold_train[col])]\n",
      "/home/elicer/.local/lib/python3.10/site-packages/pytorch_widedeep/preprocessing/tab_preprocessor.py:364: UserWarning: Continuous columns will not be normalised\n",
      "  warnings.warn(\"Continuous columns will not be normalised\")\n",
      "epoch 1: 100%|██████████| 161/161 [00:38<00:00,  4.21it/s, loss=0.617, metrics={'acc': 0.6947}]\n",
      "valid: 100%|██████████| 41/41 [00:03<00:00, 12.52it/s, loss=0.521, metrics={'acc': 0.7258}]\n",
      "epoch 2: 100%|██████████| 161/161 [00:38<00:00,  4.21it/s, loss=0.51, metrics={'acc': 0.7376}] \n",
      "valid: 100%|██████████| 41/41 [00:03<00:00, 12.33it/s, loss=0.505, metrics={'acc': 0.7343}]\n",
      "epoch 3: 100%|██████████| 161/161 [00:38<00:00,  4.22it/s, loss=0.502, metrics={'acc': 0.7394}]\n",
      "valid: 100%|██████████| 41/41 [00:03<00:00, 12.21it/s, loss=0.498, metrics={'acc': 0.7395}]\n",
      "epoch 4: 100%|██████████| 161/161 [00:38<00:00,  4.23it/s, loss=0.498, metrics={'acc': 0.741}] \n",
      "valid: 100%|██████████| 41/41 [00:03<00:00, 12.06it/s, loss=0.495, metrics={'acc': 0.7449}]\n",
      "epoch 5: 100%|██████████| 161/161 [00:38<00:00,  4.21it/s, loss=0.496, metrics={'acc': 0.7411}]\n",
      "valid: 100%|██████████| 41/41 [00:03<00:00, 12.16it/s, loss=0.494, metrics={'acc': 0.7451}]\n",
      "epoch 6: 100%|██████████| 161/161 [00:38<00:00,  4.24it/s, loss=0.494, metrics={'acc': 0.7422}]\n",
      "valid: 100%|██████████| 41/41 [00:03<00:00, 11.89it/s, loss=0.494, metrics={'acc': 0.7455}]\n",
      "epoch 7: 100%|██████████| 161/161 [00:38<00:00,  4.23it/s, loss=0.494, metrics={'acc': 0.7426}]\n",
      "valid: 100%|██████████| 41/41 [00:03<00:00, 11.52it/s, loss=0.494, metrics={'acc': 0.7457}]\n",
      "epoch 8: 100%|██████████| 161/161 [00:38<00:00,  4.22it/s, loss=0.493, metrics={'acc': 0.7425}]\n",
      "valid: 100%|██████████| 41/41 [00:03<00:00, 12.13it/s, loss=0.493, metrics={'acc': 0.7456}]\n",
      "epoch 9: 100%|██████████| 161/161 [00:38<00:00,  4.23it/s, loss=0.493, metrics={'acc': 0.7432}]\n",
      "valid: 100%|██████████| 41/41 [00:03<00:00, 12.27it/s, loss=0.493, metrics={'acc': 0.7456}]\n",
      "epoch 10: 100%|██████████| 161/161 [00:38<00:00,  4.22it/s, loss=0.492, metrics={'acc': 0.743}] \n",
      "valid: 100%|██████████| 41/41 [00:03<00:00, 12.09it/s, loss=0.493, metrics={'acc': 0.7452}]\n",
      "epoch 11: 100%|██████████| 161/161 [00:38<00:00,  4.23it/s, loss=0.491, metrics={'acc': 0.7431}]\n",
      "valid: 100%|██████████| 41/41 [00:03<00:00, 12.57it/s, loss=0.493, metrics={'acc': 0.746}] \n",
      "epoch 12: 100%|██████████| 161/161 [00:38<00:00,  4.22it/s, loss=0.491, metrics={'acc': 0.7439}]\n",
      "valid: 100%|██████████| 41/41 [00:03<00:00, 12.28it/s, loss=0.493, metrics={'acc': 0.7457}]\n",
      "epoch 13: 100%|██████████| 161/161 [00:38<00:00,  4.22it/s, loss=0.491, metrics={'acc': 0.7437}]\n",
      "valid: 100%|██████████| 41/41 [00:03<00:00, 12.21it/s, loss=0.493, metrics={'acc': 0.7462}]\n",
      "epoch 14: 100%|██████████| 161/161 [00:38<00:00,  4.23it/s, loss=0.491, metrics={'acc': 0.744}] \n",
      "valid: 100%|██████████| 41/41 [00:03<00:00, 11.54it/s, loss=0.492, metrics={'acc': 0.7463}]\n",
      "epoch 15: 100%|██████████| 161/161 [00:38<00:00,  4.22it/s, loss=0.49, metrics={'acc': 0.7441}] \n",
      "valid: 100%|██████████| 41/41 [00:03<00:00, 12.25it/s, loss=0.492, metrics={'acc': 0.7464}]\n",
      "epoch 16: 100%|██████████| 161/161 [00:38<00:00,  4.21it/s, loss=0.49, metrics={'acc': 0.7438}] \n",
      "valid: 100%|██████████| 41/41 [00:03<00:00, 12.21it/s, loss=0.492, metrics={'acc': 0.7461}]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Epoch: 11. Best val_loss: 0.49269\n",
      "Model weights restored to best epoch: 16\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "predict: 100%|██████████| 41/41 [00:03<00:00, 12.69it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Seed[333] Fold 4 | AUC: 0.734186\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "predict: 100%|██████████| 51/51 [00:03<00:00, 12.93it/s]\n",
      "/tmp/ipykernel_89/3576064510.py:69: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  df_train['배란 유도 유형'] = df_train['배란 유도 유형'].replace(mapping)\n",
      "/tmp/ipykernel_89/3576064510.py:70: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  df_test['배란 유도 유형'] = df_test['배란 유도 유형'].replace(mapping)\n",
      "/tmp/ipykernel_89/3576064510.py:199: DeprecationWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, pd.CategoricalDtype) instead\n",
      "  cat_cols = [col for col in train.columns if pd.api.types.is_categorical_dtype(train[col])]\n",
      "/tmp/ipykernel_89/3576064510.py:69: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  df_train['배란 유도 유형'] = df_train['배란 유도 유형'].replace(mapping)\n",
      "/tmp/ipykernel_89/3576064510.py:70: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  df_test['배란 유도 유형'] = df_test['배란 유도 유형'].replace(mapping)\n",
      "/tmp/ipykernel_89/3576064510.py:199: DeprecationWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, pd.CategoricalDtype) instead\n",
      "  cat_cols = [col for col in train.columns if pd.api.types.is_categorical_dtype(train[col])]\n",
      "/tmp/ipykernel_89/3861805098.py:38: DeprecationWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, pd.CategoricalDtype) instead\n",
      "  categorical_cols = [col for col in fold_train.columns if pd.api.types.is_categorical_dtype(fold_train[col])]\n",
      "/home/elicer/.local/lib/python3.10/site-packages/pytorch_widedeep/preprocessing/tab_preprocessor.py:364: UserWarning: Continuous columns will not be normalised\n",
      "  warnings.warn(\"Continuous columns will not be normalised\")\n",
      "epoch 1: 100%|██████████| 161/161 [00:38<00:00,  4.22it/s, loss=0.546, metrics={'acc': 0.7272}]\n",
      "valid: 100%|██████████| 41/41 [00:03<00:00, 10.99it/s, loss=0.507, metrics={'acc': 0.7302}]\n",
      "epoch 2: 100%|██████████| 161/161 [00:38<00:00,  4.21it/s, loss=0.503, metrics={'acc': 0.7383}]\n",
      "valid: 100%|██████████| 41/41 [00:03<00:00, 11.16it/s, loss=0.496, metrics={'acc': 0.7425}]\n",
      "epoch 3: 100%|██████████| 161/161 [00:38<00:00,  4.24it/s, loss=0.497, metrics={'acc': 0.7412}]\n",
      "valid: 100%|██████████| 41/41 [00:03<00:00, 11.23it/s, loss=0.494, metrics={'acc': 0.7441}]\n",
      "epoch 4: 100%|██████████| 161/161 [00:38<00:00,  4.22it/s, loss=0.495, metrics={'acc': 0.7415}]\n",
      "valid: 100%|██████████| 41/41 [00:03<00:00, 10.95it/s, loss=0.493, metrics={'acc': 0.745}] \n",
      "epoch 5: 100%|██████████| 161/161 [00:38<00:00,  4.22it/s, loss=0.494, metrics={'acc': 0.7428}]\n",
      "valid: 100%|██████████| 41/41 [00:03<00:00, 10.62it/s, loss=0.493, metrics={'acc': 0.7452}]\n",
      "epoch 6: 100%|██████████| 161/161 [00:38<00:00,  4.22it/s, loss=0.494, metrics={'acc': 0.7424}]\n",
      "valid: 100%|██████████| 41/41 [00:03<00:00, 10.56it/s, loss=0.492, metrics={'acc': 0.7463}]\n",
      "epoch 7: 100%|██████████| 161/161 [00:38<00:00,  4.23it/s, loss=0.493, metrics={'acc': 0.7423}]\n",
      "valid: 100%|██████████| 41/41 [00:03<00:00, 11.53it/s, loss=0.492, metrics={'acc': 0.7465}]\n",
      "epoch 8: 100%|██████████| 161/161 [00:38<00:00,  4.21it/s, loss=0.492, metrics={'acc': 0.7435}]\n",
      "valid: 100%|██████████| 41/41 [00:03<00:00, 11.10it/s, loss=0.492, metrics={'acc': 0.7463}]\n",
      "epoch 9: 100%|██████████| 161/161 [00:38<00:00,  4.21it/s, loss=0.492, metrics={'acc': 0.7428}]\n",
      "valid: 100%|██████████| 41/41 [00:03<00:00, 11.14it/s, loss=0.492, metrics={'acc': 0.7452}]\n",
      "epoch 10: 100%|██████████| 161/161 [00:38<00:00,  4.23it/s, loss=0.492, metrics={'acc': 0.7437}]\n",
      "valid: 100%|██████████| 41/41 [00:03<00:00, 10.82it/s, loss=0.491, metrics={'acc': 0.7466}]\n",
      "epoch 11: 100%|██████████| 161/161 [00:38<00:00,  4.22it/s, loss=0.491, metrics={'acc': 0.7435}]\n",
      "valid: 100%|██████████| 41/41 [00:03<00:00, 11.19it/s, loss=0.491, metrics={'acc': 0.7464}]\n",
      "epoch 12: 100%|██████████| 161/161 [00:38<00:00,  4.20it/s, loss=0.491, metrics={'acc': 0.7439}]\n",
      "valid: 100%|██████████| 41/41 [00:03<00:00, 10.71it/s, loss=0.491, metrics={'acc': 0.7454}]\n",
      "epoch 13: 100%|██████████| 161/161 [00:38<00:00,  4.22it/s, loss=0.491, metrics={'acc': 0.7438}]\n",
      "valid: 100%|██████████| 41/41 [00:03<00:00, 11.58it/s, loss=0.491, metrics={'acc': 0.746}] \n",
      "epoch 14: 100%|██████████| 161/161 [00:38<00:00,  4.23it/s, loss=0.49, metrics={'acc': 0.7446}] \n",
      "valid: 100%|██████████| 41/41 [00:03<00:00, 10.94it/s, loss=0.491, metrics={'acc': 0.7456}]\n",
      "epoch 15: 100%|██████████| 161/161 [00:38<00:00,  4.21it/s, loss=0.49, metrics={'acc': 0.7446}] \n",
      "valid: 100%|██████████| 41/41 [00:03<00:00, 11.83it/s, loss=0.49, metrics={'acc': 0.7463}] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Epoch: 10. Best val_loss: 0.49094\n",
      "Model weights restored to best epoch: 15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "predict: 100%|██████████| 41/41 [00:03<00:00, 11.10it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Seed[333] Fold 5 | AUC: 0.737963\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "predict: 100%|██████████| 51/51 [00:04<00:00, 12.15it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n",
      "Seed[333] Average Metrics | AUC: 0.7370557\n",
      "--------------------------------------------------------------------------------\n",
      "--------------------------------------------------------------------------------\n",
      "Validation Average Metrics | AUC: 0.7370557\n",
      "2936.452466726303\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "start_time = time.time()\n",
    "\n",
    "\n",
    "\n",
    "# 교차 검증 설정: seed_list를 [333] 하나만 사용, n_splits=3\n",
    "seed_list = [333]\n",
    "n_splits = 5\n",
    "\n",
    "total_auc, total_acc, total_f1 = [], [], []\n",
    "test_preds = []\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# 교차 검증 시작\n",
    "for seed in seed_list:\n",
    "    set_seed(seed)\n",
    "    # train, test 불러오기\n",
    "    train = pd.read_csv(train_path).drop(columns=[\"ID\"])\n",
    "    test = pd.read_csv(test_path).drop(columns=[\"ID\"])\n",
    "\n",
    "    skf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=seed)\n",
    "    auc_scores, acc_scores, f1_scores = [], [], []\n",
    "    \n",
    "    for fold, (train_idx, valid_idx) in enumerate(skf.split(train.drop(columns=['임신 성공 여부']), train[\"임신 성공 여부\"])):\n",
    "        # Fold 데이터 생성\n",
    "        fold_train, fold_valid = train.iloc[train_idx], train.iloc[valid_idx]\n",
    "        fold_train2 = fold_train.copy()\n",
    "        fold_test = test.copy()  # test 데이터는 별도 사용\n",
    "        \n",
    "        # 전처리 \n",
    "        fold_train, fold_valid = all_process(fold_train, fold_valid)\n",
    "        _, fold_test = all_process(fold_train2, fold_test)\n",
    "\n",
    "        # 범주형, 연속형 열 구분분\n",
    "        categorical_cols = [col for col in fold_train.columns if pd.api.types.is_categorical_dtype(fold_train[col])]\n",
    "        continuous_cols = [col for col in fold_train.columns if col not in cat_cols and col != '임신 성공 여부']\n",
    "\n",
    "\n",
    "        # Wide 부분: 원-핫 인코딩 (crossed_cols 옵션도 사용할 수 있음)\n",
    "        wide_cols = categorical_cols  # Wide 모델에는 범주형 변수를 원-핫 인코딩으로 처리\n",
    "        crossed_cols = []  # 필요시 두 개 이상의 컬럼을 교차시켜 상호작용 feature 생성\n",
    "\n",
    "        # WidePreprocessor (원-핫 인코딩, crossed_cols 사용 가능)\n",
    "        wide_preprocessor = WidePreprocessor(wide_cols=wide_cols, crossed_cols=crossed_cols)\n",
    "\n",
    "        # Deep 부분: 임베딩 + 연속형 변수 처리\n",
    "        tab_preprocessor = TabPreprocessor(embed_cols=categorical_cols, continuous_cols=continuous_cols)\n",
    "\n",
    "        # 전처리: 각 Fold 별로 Wide & Deep 데이터 생성\n",
    "        X_wide_train = wide_preprocessor.fit_transform(fold_train)\n",
    "        X_wide_valid = wide_preprocessor.transform(fold_valid)\n",
    "        X_wide_test = wide_preprocessor.transform(fold_test)\n",
    "        X_tab_train = tab_preprocessor.fit_transform(fold_train)\n",
    "        X_tab_valid = tab_preprocessor.transform(fold_valid)\n",
    "        X_tab_test = tab_preprocessor.transform(fold_test)\n",
    "\n",
    "        # Target 값: 정수형 (0,1)\n",
    "        y_train = fold_train['임신 성공 여부'].astype(int).values\n",
    "        y_valid = fold_valid['임신 성공 여부'].astype(int).values\n",
    "\n",
    "        # Wide 모델: 입력 차원은 원-핫 인코딩된 피처 수####\n",
    "        wide_model = Wide(input_dim=int(X_wide_train.max().item()), pred_dim=1)\n",
    "\n",
    "\n",
    "        \n",
    "        # Deep 모델: FTTransformer 사용\n",
    "        tab_model = FTTransformer(\n",
    "            column_idx=tab_preprocessor.column_idx,  # 각 컬럼의 인덱스 정보\n",
    "            cat_embed_input=tab_preprocessor.cat_embed_input,  # (컬럼명, 고유값 수, 임베딩 차원)\n",
    "\n",
    "            continuous_cols=continuous_cols,  # 연속형 변수 리스트\n",
    "\n",
    "            # ▶ 범주형 임베딩 관련 설정\n",
    "            cat_embed_dropout=0.0,  # 범주형 임베딩 드롭아웃\n",
    "            # 후보 값: 0.0 (기본), 0.1, 0.2, 0.3\n",
    "\n",
    "\n",
    "            cat_embed_activation=None,  # 범주형 임베딩 활성화 함수\n",
    "            # 후보 값: None (기본), 'relu', 'leaky_relu', 'tanh', 'gelu'\n",
    "\n",
    "\n",
    "            # ▶ 연속형 변수 관련 설정\n",
    "            cont_norm_layer=\"batchnorm\",  # 연속형 변수 정규화 방식\n",
    "            # 후보 값: None (사용 안함), 'batchnorm' (기본 추천), 'layernorm'\n",
    "\n",
    "            embed_continuous_method=\"standard\",  # 연속형 변수 임베딩 방식\n",
    "            # 후보 값: 'standard' (기본), 'periodic', 'piecewise'\n",
    "\n",
    "            cont_embed_dropout=0.0,  # 연속형 임베딩 드롭아웃\n",
    "            # 후보 값: 0.0 (기본), 0.1, 0.2, 0.3\n",
    "\n",
    "            cont_embed_activation='relu',  # 연속형 임베딩 활성화 함수\n",
    "            # 후보 값: None (기본), 'relu', 'leaky_relu', 'tanh', 'gelu'\n",
    "\n",
    "            quantization_setup=None,  # 'piecewise' 방식에서 구간 경계 지정\n",
    "            # 후보 값: None (기본), {\"age\": [20, 30, 40], \"income\": [10000, 30000, 60000]}\n",
    "\n",
    "            # ▶ 기타 임베딩 관련\n",
    "            full_embed_dropout=False,  # 전체 임베딩을 dropout할지 여부\n",
    "            # 후보 값: False (기본), True (더 강한 정규화)\n",
    "\n",
    "            # ▶ FTTransformer 구조 설정\n",
    "            input_dim=64,  # 임베딩 차원 수 (카테고리 + 연속형 임베딩 포함)\n",
    "            # 후보 값: 32, 64 (기본), 128\n",
    "\n",
    "\n",
    "            n_heads=8,  # Attention 헤드 수\n",
    "            # 후보 값: 4, 8 (기본), 16\n",
    "\n",
    "\n",
    "            n_blocks=4,  # Transformer block 수\n",
    "            # 후보 값: 2 (얕은 모델), 4 (기본), 6 (깊은 모델)\n",
    "\n",
    "            attn_dropout=0.2,  # Attention dropout\n",
    "            # 후보 값: 0.0, 0.1, 0.2 (기본), 0.3\n",
    "\n",
    "\n",
    "\n",
    "            transformer_activation=\"reglu\",  # Transformer 내부 활성화 함수\n",
    "            # 후보 값: 'relu', 'gelu', 'leaky_relu', 'tanh', 'geglu', 'reglu' (기본)\n",
    "\n",
    "            # ▶ MLP 설정 (선택 사항)\n",
    "            mlp_hidden_dims=[64, 32],  # MLP 은닉층 크기\n",
    "            # 후보 값: [64, 32] (기본), [128, 64], [256, 128], None (사용 안 함)\n",
    "\n",
    "            mlp_activation=\"relu\",  # MLP 활성화 함수\n",
    "            # 후보 값: 'relu' (기본), 'leaky_relu', 'tanh', 'gelu'\n",
    "\n",
    "            mlp_dropout=0.1,  # MLP dropout\n",
    "            # 후보 값: 0.0, 0.1 (기본), 0.3\n",
    "\n",
    "            mlp_batchnorm=False,  # MLP에 배치 정규화 적용 여부\n",
    "            # 후보 값: False (기본), True\n",
    "\n",
    "            mlp_batchnorm_last=False,  # MLP 마지막 층에도 BN 적용 여부\n",
    "            # 후보 값: False (기본), True\n",
    "\n",
    "        )\n",
    "        \n",
    "        # Wide & Deep 모델 결합\n",
    "        model = WideDeep(wide=wide_model, deeptabular=tab_model)\n",
    "        \n",
    "        # 옵티마이저 및 학습률 스케줄러 설정\n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr=0.0001)  # Set your desired learning rate here\n",
    "        \n",
    "\n",
    "        # ✅ Training 데이터를 딕셔너리 형태로 생성\n",
    "        X_train = {\n",
    "            \"X_wide\": X_wide_train,\n",
    "            \"X_tab\": X_tab_train,\n",
    "            \"target\": y_train\n",
    "        }\n",
    "\n",
    "        # ✅ Validation 데이터를 딕셔너리로 전달 (X_val 방식 사용)\n",
    "        X_val = {\n",
    "            \"X_wide\": X_wide_valid,\n",
    "            \"X_tab\": X_tab_valid,\n",
    "            \"target\": y_valid\n",
    "        }\n",
    "        \n",
    "        # EarlyStopping 콜백 설정 (patience=5, min_delta=0.001)\n",
    "        early_stopping = EarlyStopping(\n",
    "            monitor=\"val_loss\", \n",
    "            patience=5, \n",
    "            min_delta=0.001, \n",
    "            verbose=0\n",
    "        )\n",
    "        \n",
    "        # 현재 날짜/시간을 포함한 파일 이름 생성\n",
    "        timestamp = datetime.datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "        model_path = f\"saved_models/best_model_{timestamp}.pt\"\n",
    "        \n",
    "        # ✅ 2. ModelCheckpoint: 최상의 모델을 자동 저장\n",
    "        model_checkpoint = ModelCheckpoint(\n",
    "            filepath=model_path,  # 모델 저장 경로\n",
    "            monitor=\"val_loss\",        # 감시할 지표 ('val_loss' 또는 'val_acc')\n",
    "            save_best_only=True        # 가장 좋은 성능의 모델만 저장\n",
    "        )\n",
    "        \n",
    "        # Trainer 생성: objective \"binary\"로 설정, 평가 지표로 Accuracy, AUROC, F1Score 사용\n",
    "        trainer = Trainer(\n",
    "            model=model, \n",
    "            objective=\"binary\", \n",
    "            custom_loss_function=None,  # 기본 손실 함수 사용 (필요 시 사용자 정의 함수 가능)\n",
    "            optimizers=optimizer,       # 옵티마이저 (기본값: Adam)\n",
    "            initializers=None,          # 가중치 초기화 없음 (기본값)\n",
    "            callbacks=[early_stopping, model_checkpoint],  # 조기 종료 (3 에포크 동안 개선 없으면 종료)\n",
    "            metrics=[Accuracy()],\n",
    "            verbose=0,                  # 학습 로그 출력 (기본값: 1)\n",
    "            seed=seed                     # 랜덤 시드 설정 (기본값: 1)\n",
    "        )\n",
    "        \n",
    "        # 학습: validation 데이터는 별도의 인자로 전달\n",
    "        trainer.fit(\n",
    "            X_train=X_train,  # ✅ Training 데이터를 딕셔너리로 \n",
    "            n_epochs=100,  ####\n",
    "            batch_size=1024, \n",
    "            X_val=X_val  # ✅ Validation 데이터를 딕셔너리로 전달\n",
    "        )\n",
    "        \n",
    "        \n",
    "        \n",
    "        # 모델 학습 후 Validation 예측 코드:\n",
    "        # 이미 확률이 계산되어 있는 컬럼을 사용합니다.\n",
    "        valid_probs = trainer.predict_proba(X_wide=X_wide_valid, X_tab=X_tab_valid)[:,1] ####\n",
    "        valid_pred = (valid_probs > 0.5).astype(int)\n",
    "\n",
    "        \n",
    "\n",
    "        # 실제 정답 \n",
    "        y_valid = fold_valid['임신 성공 여부'].values.astype(int)\n",
    "\n",
    "\n",
    "        \n",
    "        # 평가 지표 계산: 클래스 1의 확률 사용\n",
    "        fold_auc = roc_auc_score(y_valid, valid_probs)\n",
    "        print(f\"Seed[{seed:<3}] Fold {fold + 1} | AUC: {fold_auc:.6f}\")\n",
    "        \n",
    "        auc_scores.append(fold_auc)\n",
    "        \n",
    "        total_auc.append(fold_auc)\n",
    "        # Test 데이터 예측 (각 fold의 모델로 예측한 결과 저장)\n",
    "        test_pred = trainer.predict_proba(X_wide=X_wide_test, X_tab=X_tab_test)[:,1]\n",
    "        test_preds.append(test_pred)\n",
    "    \n",
    "    # Fold 별 평균 성능 출력\n",
    "    avg_auc = np.mean(auc_scores)\n",
    "    \n",
    "    print(\"-\" * 80)\n",
    "    print(f\"Seed[{seed:<3}] Average Metrics | AUC: {avg_auc:.7f}\")\n",
    "    print(\"-\" * 80)\n",
    "\n",
    "# 전체 Validation 평균 성능 출력\n",
    "val_auc = np.mean(total_auc)\n",
    "print(\"-\" * 80)\n",
    "print(f\"Validation Average Metrics | AUC: {val_auc:.7f}\")\n",
    "\n",
    "finish_time = time.time()\n",
    "total_time = finish_time - start_time \n",
    "\n",
    "print(total_time)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'sample_path' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[17], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m sample_submission \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(\u001b[43msample_path\u001b[49m)\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m# test_preds\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# sample_submission['임신 성공 확률'] = np.mean(test_preds, axis=0)\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;66;03m# count = (sample_submission['임신 성공 확률'] >= 0.5).sum()\u001b[39;00m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;66;03m# print(f'test의 True 갯수: {count:<5} (예측 결과)')\u001b[39;00m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'sample_path' is not defined"
     ]
    }
   ],
   "source": [
    "sample_submission = pd.read_csv(sample_path)\n",
    "\n",
    "sample_submission['probability'] = np.mean(test_preds, axis=0)\n",
    "\n",
    "\n",
    "sample_submission\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_submission.to_csv(f'./Submission/FTT_{seed_list[0]}.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lg2025",
   "language": "python",
   "name": "lg2025"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
